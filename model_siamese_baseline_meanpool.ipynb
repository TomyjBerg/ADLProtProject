{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize features? \n",
    "## Invert h-bond and charge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'adl_model_siamese_baseline'\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grabeda2/pyg_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader #To make use of this data loader, all graph attributes in the dataset need to have the same shape. In particular, this data loader should only be used when working with dense adjacency matrices.\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir_1 = 'C:/Users/david/pyproj/pyg/adl/patch_label_1'\n",
    "#data_dir_0 = 'C:/Users/david/pyproj/pyg/adl/patch_label_0'\n",
    "data_dir_1 = 'adl_data_1'\n",
    "data_dir_0 = 'adl_data_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from c_PatchDataset import PatchDataset\n",
    "dataset = PatchDataset(data_dir_label_0 = data_dir_0,  data_dir_label_1=data_dir_1,  neg_pos_ratio=1)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(572):\n",
      "====================\n",
      "Number of graphs pairs: 572\n",
      "\n",
      "PairData(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=1)\n",
      "=============================================================\n",
      "Number of nodes in each: None\n",
      "Number of node features: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grabeda2/pyg_env/lib/python3.10/site-packages/torch_geometric/data/storage.py:280: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'x1', 'adj1', 'adj2', 'y', 'x2'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs pairs: {len(dataset)}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes in each: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.8288e-01, -0.0000e+00,  9.2380e-01],\n",
       "        [-3.6446e-02,  0.0000e+00,  9.9934e-01],\n",
       "        [-4.6978e-02,  0.0000e+00,  9.9890e-01],\n",
       "        [ 1.0696e-02,  0.0000e+00,  9.9994e-01],\n",
       "        [-3.3979e-01, -7.0446e-03,  9.4048e-01],\n",
       "        [-3.5847e-01, -3.4040e-03,  9.3353e-01],\n",
       "        [-1.7399e-02,  0.0000e+00, -9.9985e-01],\n",
       "        [-2.6579e-02,  0.0000e+00,  9.9965e-01],\n",
       "        [-3.8084e-02,  0.0000e+00,  9.9927e-01],\n",
       "        [ 1.2247e-01,  3.4226e-01,  9.3159e-01],\n",
       "        [-8.8817e-02,  0.0000e+00,  9.9605e-01],\n",
       "        [-6.3177e-02,  0.0000e+00,  9.9800e-01],\n",
       "        [-3.0755e-02,  0.0000e+00,  9.9953e-01],\n",
       "        [-4.7284e-02,  0.0000e+00,  9.9888e-01],\n",
       "        [ 8.3658e-02,  7.5403e-04,  9.9649e-01],\n",
       "        [-1.5631e-01,  0.0000e+00, -9.8771e-01],\n",
       "        [-1.6639e-01,  0.0000e+00, -9.8606e-01],\n",
       "        [-6.0766e-02, -1.4478e-01,  9.8760e-01],\n",
       "        [-9.8570e-03,  0.0000e+00,  9.9995e-01],\n",
       "        [-5.5110e-01, -1.2576e-01,  8.2491e-01],\n",
       "        [-1.6636e-01,  0.0000e+00, -9.8607e-01],\n",
       "        [ 8.3247e-02,  0.0000e+00,  9.9653e-01],\n",
       "        [-3.6020e-02,  0.0000e+00,  9.9935e-01],\n",
       "        [-1.4612e-01,  0.0000e+00,  9.8927e-01],\n",
       "        [-4.4775e-02,  0.0000e+00,  9.9900e-01],\n",
       "        [ 9.4042e-02,  1.6641e-01,  9.8156e-01],\n",
       "        [-1.5892e-01, -2.1927e-01,  9.6264e-01],\n",
       "        [-6.7388e-02,  0.0000e+00,  9.9773e-01],\n",
       "        [-2.1413e-01,  0.0000e+00, -9.7680e-01],\n",
       "        [-3.4475e-01, -1.3543e-01,  9.2887e-01],\n",
       "        [-5.0362e-02,  0.0000e+00,  9.9873e-01],\n",
       "        [-4.4087e-02,  0.0000e+00,  9.9903e-01],\n",
       "        [-1.5112e-01,  0.0000e+00, -9.8852e-01],\n",
       "        [-2.4445e-01, -1.1490e-01, -9.6283e-01],\n",
       "        [ 1.2492e-01,  0.0000e+00,  9.9217e-01],\n",
       "        [ 5.2572e-02,  0.0000e+00,  9.9862e-01],\n",
       "        [ 1.1726e-01,  0.0000e+00,  9.9310e-01],\n",
       "        [-3.9607e-02,  0.0000e+00,  9.9922e-01],\n",
       "        [-2.1020e-01,  0.0000e+00, -9.7766e-01],\n",
       "        [-4.6864e-01,  0.0000e+00, -8.8339e-01],\n",
       "        [-2.5963e-02, -6.4130e-02,  9.9760e-01],\n",
       "        [ 1.5481e-01,  0.0000e+00,  9.8794e-01],\n",
       "        [-1.2684e-01,  0.0000e+00,  9.9192e-01],\n",
       "        [-6.5123e-02, -7.2801e-03,  9.9785e-01],\n",
       "        [-4.8278e-01,  0.0000e+00, -8.7574e-01],\n",
       "        [-1.1967e-01, -1.4958e-01,  9.8148e-01],\n",
       "        [-3.6359e-01,  0.0000e+00,  9.3156e-01],\n",
       "        [-5.3640e-02,  0.0000e+00,  9.9856e-01],\n",
       "        [ 4.5398e-01,  0.0000e+00,  8.9101e-01],\n",
       "        [-3.5863e-02,  0.0000e+00,  9.9936e-01],\n",
       "        [-3.6165e-02,  0.0000e+00,  9.9935e-01],\n",
       "        [-1.4303e-01, -1.9234e-01, -9.7085e-01],\n",
       "        [-3.5678e-02,  0.0000e+00,  9.9936e-01],\n",
       "        [-2.6666e-01,  0.0000e+00, -9.6379e-01],\n",
       "        [-1.3312e-01,  0.0000e+00,  9.9110e-01],\n",
       "        [-7.4869e-02,  0.0000e+00,  9.9719e-01],\n",
       "        [-2.7615e-02,  0.0000e+00,  9.9962e-01],\n",
       "        [ 8.3811e-02, -1.4048e-01,  9.8653e-01],\n",
       "        [-2.3148e-01, -1.6662e-01,  9.5846e-01],\n",
       "        [-7.0224e-01, -1.6891e-02, -7.1174e-01],\n",
       "        [-2.6938e-01, -6.5509e-02,  9.6080e-01],\n",
       "        [-1.8299e-01,  0.0000e+00, -9.8311e-01],\n",
       "        [-2.9669e-01,  2.3042e-02,  9.5470e-01],\n",
       "        [-5.7177e-01, -3.6166e-02,  8.1962e-01],\n",
       "        [-1.4187e-02, -2.8804e-04,  9.9990e-01],\n",
       "        [-5.4964e-02,  0.0000e+00,  9.9849e-01],\n",
       "        [-1.5758e-01,  0.0000e+00, -9.8751e-01],\n",
       "        [-5.3490e-02,  0.0000e+00,  9.9857e-01],\n",
       "        [ 7.7872e-02,  1.0865e-01,  9.9103e-01],\n",
       "        [-5.9882e-02,  0.0000e+00,  9.9821e-01],\n",
       "        [-3.7271e-02,  0.0000e+00,  9.9931e-01],\n",
       "        [-1.2044e-02,  0.0000e+00,  9.9993e-01],\n",
       "        [-2.7780e-02,  0.0000e+00,  9.9961e-01],\n",
       "        [-3.4703e-02,  0.0000e+00,  9.9940e-01],\n",
       "        [-4.8392e-02,  0.0000e+00,  9.9883e-01],\n",
       "        [-4.2026e-02,  0.0000e+00,  9.9912e-01],\n",
       "        [ 2.4923e-01,  0.0000e+00,  9.6845e-01],\n",
       "        [ 4.7729e-02, -2.7654e-02,  9.9848e-01],\n",
       "        [-5.0996e-02,  0.0000e+00,  9.9870e-01],\n",
       "        [ 2.2218e-01,  1.7745e-01,  9.5872e-01],\n",
       "        [-1.5635e-01,  0.0000e+00,  9.8770e-01],\n",
       "        [-3.1853e-01, -2.1686e-02,  9.4767e-01],\n",
       "        [ 2.8212e-01,  0.0000e+00,  9.5938e-01],\n",
       "        [-7.0514e-02,  0.0000e+00,  9.9751e-01],\n",
       "        [-3.4211e-02,  0.0000e+00,  9.9941e-01],\n",
       "        [-3.2691e-01,  0.0000e+00,  9.4506e-01],\n",
       "        [-3.4183e-01,  0.0000e+00, -9.3976e-01],\n",
       "        [-5.3840e-03,  0.0000e+00,  9.9999e-01],\n",
       "        [-6.5644e-03,  0.0000e+00,  9.9998e-01],\n",
       "        [ 2.3768e-02,  0.0000e+00,  9.9972e-01],\n",
       "        [-4.9885e-01, -2.4956e-02,  8.6633e-01],\n",
       "        [ 1.3569e-01,  0.0000e+00,  9.9075e-01],\n",
       "        [-2.5473e-02,  0.0000e+00,  9.9968e-01],\n",
       "        [-3.1889e-02,  0.0000e+00,  9.9949e-01],\n",
       "        [-9.3742e-03,  9.7745e-02,  9.9517e-01],\n",
       "        [-4.6570e-02,  0.0000e+00,  9.9892e-01],\n",
       "        [ 9.9500e-02,  0.0000e+00,  9.9504e-01],\n",
       "        [ 3.9978e-01,  0.0000e+00, -9.1661e-01],\n",
       "        [-3.5510e-02,  0.0000e+00,  9.9937e-01],\n",
       "        [-4.5842e-02,  0.0000e+00,  9.9895e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work we do not have pos\n",
    "#visualize_points(data.pos, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs pairs: 382\n",
      "Number of validation graphs: 95\n",
      "Number of test graphs: 95\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "n_train = math.ceil((4/6) * len(dataset))\n",
    "n_val = math.ceil((len(dataset) - n_train)/2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f'Number of training graphs pairs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size= batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairDataBatch(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=[1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = next(iter(train_loader))\n",
    "databatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_nodes, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        # Each instance of this GNN will have 3 convolutional layers and three batch norm layers        \n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        #self.bns1 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        #self.bns2 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        #self.bns3 = torch.nn.BatchNorm1d(in_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        \n",
    "        #Step 1\n",
    "        x = self.conv1(x, adj, mask)\n",
    "        #x = self.bns1(x)\n",
    "        \n",
    "        #Step 2\n",
    "        x = self.conv2(x, adj, mask)\n",
    "        #x = self.bns2(x)\n",
    "\n",
    "        #Step 3\n",
    "        x = self.conv3(x, adj, mask)\n",
    "        #if x.shape[2] != 1: \n",
    "        #    x = self.bns3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        in_nodes = num_nodes\n",
    "        out_nodes = 25 # Number of clusters / nodes in the next layer\n",
    "        #self.gnn1_pool = GNN(in_nodes, dataset.num_features, 16, out_nodes) # PoolGNN --> Cluster Assignment Matrix to reduce to num_nodes\n",
    "        self.gnn1_embed = GNN(in_nodes, dataset.num_features, 8, 8) # EmbGNN --> Convolutions to create new node embedding\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 10\n",
    "        #self.gnn2_pool = GNN(in_nodes, 8, 8, out_nodes)\n",
    "        self.gnn2_embed = GNN(in_nodes, 8, 12, 16, lin=False)\n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 1\n",
    "        #self.gnn3_pool = GNN(in_nodes, 16, 16, out_nodes)\n",
    "        self.gnn3_embed = GNN(in_nodes, 16, 16, 32, lin=False)\n",
    "\n",
    "        # Final Classifier\n",
    "        self.lin1 = torch.nn.Linear(32, 64) \n",
    "        #self.lin2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, batch, mask=None):\n",
    "        \n",
    "        #if batch == 0: print('Shape of input data batch:')\n",
    "        #if batch == 0: print(f'Feature Matrix: {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'Adjacency Matrix: {tuple(adj.shape)}')\n",
    "       \n",
    "\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        #if batch == 0: print('Hierarchical Step #1')\n",
    "        x = self.gnn1_embed(x, adj, mask) # node feature embedding\n",
    "        #s = self.gnn1_pool(x, adj, mask) # cluster assignment matrix\n",
    "\n",
    "        #if batch == 0: print(f'X1 = {tuple(x1.shape)}    S1: {tuple(s.shape)}')\n",
    "\n",
    "        #x, adj, l1, e1 = dense_diff_pool(x1, adj, s, mask) # does the necessary matrix multiplications\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "   \n",
    "\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        #if batch == 0: print('Hierarchical Step #2')\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        #s = self.gnn2_pool(x, adj)\n",
    "\n",
    "        #if batch == 0: print(f'X2: {tuple(x2.shape)}    S2: {tuple(s.shape)}')\n",
    "        \n",
    "        #x, adj, l2, e2 = dense_diff_pool(x2, adj, s)\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "      \n",
    "        \n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        #if batch == 0: print('Hierarchical Step #3')\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        #s = self.gnn3_pool(x, adj)\n",
    "        \n",
    "        #if batch == 0: print(f'X3: {tuple(x3.shape)}    S3: {tuple(s.shape)}')\n",
    "\n",
    "        #x, adj, l3, e3 = dense_diff_pool(x3, adj, s)\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "     \n",
    "        \n",
    "\n",
    "        # Final Classification\n",
    "        #if batch == 0: print('Final Output')\n",
    "        x = x.mean(dim=1) # Pool the features of all nodes (global mean pool)  dim = 1 refers to columns\n",
    "        #if batch == 0: print(f'---X Output after mean= {tuple(x.shape)}')\n",
    "\n",
    "        x = F.relu(self.lin1(x)) # Fully connected layer + relu\n",
    "        #if batch == 0: print(f'------ X Output 3 after lin= {tuple(x.shape)}')\n",
    "\n",
    "        \n",
    "        return x #, l1 + l2 + l3, e1 + e2 + e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt of a contrastive loss function\n",
    "#   pairs with label 1 --> should get small euclid dist = small loss\n",
    "#   pairs with label 0 --> should get large euclid dist = large loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        # euclidian distance\n",
    "        #print(x0)\n",
    "        #print(x1)\n",
    "        #print(y)\n",
    "        diff = x0 - x1\n",
    "        #print(diff)\n",
    "        pow = torch.pow(diff, 2)\n",
    "        #print(pow)\n",
    "        dist_sq = torch.sum(pow, 1)\n",
    "        #print(dist_sq) # sum of squared distance = 0.5 = 9\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "        #print(dist) # euclidean distance = 0.7 = 3\n",
    "\n",
    "        mdist = self.margin - dist #negative euclidean distance - margin = 0.3 = -2\n",
    "        #print(mdist)\n",
    "        dist_marg = torch.clamp(mdist, min=0.0) # only distances <margin will be still positive here = 0.3 = 0\n",
    "        #print(dist)\n",
    "        loss =  y * torch.pow(dist, 2) + (1-y) * torch.pow(dist_marg,2)\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 0.5\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0.3^2\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 0.5\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 9\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 9\n",
    "\n",
    "        #print(loss)\n",
    "        #loss = torch.sum(loss) / 2.0 \n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        #print(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 001, Train Loss: 0.124\n",
      "Epoch: 002, Train Loss: 0.112\n",
      "Epoch: 003, Train Loss: 0.108\n",
      "Epoch: 004, Train Loss: 0.108\n",
      "Epoch: 005, Train Loss: 0.113\n",
      "Epoch: 006, Train Loss: 0.107\n",
      "Epoch: 007, Train Loss: 0.102\n",
      "Epoch: 008, Train Loss: 0.102\n",
      "Epoch: 009, Train Loss: 0.102\n",
      "Epoch: 010, Train Loss: 0.102\n",
      "Epoch: 011, Train Loss: 0.100\n",
      "Epoch: 012, Train Loss: 0.102\n",
      "Epoch: 013, Train Loss: 0.104\n",
      "Epoch: 014, Train Loss: 0.104\n",
      "Epoch: 015, Train Loss: 0.101\n",
      "Epoch: 016, Train Loss: 0.098\n",
      "Epoch: 017, Train Loss: 0.102\n",
      "Epoch: 018, Train Loss: 0.096\n",
      "Epoch: 019, Train Loss: 0.096\n",
      "Epoch: 020, Train Loss: 0.094\n",
      "Epoch: 021, Train Loss: 0.102\n",
      "Epoch: 022, Train Loss: 0.097\n",
      "Epoch: 023, Train Loss: 0.097\n",
      "Epoch: 024, Train Loss: 0.098\n",
      "Epoch: 025, Train Loss: 0.099\n",
      "Epoch: 026, Train Loss: 0.099\n",
      "Epoch: 027, Train Loss: 0.098\n",
      "Epoch: 028, Train Loss: 0.096\n",
      "Epoch: 029, Train Loss: 0.099\n",
      "Epoch: 030, Train Loss: 0.098\n",
      "Epoch: 031, Train Loss: 0.098\n",
      "Epoch: 032, Train Loss: 0.097\n",
      "Epoch: 033, Train Loss: 0.098\n",
      "Epoch: 034, Train Loss: 0.096\n",
      "Epoch: 035, Train Loss: 0.098\n",
      "Epoch: 036, Train Loss: 0.097\n",
      "Epoch: 037, Train Loss: 0.096\n",
      "Epoch: 038, Train Loss: 0.097\n",
      "Epoch: 039, Train Loss: 0.094\n",
      "Epoch: 040, Train Loss: 0.098\n",
      "Epoch: 041, Train Loss: 0.094\n",
      "Epoch: 042, Train Loss: 0.100\n",
      "Epoch: 043, Train Loss: 0.095\n",
      "Epoch: 044, Train Loss: 0.097\n",
      "Epoch: 045, Train Loss: 0.096\n",
      "Epoch: 046, Train Loss: 0.094\n",
      "Epoch: 047, Train Loss: 0.095\n",
      "Epoch: 048, Train Loss: 0.097\n",
      "Epoch: 049, Train Loss: 0.093\n",
      "Epoch: 050, Train Loss: 0.095\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = DiffPool(num_nodes = 100).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    batch = 0\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1 = model(data.x1, data.adj1, batch)\n",
    "        output2 = model(data.x2, data.adj2, batch = None)\n",
    "        \n",
    "        #Contrastive Loss\n",
    "        loss_contrastive = criterion(output1,output2,data.y)\n",
    "        loss_contrastive.backward()\n",
    "        loss_all += data.y.size(0) * loss_contrastive.item()\n",
    "        optimizer.step()\n",
    "        batch +=1\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    distances_lab1 = []\n",
    "    distances_lab0 = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    \n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output1 = model(data.x1, data.adj2, batch=None)\n",
    "        output2 = model(data.x2, data.adj2, batch=None)\n",
    "\n",
    "        test_loss_contrastive = criterion(output1, output2, data.y)\n",
    "        \n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        label = data.y\n",
    "\n",
    "        if int(label) == 1: \n",
    "            distances_lab1.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "        else:\n",
    "            distances_lab0.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "\n",
    "    return  distances_lab0, distances_lab1, losses, labels\n",
    "\n",
    "\n",
    "\n",
    "train_distances_lab0 = []\n",
    "train_distances_lab1 = []\n",
    "train_losses = []\n",
    "train_labels = []\n",
    "\n",
    "validation_distances_lab0 = []\n",
    "validation_distances_lab1 = []\n",
    "validation_losses = []\n",
    "validation_labels = []\n",
    "\n",
    "test_distances_lab0 = []\n",
    "test_distances_lab1 = []\n",
    "test_losses = []\n",
    "test_labels = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "\n",
    "    train_results = test(train_loader)\n",
    "    train_distances_lab0.append(train_results[0])\n",
    "    train_distances_lab1.append(train_results[1])\n",
    "    train_losses.append(train_results[2])\n",
    "    train_labels.append(train_results[3])\n",
    "\n",
    "\n",
    "    validation_results = test(val_loader)\n",
    "    validation_distances_lab0.append(validation_results[0])\n",
    "    validation_distances_lab1.append(validation_results[1])\n",
    "    validation_losses.append(validation_results[2])\n",
    "    validation_labels.append(validation_results[3])\n",
    "\n",
    "    test_results = test(test_loader)\n",
    "    test_distances_lab0.append(test_results[0])\n",
    "    test_distances_lab1.append(test_results[1])\n",
    "    test_losses.append(test_results[2])\n",
    "    test_labels.append(test_results[3])\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}')\n",
    "    #Train Acc: {train_acc:.3f}, f'Val Acc: {val_acc:.3f}, Test Acc: {test_acc:.3f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_euclid_distances(distances_lab0, distances_lab1):\n",
    "\n",
    "    w = 0.8    # bar width\n",
    "    x = [1, 2] # x-coordinates of your bars\n",
    "    colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "\n",
    "    # Epoch 0\n",
    "    y = [distances_lab0, distances_lab1]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x,\n",
    "        height=[np.mean(yi) for yi in y],\n",
    "        yerr=[np.std(yi) for yi in y],    # error bars\n",
    "        capsize=12, # error bar cap width in points\n",
    "        width=w,    # bar width\n",
    "        tick_label=[\"Label 0\", \"Label 1\"],\n",
    "        color=(0,0,0,0),  # face color transparent\n",
    "        edgecolor=colors,\n",
    "        )\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # distribute scatter randomly across whole width of bar\n",
    "        ax.scatter(x[i] + np.random.random(len(y[i])) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "    plt.ylabel = 'Euclidean Distance'\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_euclid_distances(train_distances_lab0[49], train_distances_lab1[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "gnn1_embed.conv1.bias \t torch.Size([8])\n",
      "gnn1_embed.conv1.lin.weight \t torch.Size([8, 3])\n",
      "gnn1_embed.conv2.bias \t torch.Size([8])\n",
      "gnn1_embed.conv2.lin.weight \t torch.Size([8, 8])\n",
      "gnn1_embed.conv3.bias \t torch.Size([8])\n",
      "gnn1_embed.conv3.lin.weight \t torch.Size([8, 8])\n",
      "gnn2_embed.conv1.bias \t torch.Size([12])\n",
      "gnn2_embed.conv1.lin.weight \t torch.Size([12, 8])\n",
      "gnn2_embed.conv2.bias \t torch.Size([12])\n",
      "gnn2_embed.conv2.lin.weight \t torch.Size([12, 12])\n",
      "gnn2_embed.conv3.bias \t torch.Size([16])\n",
      "gnn2_embed.conv3.lin.weight \t torch.Size([16, 12])\n",
      "gnn3_embed.conv1.bias \t torch.Size([16])\n",
      "gnn3_embed.conv1.lin.weight \t torch.Size([16, 16])\n",
      "gnn3_embed.conv2.bias \t torch.Size([16])\n",
      "gnn3_embed.conv2.lin.weight \t torch.Size([16, 16])\n",
      "gnn3_embed.conv3.bias \t torch.Size([32])\n",
      "gnn3_embed.conv3.lin.weight \t torch.Size([32, 16])\n",
      "lin1.weight \t torch.Size([64, 32])\n",
      "lin1.bias \t torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {0: {'step': tensor(19100.), 'exp_avg': tensor([-0.0058,  0.0173,  0.0135, -0.0101, -0.0131,  0.0151, -0.0205, -0.0008],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0117, 0.0398, 0.0180, 0.0117, 0.0541, 0.0269, 0.0489, 0.0524],\n",
      "       device='cuda:0')}, 1: {'step': tensor(19100.), 'exp_avg': tensor([[-0.0017, -0.0013,  0.0055],\n",
      "        [ 0.0029,  0.0011, -0.0200],\n",
      "        [-0.0049, -0.0008, -0.0083],\n",
      "        [-0.0007, -0.0009,  0.0090],\n",
      "        [-0.0063, -0.0023,  0.0185],\n",
      "        [-0.0044,  0.0006, -0.0071],\n",
      "        [ 0.0024, -0.0016,  0.0129],\n",
      "        [ 0.0106,  0.0022, -0.0112]], device='cuda:0'), 'exp_avg_sq': tensor([[1.4025e-03, 8.1949e-05, 7.4188e-03],\n",
      "        [7.2601e-03, 2.6235e-04, 2.3773e-02],\n",
      "        [1.9565e-03, 5.1253e-05, 1.1561e-02],\n",
      "        [1.7316e-03, 8.8687e-05, 7.6239e-03],\n",
      "        [7.6602e-03, 4.1261e-04, 3.5059e-02],\n",
      "        [2.2393e-03, 9.4139e-05, 1.7166e-02],\n",
      "        [5.3407e-03, 2.7260e-04, 3.1471e-02],\n",
      "        [4.7616e-03, 2.4996e-04, 3.3619e-02]], device='cuda:0')}, 2: {'step': tensor(19100.), 'exp_avg': tensor([ 9.2991e-03, -9.7013e-03, -1.0472e-02, -4.4936e-03,  1.6306e-02,\n",
      "         7.2351e-05,  1.4412e-02, -6.3585e-03], device='cuda:0'), 'exp_avg_sq': tensor([0.0074, 0.0165, 0.0313, 0.0096, 0.0228, 0.0021, 0.0300, 0.0299],\n",
      "       device='cuda:0')}, 3: {'step': tensor(19100.), 'exp_avg': tensor([[-0.0004,  0.0007,  0.0010,  0.0011,  0.0025,  0.0031, -0.0039, -0.0043],\n",
      "        [-0.0015, -0.0023, -0.0034, -0.0014,  0.0033,  0.0004, -0.0042, -0.0048],\n",
      "        [-0.0009,  0.0011,  0.0003, -0.0027, -0.0046, -0.0008,  0.0014,  0.0054],\n",
      "        [ 0.0005,  0.0012,  0.0013, -0.0010, -0.0044, -0.0018,  0.0041,  0.0061],\n",
      "        [-0.0005,  0.0011,  0.0017,  0.0023,  0.0046,  0.0052, -0.0064, -0.0076],\n",
      "        [ 0.0004, -0.0003, -0.0002,  0.0004,  0.0002, -0.0007,  0.0010,  0.0002],\n",
      "        [ 0.0018,  0.0016,  0.0031,  0.0029, -0.0004,  0.0002,  0.0029,  0.0013],\n",
      "        [ 0.0017,  0.0018,  0.0023, -0.0010, -0.0080, -0.0045,  0.0095,  0.0120]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[8.6672e-05, 3.3749e-04, 8.0383e-04, 1.3821e-04, 5.9460e-04, 7.9599e-04,\n",
      "         1.3334e-03, 1.4757e-03],\n",
      "        [1.0311e-04, 2.3883e-04, 5.5621e-04, 2.3129e-04, 9.5294e-04, 1.4718e-03,\n",
      "         2.5346e-03, 2.7406e-03],\n",
      "        [3.7907e-04, 9.7963e-04, 2.0099e-03, 5.7523e-04, 2.3228e-03, 3.1907e-03,\n",
      "         5.7258e-03, 5.7923e-03],\n",
      "        [9.8289e-05, 2.9712e-04, 6.6799e-04, 2.0915e-04, 9.8014e-04, 9.4647e-04,\n",
      "         1.7352e-03, 2.2364e-03],\n",
      "        [2.5141e-04, 1.0389e-03, 2.3383e-03, 4.1287e-04, 1.8026e-03, 2.5305e-03,\n",
      "         4.0555e-03, 4.4284e-03],\n",
      "        [3.1714e-05, 4.6174e-05, 1.4107e-04, 3.4961e-05, 1.3516e-04, 1.6457e-04,\n",
      "         4.0697e-04, 3.9012e-04],\n",
      "        [2.5597e-04, 7.0317e-04, 1.3673e-03, 3.2471e-04, 1.4214e-03, 2.8494e-03,\n",
      "         4.9196e-03, 4.3665e-03],\n",
      "        [2.9424e-04, 7.9423e-04, 2.0151e-03, 5.8613e-04, 3.0743e-03, 2.6788e-03,\n",
      "         5.4952e-03, 7.2616e-03]], device='cuda:0')}, 4: {'step': tensor(19100.), 'exp_avg': tensor([-0.0128,  0.0057,  0.0052,  0.0123,  0.0047,  0.0052, -0.0017,  0.0009],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0123, 0.0028, 0.0094, 0.0135, 0.0043, 0.0475, 0.0028, 0.0028],\n",
      "       device='cuda:0')}, 5: {'step': tensor(19100.), 'exp_avg': tensor([[-1.8456e-03,  7.3113e-03,  4.2395e-04,  2.7462e-03, -2.9201e-03,\n",
      "          1.6128e-04, -2.9156e-03, -6.1845e-03],\n",
      "        [ 1.1847e-03, -6.7072e-04,  3.6410e-04, -8.1623e-04,  1.8067e-03,\n",
      "         -1.9779e-04,  2.1191e-04,  1.3612e-03],\n",
      "        [-1.7622e-03, -2.1288e-03,  2.3602e-03,  1.7175e-03, -2.2362e-03,\n",
      "         -4.7224e-04,  8.4695e-04,  2.7821e-03],\n",
      "        [ 1.4190e-03,  1.2077e-03,  1.8367e-03, -2.4288e-04,  2.2367e-03,\n",
      "         -6.0954e-04, -6.6948e-04,  1.2932e-03],\n",
      "        [-1.0891e-03, -3.3724e-03,  9.4414e-04,  4.6843e-04, -1.3737e-03,\n",
      "         -2.1146e-04,  1.3652e-03,  2.9234e-03],\n",
      "        [ 3.7368e-03,  1.9228e-02,  2.6079e-03,  1.4230e-03,  5.0670e-03,\n",
      "         -7.8792e-04, -8.1921e-03, -1.0918e-02],\n",
      "        [ 1.0704e-03,  4.2579e-03, -7.1763e-04, -3.6540e-04,  1.3209e-03,\n",
      "          1.0911e-04, -1.7903e-03, -3.3434e-03],\n",
      "        [ 1.5017e-03,  2.7080e-03, -9.2703e-04, -1.0282e-03,  1.9838e-03,\n",
      "          9.7774e-05, -1.1725e-03, -2.1060e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.2691e-03, 4.4326e-03, 3.4173e-04, 6.9485e-04, 2.6804e-03, 3.0908e-05,\n",
      "         7.5397e-04, 2.4618e-03],\n",
      "        [1.6004e-04, 8.3289e-04, 5.2167e-05, 1.5003e-04, 3.3871e-04, 4.7638e-06,\n",
      "         1.4036e-04, 5.1639e-04],\n",
      "        [6.2858e-04, 3.5443e-03, 4.3677e-04, 6.0420e-04, 1.2896e-03, 2.7033e-05,\n",
      "         5.9856e-04, 2.0601e-03],\n",
      "        [8.4000e-04, 4.1455e-03, 3.0024e-04, 8.3613e-04, 1.7542e-03, 2.1305e-05,\n",
      "         6.9719e-04, 2.4369e-03],\n",
      "        [3.5800e-04, 1.7973e-03, 1.8957e-04, 2.7365e-04, 7.3309e-04, 1.2409e-05,\n",
      "         3.0002e-04, 1.0961e-03],\n",
      "        [1.4247e-03, 1.6285e-02, 7.5320e-04, 1.0628e-03, 3.1047e-03, 9.0900e-05,\n",
      "         2.6805e-03, 9.6031e-03],\n",
      "        [1.8060e-04, 1.1842e-03, 1.2502e-04, 1.5104e-04, 3.7101e-04, 8.7416e-06,\n",
      "         1.9853e-04, 7.4671e-04],\n",
      "        [1.3577e-04, 1.1198e-03, 1.1731e-04, 1.1273e-04, 2.8671e-04, 9.8453e-06,\n",
      "         1.9055e-04, 7.1973e-04]], device='cuda:0')}, 6: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0025, -0.0078, -0.0031,  0.0046,  0.0025,  0.0027,  0.0086,  0.0067,\n",
      "         0.0041, -0.0031, -0.0007, -0.0035], device='cuda:0'), 'exp_avg_sq': tensor([0.0015, 0.0047, 0.0023, 0.0077, 0.0068, 0.0047, 0.0163, 0.0042, 0.0034,\n",
      "        0.0022, 0.0013, 0.0137], device='cuda:0')}, 7: {'step': tensor(19100.), 'exp_avg': tensor([[-6.5004e-04, -2.2397e-04,  6.8892e-04,  7.8282e-04,  4.0434e-04,\n",
      "         -3.1920e-03,  3.3602e-04, -6.5473e-04],\n",
      "        [ 1.0544e-03, -6.7529e-04,  1.2065e-03, -3.8271e-04,  2.0062e-04,\n",
      "         -3.9942e-03, -4.7532e-04, -1.5818e-03],\n",
      "        [-4.8444e-04,  6.1128e-04,  7.1168e-04,  7.6093e-04,  5.8694e-04,\n",
      "          2.6354e-03, -8.2138e-04, -6.2895e-04],\n",
      "        [-1.1718e-03, -8.3687e-04,  1.7189e-03,  1.5690e-03,  8.5208e-04,\n",
      "         -9.2678e-03,  9.0832e-04, -1.7584e-03],\n",
      "        [ 2.7221e-04, -9.4635e-04, -4.3161e-04, -4.9023e-04, -4.8570e-04,\n",
      "         -6.6406e-03,  1.3427e-03,  2.3502e-04],\n",
      "        [ 1.6890e-03, -2.1706e-04, -1.9602e-03, -2.2381e-03, -1.3382e-03,\n",
      "          4.9857e-03,  2.2116e-05,  1.9594e-03],\n",
      "        [-2.7914e-04, -1.3694e-03, -3.9571e-04, -9.6687e-05, -4.6337e-04,\n",
      "         -1.0148e-02,  2.1823e-03,  2.7540e-04],\n",
      "        [-1.5358e-03,  1.0836e-03, -5.9070e-04,  1.0704e-03,  2.8876e-04,\n",
      "          5.6242e-03, -7.1452e-05,  1.0574e-03],\n",
      "        [-1.4947e-03, -4.4397e-05,  1.1813e-03,  1.6874e-03,  8.5391e-04,\n",
      "         -4.1439e-03,  4.3422e-04, -1.0450e-03],\n",
      "        [-6.6894e-05,  6.7689e-04,  7.4987e-05,  1.6148e-04,  2.1390e-04,\n",
      "          4.5374e-03, -8.5627e-04,  1.2013e-05],\n",
      "        [ 1.1482e-03, -9.4339e-04, -2.1495e-04, -1.1099e-03, -5.6484e-04,\n",
      "         -4.0317e-03,  5.2011e-04, -5.0760e-05],\n",
      "        [ 1.6918e-03, -2.2236e-03,  6.7917e-04, -1.2763e-03, -5.5210e-04,\n",
      "         -1.3791e-02,  1.4099e-03, -1.3537e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.0932e-04, 4.6631e-05, 5.7572e-05, 9.8421e-05, 2.6344e-05, 5.7400e-04,\n",
      "         2.2277e-05, 7.5435e-05],\n",
      "        [8.0184e-04, 1.9433e-04, 3.3770e-04, 8.5425e-04, 2.1979e-04, 2.1028e-03,\n",
      "         7.4687e-05, 2.8977e-04],\n",
      "        [1.7245e-04, 5.3470e-05, 6.4460e-05, 1.6863e-04, 4.0365e-05, 7.3650e-04,\n",
      "         2.7207e-05, 5.3795e-05],\n",
      "        [4.8004e-04, 2.1348e-04, 2.9825e-04, 4.5077e-04, 1.3102e-04, 3.0790e-03,\n",
      "         9.7049e-05, 3.9169e-04],\n",
      "        [3.7420e-04, 1.4710e-04, 1.2713e-04, 3.3498e-04, 7.9015e-05, 2.2441e-03,\n",
      "         7.3782e-05, 1.5105e-04],\n",
      "        [4.9642e-04, 1.6183e-04, 3.7063e-04, 5.6291e-04, 1.8168e-04, 2.3432e-03,\n",
      "         8.6853e-05, 3.9744e-04],\n",
      "        [9.7351e-04, 3.4943e-04, 3.1996e-04, 8.8427e-04, 1.9871e-04, 4.9634e-03,\n",
      "         1.4671e-04, 3.5871e-04],\n",
      "        [7.0181e-04, 1.8749e-04, 2.5721e-04, 7.1728e-04, 1.7802e-04, 2.0403e-03,\n",
      "         7.5106e-05, 2.3854e-04],\n",
      "        [2.7049e-04, 1.1496e-04, 1.3480e-04, 2.4268e-04, 6.3339e-05, 1.2985e-03,\n",
      "         5.4573e-05, 1.7414e-04],\n",
      "        [1.2892e-04, 4.8089e-05, 4.4811e-05, 1.1569e-04, 2.6255e-05, 6.9526e-04,\n",
      "         2.0136e-05, 5.3338e-05],\n",
      "        [1.0833e-04, 4.1195e-05, 3.1423e-05, 9.7491e-05, 2.2622e-05, 5.5675e-04,\n",
      "         2.0410e-05, 3.6952e-05],\n",
      "        [1.2689e-03, 4.4937e-04, 5.2695e-04, 1.2417e-03, 3.2579e-04, 6.2828e-03,\n",
      "         2.0059e-04, 6.0952e-04]], device='cuda:0')}, 8: {'step': tensor(19100.), 'exp_avg': tensor([-0.0006,  0.0018, -0.0077,  0.0004, -0.0008,  0.0021,  0.0006,  0.0009,\n",
      "         0.0017,  0.0002, -0.0140, -0.0018], device='cuda:0'), 'exp_avg_sq': tensor([1.0146e-04, 4.5124e-03, 4.8075e-03, 1.0522e-04, 1.8944e-02, 1.1001e-03,\n",
      "        1.2391e-04, 4.4536e-05, 2.5731e-03, 5.8821e-05, 1.9582e-02, 7.9997e-04],\n",
      "       device='cuda:0')}, 9: {'step': tensor(19100.), 'exp_avg': tensor([[-1.4720e-04, -5.8025e-04, -6.9509e-04,  8.1254e-05,  6.4078e-04,\n",
      "          2.4823e-04,  8.8830e-04,  2.4264e-04,  5.0998e-04, -6.4980e-04,\n",
      "          9.1774e-07, -4.4691e-04],\n",
      "        [-7.0598e-04, -5.7630e-04, -3.2552e-03,  2.2730e-03,  3.9237e-03,\n",
      "         -1.8744e-03,  5.1248e-03, -8.9304e-04,  2.5278e-03, -3.3546e-03,\n",
      "          1.9818e-03,  1.4312e-03],\n",
      "        [ 5.2078e-04,  3.1473e-03,  2.9002e-03,  1.9324e-04, -2.1751e-03,\n",
      "         -2.0908e-03, -3.5826e-03, -1.7723e-03, -2.0026e-03,  2.6311e-03,\n",
      "          5.8263e-04,  2.9405e-03],\n",
      "        [ 9.6340e-05,  7.6275e-05, -3.0202e-04,  7.5654e-04,  2.8764e-04,\n",
      "         -6.1032e-04,  4.8696e-04, -1.5761e-04,  7.3340e-04, -3.6073e-04,\n",
      "          2.0098e-04,  4.0453e-04],\n",
      "        [-1.2336e-03,  1.1875e-03, -4.6077e-03,  4.4966e-03,  6.6639e-03,\n",
      "         -5.2631e-03,  8.0646e-03, -3.2558e-03,  3.2816e-03, -4.9912e-03,\n",
      "          4.6996e-03,  5.1876e-03],\n",
      "        [-7.8163e-05,  1.4360e-04, -9.4846e-04,  1.2684e-03,  1.1642e-03,\n",
      "         -1.1738e-03,  1.6153e-03, -5.1412e-04,  1.1175e-03, -1.0483e-03,\n",
      "          7.9575e-04,  9.8276e-04],\n",
      "        [ 1.4367e-05,  4.8456e-04,  5.4642e-04, -3.0785e-04, -4.0811e-04,\n",
      "         -7.7745e-05, -6.5549e-04, -2.4793e-04, -7.0204e-04,  5.2475e-04,\n",
      "          8.8904e-05,  3.6004e-04],\n",
      "        [-4.6682e-05, -3.4468e-04, -4.4513e-04,  1.9099e-04,  3.6916e-04,\n",
      "          6.3359e-05,  5.9122e-04,  1.4954e-04,  4.3682e-04, -4.2978e-04,\n",
      "          9.8764e-06, -2.1494e-04],\n",
      "        [ 8.0812e-04,  3.8922e-04,  1.2628e-03,  6.6213e-04, -1.9096e-03,\n",
      "         -3.0963e-04, -2.0292e-03,  3.5673e-04,  5.7377e-04,  1.1776e-03,\n",
      "         -9.2627e-04, -6.9380e-05],\n",
      "        [ 5.9611e-05, -8.2164e-05,  5.6353e-05,  1.0191e-06, -1.5266e-04,\n",
      "          9.5580e-05, -1.3619e-04,  1.1931e-04,  7.8735e-05,  6.3195e-05,\n",
      "         -1.4397e-04, -1.5504e-04],\n",
      "        [-2.3785e-03, -1.3096e-03, -4.4522e-03, -9.4368e-04,  6.4550e-03,\n",
      "          1.5645e-04,  6.8896e-03, -1.2084e-03, -3.7342e-04, -4.2414e-03,\n",
      "          2.9319e-03,  4.6394e-04],\n",
      "        [ 1.4603e-04,  8.3548e-04,  2.9509e-08,  1.1811e-03,  2.6914e-04,\n",
      "         -1.4708e-03,  2.6197e-04, -7.6016e-04,  6.1554e-04, -1.4682e-04,\n",
      "          6.3371e-04,  1.4053e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.6730e-06, 1.2705e-05, 1.2600e-05, 1.0296e-05, 1.4036e-05, 1.3069e-05,\n",
      "         2.2726e-05, 6.5114e-06, 1.4436e-05, 1.1244e-05, 6.3467e-06, 1.8057e-05],\n",
      "        [5.6468e-05, 4.0044e-04, 4.2958e-04, 3.7847e-04, 5.0958e-04, 4.2337e-04,\n",
      "         8.3381e-04, 2.2763e-04, 5.8225e-04, 4.0993e-04, 2.5071e-04, 6.0191e-04],\n",
      "        [9.1573e-05, 5.3809e-04, 6.1795e-04, 3.9258e-04, 7.1845e-04, 3.4992e-04,\n",
      "         1.1535e-03, 2.9238e-04, 9.5489e-04, 5.8049e-04, 3.1395e-04, 6.4691e-04],\n",
      "        [1.5657e-06, 7.9199e-06, 9.5186e-06, 7.9332e-06, 1.1818e-05, 8.4996e-06,\n",
      "         1.8649e-05, 4.1303e-06, 1.1578e-05, 8.7550e-06, 4.4319e-06, 1.0654e-05],\n",
      "        [2.2404e-04, 1.2231e-03, 1.6187e-03, 1.7429e-03, 1.9205e-03, 1.5901e-03,\n",
      "         3.2002e-03, 6.1559e-04, 2.1972e-03, 1.5696e-03, 7.1104e-04, 1.8324e-03],\n",
      "        [1.6279e-05, 1.5329e-04, 1.2560e-04, 1.2803e-04, 1.3437e-04, 1.7192e-04,\n",
      "         2.2725e-04, 8.3528e-05, 1.6565e-04, 1.1527e-04, 7.3821e-05, 2.3131e-04],\n",
      "        [1.0779e-06, 7.2560e-06, 9.6045e-06, 8.4105e-06, 1.0985e-05, 8.2442e-06,\n",
      "         1.8533e-05, 3.5838e-06, 1.2082e-05, 9.2658e-06, 3.5610e-06, 9.6233e-06],\n",
      "        [4.3061e-07, 2.9364e-06, 4.2234e-06, 3.5094e-06, 4.6041e-06, 3.0733e-06,\n",
      "         8.0784e-06, 1.3010e-06, 5.4009e-06, 4.1000e-06, 1.2817e-06, 3.5617e-06],\n",
      "        [2.9299e-05, 1.3818e-04, 2.0219e-04, 1.1353e-04, 2.5640e-04, 1.1987e-04,\n",
      "         4.1908e-04, 7.0361e-05, 2.0227e-04, 1.8969e-04, 8.8016e-05, 1.6724e-04],\n",
      "        [7.6993e-07, 4.6638e-06, 6.0469e-06, 5.3815e-06, 7.1410e-06, 5.3630e-06,\n",
      "         1.1787e-05, 2.3797e-06, 7.5052e-06, 5.7570e-06, 2.6149e-06, 6.5494e-06],\n",
      "        [2.0248e-04, 1.7523e-03, 1.9186e-03, 1.1564e-03, 1.9880e-03, 1.5479e-03,\n",
      "         3.6288e-03, 7.9976e-04, 1.8780e-03, 1.7540e-03, 6.4765e-04, 2.0650e-03],\n",
      "        [1.3985e-05, 5.3759e-05, 8.6776e-05, 9.0433e-05, 1.0462e-04, 6.6755e-05,\n",
      "         1.6776e-04, 2.2927e-05, 1.2282e-04, 8.2680e-05, 2.7109e-05, 6.7612e-05]],\n",
      "       device='cuda:0')}, 10: {'step': tensor(19100.), 'exp_avg': tensor([-0.0013,  0.0022,  0.0015,  0.0004,  0.0045,  0.0031, -0.0059, -0.0008,\n",
      "         0.0120, -0.0028,  0.0041,  0.0002,  0.0029,  0.0005, -0.0006,  0.0048],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([3.1269e-04, 1.5741e-03, 6.2722e-03, 8.3682e-03, 1.5502e-03, 8.5933e-03,\n",
      "        2.0900e-03, 4.1389e-05, 6.9792e-03, 8.8151e-04, 1.5007e-03, 8.9508e-05,\n",
      "        5.6866e-04, 4.8910e-04, 1.8910e-04, 1.0880e-02], device='cuda:0')}, 11: {'step': tensor(19100.), 'exp_avg': tensor([[-1.3377e-05, -1.5288e-04,  9.2174e-05,  3.1889e-05, -1.7580e-04,\n",
      "         -9.4624e-05, -2.0960e-05, -1.6208e-05,  3.1779e-05, -3.9155e-06,\n",
      "          6.8469e-05, -1.4397e-05],\n",
      "        [ 6.8464e-05,  1.6240e-03, -9.9273e-05, -3.2515e-04,  3.2146e-03,\n",
      "          1.3466e-03,  2.1720e-04, -2.9109e-06, -1.3849e-03,  3.0589e-05,\n",
      "          2.2331e-03,  1.3444e-03],\n",
      "        [ 2.0813e-04,  5.3842e-03,  7.8188e-04, -4.9930e-04,  1.0201e-02,\n",
      "          3.4043e-03,  6.7770e-04, -2.9213e-05, -1.4425e-03, -1.8878e-04,\n",
      "          5.0221e-04,  3.2277e-03],\n",
      "        [-4.2461e-05, -1.4906e-03,  4.8167e-05,  7.0549e-04, -4.3227e-03,\n",
      "         -2.1541e-03, -1.8082e-04,  8.1435e-05,  4.0175e-03, -2.4168e-04,\n",
      "         -8.5848e-03, -2.9579e-03],\n",
      "        [ 9.4347e-05,  1.7576e-03, -1.4015e-03, -7.0180e-05,  1.2600e-03,\n",
      "          7.3603e-04,  3.5478e-04,  8.9275e-05,  5.7737e-04, -1.6202e-04,\n",
      "         -3.1439e-03, -7.2659e-04],\n",
      "        [ 2.4744e-04,  6.3298e-03,  6.5365e-04, -6.8409e-04,  1.1986e-02,\n",
      "          4.1772e-03,  8.1261e-04, -2.7004e-05, -2.2101e-03, -1.7303e-04,\n",
      "          1.6786e-03,  3.9421e-03],\n",
      "        [-7.4662e-05, -1.2438e-03,  1.0738e-03,  2.5122e-04, -1.3459e-03,\n",
      "         -9.1367e-04, -2.3622e-04, -6.9569e-05,  7.1437e-04, -3.8157e-06,\n",
      "         -3.9976e-04, -1.5458e-04],\n",
      "        [-1.9162e-05, -4.2421e-04,  2.2238e-04,  7.5951e-06, -4.0273e-04,\n",
      "         -1.7498e-04, -8.0541e-05, -1.2591e-05, -1.5898e-04,  3.9530e-05,\n",
      "          7.4963e-04,  1.0631e-04],\n",
      "        [ 4.3350e-05, -3.5318e-04, -1.9773e-03, -3.1586e-04, -1.8974e-03,\n",
      "          2.2686e-04,  6.1295e-05,  1.3752e-04, -1.2057e-03,  1.6030e-04,\n",
      "          2.0829e-03, -6.5738e-04],\n",
      "        [-8.2443e-05, -1.8419e-03,  3.6809e-04,  1.9492e-04, -2.8583e-03,\n",
      "         -1.1418e-03, -2.7875e-04, -2.7689e-05,  4.3742e-04,  6.6851e-05,\n",
      "          2.7576e-04, -6.4417e-04],\n",
      "        [ 1.4295e-05, -2.6634e-04, -1.6807e-03, -3.0980e-05, -2.1099e-03,\n",
      "         -2.2683e-04,  8.7954e-05,  8.4873e-05,  1.8494e-04, -9.5953e-06,\n",
      "         -1.1080e-03, -1.3455e-03],\n",
      "        [ 1.3486e-05,  3.3784e-04, -1.3330e-05, -1.9816e-05,  5.3746e-04,\n",
      "          1.8282e-04,  4.8292e-05,  1.1015e-06,  9.9071e-07, -1.8282e-05,\n",
      "         -2.1634e-04,  1.0503e-04],\n",
      "        [ 3.0391e-05,  4.4274e-04, -4.0855e-04, -1.2252e-04,  5.2893e-04,\n",
      "          3.8399e-04,  8.3864e-05,  2.7767e-05, -4.2171e-04,  1.5081e-05,\n",
      "          5.1768e-04,  1.4232e-04],\n",
      "        [-4.8028e-05, -1.1965e-03,  8.5309e-04, -2.1100e-05, -8.5130e-04,\n",
      "         -4.0546e-04, -2.5068e-04, -2.8745e-05, -6.3264e-04,  1.5566e-04,\n",
      "          2.6587e-03,  6.0181e-04],\n",
      "        [ 6.7768e-06,  2.0459e-04, -9.4892e-06,  3.9173e-05,  1.9123e-04,\n",
      "          1.1161e-05,  3.4204e-05,  1.4726e-06,  2.8113e-04, -4.1069e-05,\n",
      "         -7.9397e-04, -1.1885e-04],\n",
      "        [ 1.6953e-04,  4.1116e-03, -4.1601e-04, -8.7900e-04,  8.1888e-03,\n",
      "          3.5473e-03,  5.6757e-04, -2.0152e-05, -3.9343e-03,  9.0178e-05,\n",
      "          6.5854e-03,  3.5434e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[2.8989e-07, 6.1395e-05, 5.9264e-05, 1.3013e-06, 1.3556e-04, 2.4727e-05,\n",
      "         3.1692e-06, 7.2914e-07, 2.2430e-05, 5.2335e-07, 1.4429e-04, 3.6354e-05],\n",
      "        [7.8119e-07, 1.1746e-04, 1.1019e-04, 3.3088e-06, 4.1034e-04, 5.6302e-05,\n",
      "         4.8431e-06, 1.1589e-06, 6.6216e-05, 1.2482e-06, 3.8298e-04, 1.1233e-04],\n",
      "        [4.3074e-06, 8.4385e-04, 6.0636e-04, 1.8637e-05, 3.1086e-03, 3.1594e-04,\n",
      "         2.4238e-05, 6.4410e-06, 3.6745e-04, 6.8206e-06, 2.3063e-03, 6.4983e-04],\n",
      "        [4.3301e-06, 7.7688e-04, 8.3874e-04, 2.5875e-05, 1.9689e-03, 3.8132e-04,\n",
      "         4.0992e-05, 8.9447e-06, 4.7027e-04, 7.2085e-06, 2.3628e-03, 5.5325e-04],\n",
      "        [9.7881e-07, 2.0168e-04, 2.6031e-04, 4.7897e-06, 5.1103e-04, 7.9352e-05,\n",
      "         1.1183e-05, 2.4245e-06, 9.3846e-05, 1.9490e-06, 6.7030e-04, 1.6852e-04],\n",
      "        [5.2048e-06, 1.0158e-03, 7.5236e-04, 2.1997e-05, 3.7947e-03, 3.9566e-04,\n",
      "         3.0291e-05, 7.7390e-06, 4.3339e-04, 8.3548e-06, 2.7092e-03, 8.2036e-04],\n",
      "        [1.3334e-06, 3.5900e-04, 2.3815e-04, 4.6687e-06, 7.8229e-04, 1.1609e-04,\n",
      "         1.3254e-05, 2.9383e-06, 1.1315e-04, 3.3385e-06, 9.8855e-04, 1.7780e-04],\n",
      "        [4.1429e-08, 5.8712e-06, 4.2421e-06, 1.2883e-07, 1.6673e-05, 2.2092e-06,\n",
      "         2.2847e-07, 5.8226e-08, 2.4876e-06, 8.2726e-08, 1.7004e-05, 4.1234e-06],\n",
      "        [5.3858e-06, 1.7910e-03, 8.6547e-04, 1.9496e-05, 3.7751e-03, 5.3360e-04,\n",
      "         5.7733e-05, 1.2817e-05, 4.6561e-04, 1.5573e-05, 4.2073e-03, 6.3584e-04],\n",
      "        [5.5977e-07, 8.5263e-05, 1.0351e-04, 2.2110e-06, 2.9905e-04, 3.6236e-05,\n",
      "         4.0572e-06, 9.7952e-07, 4.2926e-05, 9.1075e-07, 2.7925e-04, 8.6463e-05],\n",
      "        [1.1883e-06, 4.0726e-04, 2.0130e-04, 3.8800e-06, 8.6512e-04, 1.2190e-04,\n",
      "         1.3048e-05, 2.8244e-06, 9.2397e-05, 3.3534e-06, 8.5890e-04, 1.4287e-04],\n",
      "        [7.6887e-08, 1.1784e-05, 7.3248e-06, 2.2869e-07, 3.6295e-05, 4.6843e-06,\n",
      "         3.9004e-07, 1.1853e-07, 4.5799e-06, 1.3323e-07, 3.1590e-05, 8.4660e-06],\n",
      "        [3.9500e-07, 8.5545e-05, 5.8148e-05, 1.4400e-06, 2.1683e-04, 3.0715e-05,\n",
      "         3.3094e-06, 7.8411e-07, 3.1466e-05, 9.0057e-07, 2.5171e-04, 5.3606e-05],\n",
      "        [3.9027e-07, 6.8840e-05, 5.2286e-05, 1.8758e-06, 2.3425e-04, 2.4225e-05,\n",
      "         2.2121e-06, 6.0461e-07, 3.7897e-05, 6.5806e-07, 2.3179e-04, 4.8343e-05],\n",
      "        [2.1381e-07, 5.8761e-05, 2.9924e-05, 7.3545e-07, 1.3569e-04, 1.8805e-05,\n",
      "         1.8564e-06, 4.1541e-07, 1.5267e-05, 5.0816e-07, 1.2106e-04, 2.2830e-05],\n",
      "        [5.0277e-06, 8.1249e-04, 7.7095e-04, 2.3634e-05, 2.8139e-03, 3.8867e-04,\n",
      "         3.4049e-05, 8.2346e-06, 4.6820e-04, 8.0308e-06, 2.7548e-03, 7.7772e-04]],\n",
      "       device='cuda:0')}, 12: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0020,  0.0007, -0.0012, -0.0113,  0.0035, -0.0005, -0.0078, -0.0005,\n",
      "         0.0012,  0.0002,  0.0029, -0.0062, -0.0085,  0.0010,  0.0005,  0.0007],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([1.5528e-04, 8.5536e-03, 2.0491e-04, 1.9002e-02, 1.0036e-03, 8.2482e-04,\n",
      "        4.3499e-03, 1.1158e-04, 5.3271e-05, 5.6804e-05, 2.3219e-03, 4.1750e-03,\n",
      "        1.0580e-02, 1.1145e-04, 1.4361e-04, 4.7288e-05], device='cuda:0')}, 13: {'step': tensor(19100.), 'exp_avg': tensor([[ 7.8501e-05, -1.2412e-04, -2.7750e-04, -1.9444e-04, -2.2404e-04,\n",
      "         -1.6276e-04,  1.6649e-04, -4.1116e-05, -9.1187e-05,  1.3932e-04,\n",
      "         -5.6490e-05, -2.3401e-05, -1.0245e-04,  1.2243e-04,  8.1034e-05,\n",
      "         -1.6315e-04],\n",
      "        [-3.7553e-03, -5.9042e-03, -3.7687e-03,  5.8665e-03, -2.7213e-03,\n",
      "         -7.9318e-03,  6.9069e-05,  1.0176e-05,  1.8731e-03,  1.2197e-03,\n",
      "          3.7868e-04,  1.1788e-04, -2.3990e-03,  1.9589e-03, -9.5112e-04,\n",
      "         -7.9295e-03],\n",
      "        [ 4.0274e-04,  1.2117e-03,  9.5064e-04, -9.6104e-04,  8.6517e-04,\n",
      "          1.5595e-03, -5.3298e-04,  6.0973e-05,  1.6995e-04, -4.3525e-04,\n",
      "          1.9781e-04, -4.6777e-06,  6.0468e-04, -4.5102e-04,  5.4579e-05,\n",
      "          1.7578e-03],\n",
      "        [ 4.7822e-03,  5.1723e-03,  9.4259e-04, -8.5897e-03,  1.1834e-03,\n",
      "          6.3869e-03,  2.9087e-04,  1.2259e-04, -1.0173e-03, -7.7293e-05,\n",
      "         -5.2937e-04, -4.1200e-04,  1.8932e-03, -8.4974e-04,  1.5749e-03,\n",
      "          7.7586e-03],\n",
      "        [ 3.0814e-05,  2.4790e-04, -2.1047e-04, -9.0184e-04,  1.2197e-04,\n",
      "          2.0678e-04, -4.2344e-04, -9.4526e-06,  5.3943e-04, -9.0584e-06,\n",
      "          2.7494e-04, -6.2900e-05,  1.3776e-04,  9.4094e-05,  1.2162e-04,\n",
      "          6.1167e-04],\n",
      "        [-5.8283e-04, -1.5887e-04, -7.1160e-04, -9.0541e-04,  2.4597e-04,\n",
      "         -6.0157e-04, -1.3443e-03,  1.2898e-04,  1.8976e-03, -4.2464e-05,\n",
      "          7.9877e-04, -9.5324e-05,  1.3642e-04,  3.4269e-04, -2.7268e-05,\n",
      "          5.3106e-04],\n",
      "        [-7.7065e-04,  4.4704e-04, -1.1726e-03, -2.9022e-03,  9.8319e-04,\n",
      "         -3.9057e-04, -3.2312e-03,  3.8881e-04,  4.3213e-03, -2.7721e-04,\n",
      "          1.7940e-03, -2.3436e-04,  6.7554e-04,  5.3555e-04,  2.6152e-05,\n",
      "          2.4079e-03],\n",
      "        [ 2.7842e-04, -1.5001e-04,  2.2832e-06,  3.7337e-04, -3.2938e-04,\n",
      "         -4.8852e-05,  7.0231e-04, -5.2487e-05, -8.2066e-04,  1.5468e-04,\n",
      "         -4.0049e-04,  2.3121e-05, -1.7497e-04, -3.5190e-05,  4.6431e-05,\n",
      "         -5.0140e-04],\n",
      "        [-3.2078e-05, -3.3560e-05,  1.0247e-05,  7.4373e-05, -2.5929e-05,\n",
      "         -2.3820e-05,  4.9209e-05, -1.9360e-05, -6.1659e-05,  7.6835e-06,\n",
      "         -1.2551e-05,  4.1171e-06, -2.6092e-05,  1.1270e-05, -4.1344e-06,\n",
      "         -7.8071e-05],\n",
      "        [-2.4587e-04, -1.5307e-04, -4.4504e-05,  2.1196e-04,  4.8380e-05,\n",
      "         -2.3060e-04, -2.0515e-04,  1.7138e-05,  2.7577e-04, -3.6663e-05,\n",
      "          1.2775e-04,  4.9743e-06, -2.3132e-05,  3.7670e-05, -6.9968e-05,\n",
      "         -1.4260e-04],\n",
      "        [ 3.5449e-05, -1.2513e-03,  2.8834e-04,  3.1136e-03, -1.1454e-03,\n",
      "         -9.6880e-04,  2.4081e-03, -2.4624e-04, -2.8993e-03,  3.9255e-04,\n",
      "         -1.3047e-03,  1.9588e-04, -8.6676e-04, -1.0859e-04, -1.8464e-04,\n",
      "         -3.0128e-03],\n",
      "        [ 3.4684e-03,  2.4461e-03,  2.1792e-03, -1.0380e-03, -1.7737e-04,\n",
      "          4.0409e-03,  3.6094e-03, -2.3369e-04, -5.2752e-03,  1.0347e-04,\n",
      "         -2.3113e-03,  7.4481e-05,  4.0313e-04, -1.2439e-03,  6.8560e-04,\n",
      "          1.7104e-03],\n",
      "        [-1.4246e-03, -5.0099e-03, -2.7256e-03,  6.1390e-03, -3.4692e-03,\n",
      "         -6.1255e-03,  3.2330e-03, -1.4633e-04, -2.2858e-03,  1.5925e-03,\n",
      "         -1.5943e-03,  2.0765e-04, -2.5468e-03,  1.3069e-03, -5.5726e-04,\n",
      "         -8.0597e-03],\n",
      "        [-5.7487e-04,  2.1022e-04,  1.4904e-04, -3.1300e-04,  6.1940e-04,\n",
      "          7.5915e-05, -1.1790e-03,  8.7623e-05,  1.3147e-03, -3.2190e-04,\n",
      "          6.6567e-04, -1.6983e-05,  2.9292e-04, -1.5385e-05, -1.3532e-04,\n",
      "          7.1833e-04],\n",
      "        [-6.4963e-04,  2.3781e-04,  2.7292e-04, -1.7948e-04,  7.1950e-04,\n",
      "          1.1454e-04, -1.2658e-03,  9.8799e-05,  1.3686e-03, -3.8686e-04,\n",
      "          7.0378e-04, -1.1316e-06,  3.3060e-04, -6.1776e-05, -1.8317e-04,\n",
      "          7.5016e-04],\n",
      "        [-1.3227e-05,  9.9804e-05, -3.3092e-05, -2.7626e-04,  8.6702e-05,\n",
      "          8.3594e-05, -1.9120e-04,  8.5567e-06,  2.3154e-04, -2.6890e-05,\n",
      "          1.1463e-04, -1.9979e-05,  6.4745e-05,  1.4541e-05,  2.4576e-05,\n",
      "          2.4322e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[1.3285e-05, 1.4920e-05, 7.1668e-06, 3.1551e-05, 7.2965e-06, 2.1501e-05,\n",
      "         2.1373e-05, 2.3202e-07, 2.9663e-05, 3.0751e-06, 5.9388e-06, 2.0085e-07,\n",
      "         2.7347e-06, 1.0428e-06, 1.3127e-06, 3.1971e-05],\n",
      "        [6.7661e-04, 1.0287e-03, 6.9378e-04, 1.2486e-03, 3.2627e-04, 1.7094e-03,\n",
      "         1.1030e-03, 1.1198e-05, 1.8972e-03, 1.3719e-04, 3.3388e-04, 1.0693e-05,\n",
      "         1.5801e-04, 9.2987e-05, 5.2629e-05, 1.6713e-03],\n",
      "        [1.5980e-05, 1.8323e-05, 9.7011e-06, 2.8881e-05, 9.6471e-06, 2.7125e-05,\n",
      "         2.8519e-05, 3.0043e-07, 4.0736e-05, 4.1946e-06, 8.2493e-06, 2.6321e-07,\n",
      "         3.4517e-06, 1.3529e-06, 1.5069e-06, 3.6247e-05],\n",
      "        [1.2248e-03, 1.4545e-03, 7.8568e-04, 2.2562e-03, 6.3250e-04, 2.2180e-03,\n",
      "         1.9872e-03, 1.5958e-05, 3.0053e-03, 2.5500e-04, 6.3449e-04, 1.5753e-05,\n",
      "         2.5676e-04, 1.0797e-04, 1.0521e-04, 2.7513e-03],\n",
      "        [1.3881e-04, 1.0840e-04, 4.6457e-05, 2.7109e-04, 5.4928e-05, 1.5753e-04,\n",
      "         1.7218e-04, 1.5221e-06, 2.4127e-04, 2.4014e-05, 4.9323e-05, 1.2587e-06,\n",
      "         1.7375e-05, 6.9185e-06, 1.3555e-05, 2.1596e-04],\n",
      "        [8.0248e-05, 9.0032e-05, 6.3894e-05, 2.0633e-04, 3.7356e-05, 1.4105e-04,\n",
      "         1.3699e-04, 1.7846e-06, 2.2235e-04, 1.6409e-05, 4.1235e-05, 1.8794e-06,\n",
      "         1.5155e-05, 9.9707e-06, 7.8605e-06, 1.7833e-04],\n",
      "        [7.6061e-04, 5.7138e-04, 3.9219e-04, 8.0320e-04, 2.0076e-04, 1.0483e-03,\n",
      "         1.1825e-03, 8.2927e-06, 2.1107e-03, 7.6470e-05, 4.1278e-04, 9.2864e-06,\n",
      "         6.6049e-05, 5.9709e-05, 5.3495e-05, 8.0997e-04],\n",
      "        [8.1240e-06, 1.1865e-05, 5.1255e-06, 2.0497e-05, 5.2333e-06, 1.6272e-05,\n",
      "         1.4495e-05, 1.9924e-07, 1.9990e-05, 2.2002e-06, 3.9240e-06, 1.4188e-07,\n",
      "         2.2502e-06, 7.4037e-07, 8.1082e-07, 2.4606e-05],\n",
      "        [4.2290e-06, 5.5076e-06, 3.6347e-06, 9.8160e-06, 2.5538e-06, 8.5636e-06,\n",
      "         9.5821e-06, 9.8696e-08, 1.5682e-05, 9.6220e-07, 2.8013e-06, 9.5395e-08,\n",
      "         9.7889e-07, 5.2104e-07, 3.5246e-07, 1.1434e-05],\n",
      "        [5.7641e-06, 6.5756e-06, 3.2702e-06, 1.0062e-05, 2.5438e-06, 9.7845e-06,\n",
      "         8.4687e-06, 1.0901e-07, 1.2845e-05, 1.1660e-06, 2.5447e-06, 9.1647e-08,\n",
      "         1.0986e-06, 4.8401e-07, 5.3373e-07, 1.1542e-05],\n",
      "        [1.7217e-04, 1.7400e-04, 9.3108e-05, 4.0079e-04, 7.0856e-05, 2.6124e-04,\n",
      "         2.5805e-04, 2.7943e-06, 4.0646e-04, 2.9328e-05, 8.1588e-05, 2.9320e-06,\n",
      "         2.7244e-05, 1.4677e-05, 1.6351e-05, 3.4772e-04],\n",
      "        [3.7195e-04, 4.6845e-04, 3.4262e-04, 9.2032e-04, 2.3511e-04, 7.1046e-04,\n",
      "         6.2276e-04, 7.1266e-06, 9.1101e-04, 1.0382e-04, 1.7508e-04, 6.6061e-06,\n",
      "         9.4001e-05, 5.0513e-05, 3.8046e-05, 9.3507e-04],\n",
      "        [1.7143e-03, 1.4126e-03, 7.3276e-04, 2.3217e-03, 5.7831e-04, 2.3392e-03,\n",
      "         2.2954e-03, 1.5937e-05, 3.6479e-03, 2.4117e-04, 7.0951e-04, 1.4050e-05,\n",
      "         1.9096e-04, 9.9520e-05, 1.4244e-04, 2.2666e-03],\n",
      "        [7.7581e-06, 9.9091e-06, 4.7109e-06, 1.6586e-05, 4.1514e-06, 1.4225e-05,\n",
      "         1.2355e-05, 1.4049e-07, 1.8036e-05, 1.7333e-06, 3.6618e-06, 1.2592e-07,\n",
      "         1.7675e-06, 6.9707e-07, 7.4922e-07, 1.9230e-05],\n",
      "        [1.2282e-05, 1.5322e-05, 9.2632e-06, 1.9822e-05, 7.1047e-06, 2.4615e-05,\n",
      "         2.3141e-05, 1.5962e-07, 3.5564e-05, 2.8272e-06, 7.2288e-06, 1.5185e-07,\n",
      "         2.5947e-06, 1.2336e-06, 1.1075e-06, 2.8281e-05],\n",
      "        [4.9676e-06, 6.8920e-06, 3.5622e-06, 9.0302e-06, 2.1589e-06, 1.0665e-05,\n",
      "         7.0420e-06, 8.7863e-08, 1.1178e-05, 1.0019e-06, 2.1735e-06, 6.9677e-08,\n",
      "         1.0684e-06, 4.8817e-07, 4.3008e-07, 1.1382e-05]], device='cuda:0')}, 14: {'step': tensor(19100.), 'exp_avg': tensor([ 2.1905e-04, -7.9814e-03,  3.8145e-04,  1.3181e-02, -1.2371e-03,\n",
      "         5.7689e-03,  6.4593e-05, -5.8813e-04, -1.2000e-03,  1.7147e-02,\n",
      "        -1.4081e-03, -3.3330e-03, -1.8765e-03, -4.3501e-04, -1.1322e-03,\n",
      "        -4.4944e-03], device='cuda:0'), 'exp_avg_sq': tensor([9.6050e-04, 5.5520e-03, 2.7005e-02, 7.0079e-03, 3.0380e-04, 4.8938e-03,\n",
      "        2.7320e-04, 6.5911e-04, 6.9032e-04, 1.5857e-02, 3.2161e-03, 7.5391e-04,\n",
      "        1.8378e-03, 3.4900e-04, 7.1534e-05, 1.0180e-03], device='cuda:0')}, 15: {'step': tensor(19100.), 'exp_avg': tensor([[ 4.8298e-04,  7.4684e-04, -9.9364e-05,  6.3548e-04,  2.0101e-04,\n",
      "          5.3289e-04,  1.1730e-03, -1.1649e-05, -2.9992e-05,  2.7448e-04,\n",
      "         -7.4047e-04, -6.7453e-04,  2.6339e-04,  1.1930e-05,  5.1452e-05,\n",
      "         -1.5295e-05],\n",
      "        [-9.5918e-04,  1.4073e-03,  3.6198e-04, -4.0274e-03, -5.4331e-04,\n",
      "         -4.3061e-04, -8.3604e-04,  5.9313e-04, -1.7834e-04, -4.0227e-04,\n",
      "          1.6378e-03,  2.9590e-04,  2.1116e-03,  3.1933e-04, -2.1500e-04,\n",
      "          4.4990e-04],\n",
      "        [ 1.9442e-03, -9.5752e-03, -1.6559e-04,  1.8812e-02,  7.3966e-04,\n",
      "         -1.4261e-04, -1.1990e-04, -2.1639e-03,  1.3105e-03,  1.0239e-03,\n",
      "         -4.1670e-03,  4.0223e-03, -9.5321e-03, -9.5766e-04,  5.4333e-04,\n",
      "         -1.2248e-03],\n",
      "        [-1.8028e-04,  9.2441e-04, -5.8536e-04, -5.9495e-03,  4.4051e-04,\n",
      "         -1.0063e-04, -1.7552e-03,  2.5105e-04, -6.0625e-04, -5.0344e-04,\n",
      "          8.5852e-04, -2.1263e-03, -4.8533e-04,  2.9806e-05,  3.0363e-05,\n",
      "         -8.3553e-05],\n",
      "        [ 1.2784e-04,  4.4026e-04, -1.0860e-04, -1.0736e-03,  1.4284e-04,\n",
      "          2.2895e-04, -4.4967e-05,  1.1255e-04, -1.4111e-04, -1.4832e-05,\n",
      "         -6.3368e-05, -6.1863e-04, -1.2572e-04,  8.2567e-05,  1.0392e-05,\n",
      "          5.7007e-05],\n",
      "        [ 4.7692e-04,  5.4999e-03, -2.3536e-04, -4.9765e-03,  2.2859e-04,\n",
      "          1.2966e-03,  3.2629e-03,  6.2367e-04, -4.5265e-04,  2.9217e-04,\n",
      "         -3.4383e-04, -3.1485e-03,  4.6762e-03,  2.3381e-04, -6.2674e-05,\n",
      "          2.7699e-04],\n",
      "        [-9.3031e-05, -1.1783e-03, -1.1015e-06,  7.2080e-04,  4.7826e-06,\n",
      "         -2.7648e-04, -8.4566e-04, -1.2979e-04,  5.5891e-05, -9.2058e-05,\n",
      "          1.0016e-04,  5.0954e-04, -1.1419e-03, -5.7493e-05,  2.1355e-05,\n",
      "         -7.9949e-05],\n",
      "        [-2.3334e-04,  1.8812e-03, -8.2893e-05, -4.2069e-03,  1.9559e-05,\n",
      "          1.9024e-04, -1.8234e-04,  4.5993e-04, -3.6074e-04, -2.0378e-04,\n",
      "          6.8474e-04, -1.2259e-03,  1.3233e-03,  2.2959e-04, -8.0857e-05,\n",
      "          2.4420e-04],\n",
      "        [ 1.2221e-04, -4.5806e-04, -3.2417e-04, -2.3059e-03,  3.7547e-04,\n",
      "          1.0815e-04, -1.4939e-03,  1.0531e-04, -3.1228e-04, -2.3440e-04,\n",
      "          1.4368e-04, -9.7820e-04, -1.8301e-03,  1.1061e-04,  4.5930e-05,\n",
      "         -2.9567e-06],\n",
      "        [ 1.5122e-03, -2.7521e-03, -6.6521e-04,  6.6341e-03,  9.2308e-04,\n",
      "          5.1465e-04,  9.8000e-04, -1.0712e-03,  2.9248e-04,  5.9088e-04,\n",
      "         -2.5918e-03, -4.1041e-04, -3.8489e-03, -6.0766e-04,  3.7414e-04,\n",
      "         -8.4205e-04],\n",
      "        [-1.1355e-03, -2.7948e-03, -1.6953e-04, -4.6353e-03, -3.8503e-05,\n",
      "         -1.3211e-03, -5.2228e-03,  1.7676e-04, -3.8666e-04, -1.0291e-03,\n",
      "          2.1525e-03,  5.6592e-04, -3.3919e-03,  1.2535e-04, -6.1311e-05,\n",
      "          5.2116e-05],\n",
      "        [-3.3769e-04, -1.4728e-03, -1.5188e-04, -2.1245e-03,  1.2402e-04,\n",
      "         -4.2939e-04, -2.5520e-03,  9.6862e-05, -2.3914e-04, -4.3873e-04,\n",
      "          7.6452e-04,  7.1889e-06, -2.2633e-03,  1.1474e-04, -1.3637e-07,\n",
      "          4.2255e-05],\n",
      "        [ 4.2304e-04, -1.9636e-03,  1.2773e-04,  5.4451e-03, -9.4789e-06,\n",
      "         -2.2000e-05,  6.9888e-04, -5.3106e-04,  4.6245e-04,  3.5265e-04,\n",
      "         -1.0641e-03,  1.3842e-03, -1.3696e-03, -2.3989e-04,  9.8879e-05,\n",
      "         -2.5414e-04],\n",
      "        [-1.6294e-04,  8.2851e-04, -2.5404e-05, -1.9472e-03, -2.7936e-05,\n",
      "          3.5326e-05, -1.4320e-04,  2.1455e-04, -1.6911e-04, -1.1682e-04,\n",
      "          3.8081e-04, -4.7467e-04,  6.5927e-04,  1.0905e-04, -3.9994e-05,\n",
      "          1.2714e-04],\n",
      "        [-4.5991e-05, -1.3377e-05, -7.3098e-06, -4.7248e-04,  6.2387e-06,\n",
      "         -1.5851e-05, -2.5448e-04,  5.4001e-05, -4.6762e-05, -4.6772e-05,\n",
      "          1.1695e-04, -6.4520e-05, -1.4565e-04,  4.0819e-05, -9.0010e-06,\n",
      "          3.4259e-05],\n",
      "        [ 6.5253e-05,  1.4833e-03,  2.6615e-04,  8.0044e-04, -2.4671e-04,\n",
      "          3.3985e-04,  1.7929e-03,  1.3940e-04,  1.3966e-04,  2.7429e-04,\n",
      "         -2.5485e-04,  1.4140e-04,  2.1295e-03,  8.0863e-05, -6.3412e-05,\n",
      "          1.7275e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[1.0766e-05, 1.4275e-04, 3.4943e-06, 3.0498e-04, 7.9175e-06, 1.1936e-05,\n",
      "         1.3290e-04, 3.3826e-06, 2.2846e-06, 6.2657e-06, 3.6242e-05, 3.4372e-05,\n",
      "         2.3558e-04, 1.0438e-06, 1.7585e-06, 1.1138e-06],\n",
      "        [5.7203e-05, 7.3772e-04, 2.5307e-05, 2.1177e-03, 4.6462e-05, 6.4740e-05,\n",
      "         8.1969e-04, 1.9146e-05, 1.4573e-05, 3.7401e-05, 2.1771e-04, 2.0812e-04,\n",
      "         1.2674e-03, 5.7433e-06, 9.0320e-06, 5.7016e-06],\n",
      "        [2.8352e-04, 3.4139e-03, 8.8720e-05, 1.0412e-02, 1.9565e-04, 2.5514e-04,\n",
      "         2.3581e-03, 1.0257e-04, 6.1130e-05, 1.5201e-04, 9.8696e-04, 1.1552e-03,\n",
      "         5.2282e-03, 3.3901e-05, 3.2211e-05, 2.4024e-05],\n",
      "        [1.1258e-04, 1.6451e-03, 6.2797e-05, 4.0342e-03, 1.2769e-04, 1.1715e-04,\n",
      "         1.9814e-03, 3.7296e-05, 2.3452e-05, 7.0169e-05, 3.7010e-04, 4.3828e-04,\n",
      "         3.5891e-03, 1.8553e-05, 2.2174e-05, 1.2733e-05],\n",
      "        [4.5803e-06, 4.1041e-05, 1.1830e-06, 9.8572e-05, 3.2434e-06, 4.8270e-06,\n",
      "         4.1936e-05, 1.1975e-06, 6.8290e-07, 2.2447e-06, 1.4612e-05, 1.0983e-05,\n",
      "         7.3753e-05, 3.2819e-07, 4.8786e-07, 3.1739e-07],\n",
      "        [8.3602e-05, 1.6092e-03, 2.7135e-05, 2.4379e-03, 5.5693e-05, 1.3819e-04,\n",
      "         1.7233e-03, 2.6926e-05, 1.5105e-05, 6.0550e-05, 3.1630e-04, 3.3747e-04,\n",
      "         2.5006e-03, 1.1380e-05, 1.5940e-05, 9.6046e-06],\n",
      "        [3.8183e-06, 5.2950e-05, 1.1929e-06, 1.2778e-04, 2.4093e-06, 5.2461e-06,\n",
      "         4.6521e-05, 1.3149e-06, 8.6067e-07, 2.8103e-06, 1.3801e-05, 1.5548e-05,\n",
      "         7.3183e-05, 3.7857e-07, 4.8038e-07, 3.6275e-07],\n",
      "        [7.6894e-06, 1.3942e-04, 2.8447e-06, 2.9198e-04, 6.1795e-06, 9.3282e-06,\n",
      "         1.0887e-04, 3.3452e-06, 2.0066e-06, 5.2622e-06, 2.7345e-05, 3.8376e-05,\n",
      "         2.1559e-04, 1.1494e-06, 1.3163e-06, 8.6642e-07],\n",
      "        [1.1400e-05, 8.7572e-05, 2.5597e-06, 3.0448e-04, 7.8209e-06, 8.1343e-06,\n",
      "         5.6380e-05, 3.4058e-06, 1.7832e-06, 4.7331e-06, 3.3741e-05, 2.7474e-05,\n",
      "         1.6832e-04, 9.4485e-07, 9.7658e-07, 7.0189e-07],\n",
      "        [1.6118e-04, 2.3037e-03, 8.5800e-05, 6.4800e-03, 1.5164e-04, 1.8548e-04,\n",
      "         2.6876e-03, 5.7400e-05, 4.3666e-05, 1.1087e-04, 6.1660e-04, 6.4699e-04,\n",
      "         4.1965e-03, 1.8711e-05, 2.9610e-05, 1.7698e-05],\n",
      "        [6.7106e-05, 7.4194e-04, 1.4500e-05, 1.4057e-03, 4.0292e-05, 9.0778e-05,\n",
      "         8.2515e-04, 1.5779e-05, 9.1193e-06, 3.5506e-05, 2.2651e-04, 1.9811e-04,\n",
      "         1.1322e-03, 5.0702e-06, 7.1830e-06, 4.5354e-06],\n",
      "        [1.3273e-05, 2.0216e-04, 4.4378e-06, 4.1691e-04, 8.6935e-06, 1.9380e-05,\n",
      "         2.3017e-04, 4.4036e-06, 2.7653e-06, 9.8475e-06, 5.2106e-05, 4.6832e-05,\n",
      "         3.2349e-04, 1.5635e-06, 2.1443e-06, 1.2633e-06],\n",
      "        [2.8319e-05, 3.5192e-04, 9.7898e-06, 8.2973e-04, 2.5385e-05, 2.4436e-05,\n",
      "         2.8709e-04, 9.4438e-06, 5.2154e-06, 1.3914e-05, 8.5005e-05, 9.8058e-05,\n",
      "         6.9496e-04, 4.1445e-06, 3.8293e-06, 2.7496e-06],\n",
      "        [8.6935e-06, 4.1207e-05, 1.8499e-06, 1.8599e-04, 5.7980e-06, 7.6154e-06,\n",
      "         4.3921e-05, 2.1326e-06, 1.0714e-06, 3.9574e-06, 2.6654e-05, 1.8407e-05,\n",
      "         8.7120e-05, 6.4558e-07, 6.2349e-07, 3.9995e-07],\n",
      "        [7.9433e-07, 1.3118e-05, 2.9778e-07, 3.3270e-05, 6.5771e-07, 9.2085e-07,\n",
      "         7.9321e-06, 3.8933e-07, 2.2975e-07, 5.3738e-07, 2.6570e-06, 4.1527e-06,\n",
      "         1.9022e-05, 1.1647e-07, 1.1797e-07, 1.0675e-07],\n",
      "        [1.4435e-05, 1.3786e-04, 6.8903e-06, 5.7041e-04, 1.5344e-05, 1.3693e-05,\n",
      "         1.3751e-04, 4.6062e-06, 3.2410e-06, 8.3605e-06, 4.7073e-05, 7.3001e-05,\n",
      "         2.3975e-04, 1.6670e-06, 1.6761e-06, 1.1567e-06]], device='cuda:0')}, 16: {'step': tensor(19100.), 'exp_avg': tensor([ 4.0894e-03, -7.4082e-04,  7.0714e-03, -7.3181e-03, -1.0994e-02,\n",
      "        -1.1208e-03, -4.8932e-03,  8.9436e-03,  2.2153e-04,  4.9631e-05,\n",
      "         2.5266e-03, -7.5622e-04,  3.8333e-03,  2.4051e-04,  1.3137e-02,\n",
      "        -1.1898e-03,  2.4643e-03,  3.4002e-05,  1.8416e-03, -1.3549e-03,\n",
      "         5.9565e-03,  9.6890e-03,  1.3619e-03,  1.9017e-03,  1.2245e-03,\n",
      "        -2.4985e-03, -5.8842e-03, -2.1983e-03, -8.8155e-04,  4.4364e-03,\n",
      "         6.0257e-04,  6.0118e-03], device='cuda:0'), 'exp_avg_sq': tensor([7.9488e-04, 6.7494e-04, 1.1211e-02, 1.7056e-03, 1.9458e-03, 9.5114e-04,\n",
      "        1.4145e-03, 8.3363e-03, 2.0994e-04, 2.7861e-03, 2.8879e-04, 1.8201e-03,\n",
      "        7.3794e-03, 6.4760e-04, 3.4781e-03, 1.1242e-04, 3.5475e-03, 2.9840e-04,\n",
      "        1.0331e-03, 6.2295e-04, 1.6322e-03, 4.4430e-03, 1.4539e-04, 7.2739e-04,\n",
      "        3.5522e-04, 8.3464e-04, 2.9585e-04, 2.5371e-04, 9.5931e-04, 5.1804e-04,\n",
      "        7.5907e-05, 4.9408e-03], device='cuda:0')}, 17: {'step': tensor(19100.), 'exp_avg': tensor([[ 1.3130e-04, -5.1767e-04,  1.1362e-03,  1.4882e-04, -1.2676e-06,\n",
      "         -4.4870e-04,  1.3645e-04, -1.5833e-04,  1.4971e-04,  8.2801e-04,\n",
      "         -1.0923e-04,  1.2502e-04,  4.1252e-04, -6.6575e-05, -2.9395e-05,\n",
      "         -7.8520e-05],\n",
      "        [-5.1451e-04,  1.9021e-03, -2.1362e-03, -2.2845e-03, -2.7930e-04,\n",
      "          4.8246e-04, -1.7000e-04, -4.4106e-07,  2.1934e-04, -2.6302e-03,\n",
      "          7.5890e-04, -4.0486e-05,  4.1438e-04, -5.5368e-06, -2.4661e-04,\n",
      "          5.2260e-04],\n",
      "        [-1.7441e-03,  3.1466e-03, -9.4751e-03, -2.2872e-04, -7.5306e-04,\n",
      "          4.7739e-03, -1.2464e-03, -3.0716e-04, -9.3524e-04, -5.4630e-03,\n",
      "         -3.0924e-04, -9.8143e-04, -1.3647e-03,  5.9778e-04,  1.6217e-04,\n",
      "          8.6241e-04],\n",
      "        [-1.7266e-04,  1.1070e-03, -1.1682e-03, -1.0027e-03, -6.2905e-05,\n",
      "         -4.2063e-05, -8.9197e-05,  2.8973e-04, -1.0184e-05, -1.4315e-03,\n",
      "          7.1485e-04,  1.6672e-05, -2.4263e-04,  6.2076e-05, -4.5785e-05,\n",
      "          1.6464e-04],\n",
      "        [ 2.1911e-04, -4.9531e-04, -1.0697e-03,  1.7744e-03,  3.3929e-04,\n",
      "          1.0744e-03, -2.1261e-04,  4.3236e-04, -5.2194e-04,  2.5916e-04,\n",
      "         -7.2888e-04, -3.6545e-04, -1.4721e-03,  1.0256e-04,  3.5166e-04,\n",
      "         -2.2544e-04],\n",
      "        [ 3.8708e-04, -1.2654e-03,  2.2614e-03,  1.4035e-03,  1.2621e-04,\n",
      "         -1.3884e-03,  2.3985e-04,  8.6442e-05, -1.1181e-04,  2.1200e-03,\n",
      "          1.7676e-04,  2.7352e-04, -1.6162e-04,  6.9000e-05,  1.2984e-04,\n",
      "         -4.9408e-04],\n",
      "        [-7.1236e-04,  2.8853e-03, -4.0179e-03, -3.2935e-03, -2.2524e-04,\n",
      "          1.7608e-03, -3.6869e-04,  1.1174e-04,  2.4078e-04, -4.3844e-03,\n",
      "          4.7545e-04, -3.5685e-04,  1.9558e-04, -1.0225e-04, -3.1097e-04,\n",
      "          8.9320e-04],\n",
      "        [-1.5414e-03,  2.2903e-03, -6.6678e-03,  4.8955e-04, -8.8179e-04,\n",
      "          2.1731e-03, -8.7758e-04, -3.2375e-04, -9.3644e-04, -3.4254e-03,\n",
      "          9.0875e-04, -3.6342e-04, -9.9666e-04,  7.7514e-04,  1.4914e-04,\n",
      "          3.3616e-04],\n",
      "        [ 1.2788e-04, -3.9583e-04,  3.1123e-04,  3.9998e-04,  9.7033e-05,\n",
      "          1.7759e-04,  8.3613e-06, -1.3505e-05, -2.6732e-06,  4.3814e-04,\n",
      "         -3.7698e-04, -7.2354e-05, -7.1312e-05, -4.8778e-05,  4.7004e-05,\n",
      "         -5.4303e-05],\n",
      "        [ 4.5457e-04, -4.4495e-04,  3.3845e-03, -1.2491e-03,  1.0905e-04,\n",
      "         -2.0512e-03,  5.2169e-04, -7.4563e-06,  5.9641e-04,  1.1874e-03,\n",
      "          5.9984e-04,  5.0184e-04,  1.1285e-03, -2.8053e-04, -2.5663e-04,\n",
      "         -8.7737e-05],\n",
      "        [ 1.0246e-04, -5.2208e-04,  4.2064e-04,  4.5984e-04,  8.0325e-05,\n",
      "          2.3061e-04,  1.5641e-05, -1.1561e-04,  1.4292e-05,  5.8787e-04,\n",
      "         -5.0054e-04, -8.7501e-05,  4.8207e-05, -6.4253e-05,  2.8844e-05,\n",
      "         -4.6114e-05],\n",
      "        [ 9.9818e-04, -2.7763e-03,  4.4353e-03,  2.3094e-03,  5.3955e-04,\n",
      "         -1.2902e-03,  4.6190e-04,  5.1654e-05,  6.1905e-05,  3.9548e-03,\n",
      "         -1.0413e-03,  1.7785e-04, -5.8768e-05, -2.1745e-04,  2.0548e-04,\n",
      "         -6.7572e-04],\n",
      "        [-1.8427e-03,  4.0246e-03, -9.9928e-03, -1.8540e-03, -6.9987e-04,\n",
      "          5.0517e-03, -1.2290e-03, -2.8369e-04, -6.2059e-04, -6.7730e-03,\n",
      "         -1.8955e-04, -1.0894e-03, -9.7253e-04,  3.8798e-04, -3.8521e-05,\n",
      "          1.2133e-03],\n",
      "        [-3.8461e-04,  1.4269e-03, -3.9662e-04, -1.9403e-03, -3.6854e-04,\n",
      "         -1.1190e-03,  7.8428e-05,  6.5368e-06,  2.3402e-04, -1.4309e-03,\n",
      "          1.5242e-03,  3.7485e-04,  6.6820e-04,  8.8945e-05, -2.5462e-04,\n",
      "          2.2461e-04],\n",
      "        [-5.0169e-04,  9.0087e-04,  1.5759e-03, -1.8712e-03, -7.3045e-04,\n",
      "         -2.9419e-03,  3.2999e-04, -4.5947e-04,  3.9722e-04, -4.9847e-05,\n",
      "          2.3979e-03,  8.8773e-04,  1.7120e-03,  2.2188e-04, -4.1735e-04,\n",
      "         -1.8225e-05],\n",
      "        [ 3.3798e-05,  2.3404e-05,  1.3719e-04, -6.6130e-05,  1.5883e-05,\n",
      "         -1.5223e-04,  2.5764e-05,  4.3034e-05,  2.0524e-05,  1.6568e-05,\n",
      "          9.8217e-05,  3.1030e-05, -1.2876e-06, -9.4704e-06, -3.5656e-06,\n",
      "         -1.4604e-05],\n",
      "        [-6.3921e-04,  1.5802e-03, -1.1158e-03, -1.1557e-03, -6.4714e-04,\n",
      "         -1.5364e-03, -4.2251e-05,  7.0753e-06, -1.0778e-04, -1.3817e-03,\n",
      "          2.2356e-03,  5.4964e-04,  3.2637e-04,  4.3418e-04, -1.4720e-04,\n",
      "          1.9172e-05],\n",
      "        [ 5.8127e-05, -2.9242e-05, -3.8676e-04, -1.3123e-04,  1.1346e-04,\n",
      "          7.8852e-04, -7.6447e-05, -3.1884e-05,  9.7316e-05, -2.6793e-04,\n",
      "         -6.1389e-04, -2.2165e-04,  5.4801e-06, -1.2805e-04,  6.4302e-07,\n",
      "          1.4047e-04],\n",
      "        [-6.3718e-05,  6.6726e-04, -9.7082e-04, -1.6493e-03,  1.2189e-04,\n",
      "          1.4953e-03, -8.9080e-05, -1.6029e-04,  4.0262e-04, -1.4717e-03,\n",
      "         -8.9837e-04, -3.6845e-04,  5.3933e-04, -3.4979e-04, -2.1056e-04,\n",
      "          5.3912e-04],\n",
      "        [-3.9708e-04,  1.3877e-03, -8.5500e-04, -1.6252e-03, -2.9860e-04,\n",
      "         -6.9082e-04,  1.3457e-05,  3.5101e-05,  1.1495e-04, -1.5305e-03,\n",
      "          1.2876e-03,  2.4407e-04,  3.8674e-04,  9.1965e-05, -1.9940e-04,\n",
      "          2.2510e-04],\n",
      "        [-4.2613e-04,  6.8936e-05, -2.2906e-03,  1.0948e-03, -1.5599e-04,\n",
      "          1.5787e-03, -3.5705e-04, -2.6025e-04, -4.2496e-04, -5.3835e-04,\n",
      "         -6.2886e-04, -3.6103e-04, -5.2865e-04,  1.9722e-04,  1.4842e-04,\n",
      "          7.1357e-05],\n",
      "        [-1.2201e-03,  2.1688e-03, -4.0649e-03, -9.8043e-04, -8.2932e-04,\n",
      "          5.6516e-04, -4.5785e-04, -3.6526e-04, -3.4485e-04, -2.7975e-03,\n",
      "          1.4497e-03,  4.4171e-05,  1.1660e-04,  5.0422e-04, -1.2202e-04,\n",
      "          3.6345e-04],\n",
      "        [ 1.6583e-04, -9.6157e-04,  8.8225e-04,  1.4539e-03,  5.8476e-05,\n",
      "         -3.6417e-04,  5.2259e-05, -2.5712e-05, -2.2272e-04,  1.4047e-03,\n",
      "         -2.0178e-04,  5.9218e-05, -3.0784e-04,  1.0313e-04,  1.6125e-04,\n",
      "         -3.2768e-04],\n",
      "        [-6.9548e-04,  2.1524e-03, -3.0547e-03, -2.4831e-03, -3.2204e-04,\n",
      "          1.2543e-03, -2.8436e-04, -1.5508e-04,  2.3093e-04, -3.2065e-03,\n",
      "          4.2203e-04, -2.2004e-04,  4.9113e-04, -4.6182e-05, -2.7035e-04,\n",
      "          7.0612e-04],\n",
      "        [-5.8255e-05, -2.8970e-04,  1.7767e-04,  8.2206e-04, -9.7461e-05,\n",
      "         -4.7833e-04,  8.3200e-06, -1.9028e-05, -2.2350e-04,  5.9567e-04,\n",
      "          3.2779e-04,  1.3277e-04, -2.1465e-04,  1.7777e-04,  8.9500e-05,\n",
      "         -2.1771e-04],\n",
      "        [-4.3068e-04,  2.0108e-03, -6.0927e-04, -2.7736e-03, -4.1389e-04,\n",
      "         -1.3724e-03,  9.1533e-05,  1.1016e-04,  3.5574e-04, -2.1152e-03,\n",
      "          1.9769e-03,  4.5394e-04,  8.0698e-04,  7.0807e-05, -3.3837e-04,\n",
      "          3.3118e-04],\n",
      "        [ 6.0810e-04, -1.3715e-03,  2.1282e-03,  1.4804e-03,  3.4279e-04,\n",
      "         -7.8354e-04,  2.2021e-04,  2.6838e-04, -9.0787e-05,  2.0046e-03,\n",
      "         -3.3700e-04,  1.2161e-04, -4.5028e-04, -4.5918e-05,  2.0488e-04,\n",
      "         -4.4452e-04],\n",
      "        [ 1.0377e-04, -2.7355e-04,  2.7650e-05,  5.0886e-04,  1.0244e-04,\n",
      "          1.7427e-04, -1.1833e-05,  7.6699e-05, -9.0785e-05,  2.9072e-04,\n",
      "         -2.2962e-04, -6.5510e-05, -2.8349e-04, -2.3298e-06,  8.6307e-05,\n",
      "         -7.7509e-05],\n",
      "        [ 5.2056e-04, -1.6560e-03,  1.1521e-03,  1.9803e-03,  4.0858e-04,\n",
      "          6.8778e-04,  3.8499e-05,  3.5337e-05, -1.7796e-04,  1.8686e-03,\n",
      "         -1.4301e-03, -2.4889e-04, -5.7229e-04, -1.1780e-04,  2.5492e-04,\n",
      "         -2.9969e-04],\n",
      "        [-3.9671e-04,  1.1702e-03, -1.9754e-03, -1.7034e-03, -1.1968e-04,\n",
      "          1.4864e-03, -2.0586e-04, -2.4813e-04,  2.4630e-04, -2.0535e-03,\n",
      "         -4.1026e-04, -3.1779e-04,  4.9234e-04, -1.6510e-04, -2.2188e-04,\n",
      "          5.7285e-04],\n",
      "        [-1.0206e-04,  2.5882e-04, -7.8003e-04, -2.3445e-04,  1.4733e-06,\n",
      "          6.5669e-04, -1.0195e-04, -4.3228e-05, -1.6687e-06, -5.6858e-04,\n",
      "         -2.7395e-04, -1.5938e-04, -3.6889e-05, -3.3508e-05, -1.9919e-05,\n",
      "          1.4377e-04],\n",
      "        [ 7.9510e-04, -2.7781e-03,  7.0060e-03,  2.3186e-03, -8.9222e-05,\n",
      "         -5.2243e-03,  8.5679e-04, -5.9023e-05,  4.3453e-05,  5.4581e-03,\n",
      "          1.7250e-03,  1.2730e-03,  7.6679e-04,  2.5760e-04,  7.3788e-05,\n",
      "         -1.2378e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.1084e-05, 7.8615e-05, 2.5119e-04, 1.9771e-04, 6.4754e-06, 1.0455e-04,\n",
      "         1.1240e-05, 4.2812e-06, 1.6653e-05, 1.3652e-04, 6.0774e-05, 1.2762e-05,\n",
      "         7.0786e-05, 9.6830e-06, 1.8146e-06, 1.1003e-05],\n",
      "        [7.1950e-06, 5.0409e-05, 1.4289e-04, 8.2641e-05, 4.0802e-06, 4.6945e-05,\n",
      "         5.3166e-06, 2.1973e-06, 7.4033e-06, 9.6861e-05, 3.2209e-05, 4.7439e-06,\n",
      "         2.8403e-05, 5.8418e-06, 8.5318e-07, 5.4363e-06],\n",
      "        [8.4542e-05, 5.8714e-04, 2.2553e-03, 7.7631e-04, 5.3101e-05, 1.0618e-03,\n",
      "         1.0171e-04, 4.0551e-05, 7.7263e-05, 1.2838e-03, 5.0738e-04, 9.9571e-05,\n",
      "         3.3444e-04, 5.6996e-05, 1.3744e-05, 6.2254e-05],\n",
      "        [2.4058e-05, 1.7206e-04, 5.5107e-04, 4.9079e-04, 1.4262e-05, 2.2319e-04,\n",
      "         2.1786e-05, 8.7543e-06, 4.2020e-05, 2.9582e-04, 1.3790e-04, 2.6716e-05,\n",
      "         1.7842e-04, 2.5900e-05, 3.8780e-06, 2.8882e-05],\n",
      "        [2.9357e-05, 1.8664e-04, 7.0767e-04, 5.6534e-04, 1.8132e-05, 3.4544e-04,\n",
      "         3.2043e-05, 1.2893e-05, 4.9005e-05, 3.0131e-04, 1.8961e-04, 4.2966e-05,\n",
      "         2.2774e-04, 2.7007e-05, 5.0780e-06, 2.8345e-05],\n",
      "        [1.1875e-05, 6.5989e-05, 2.0789e-04, 7.5178e-05, 8.0162e-06, 1.1495e-04,\n",
      "         9.4140e-06, 3.7320e-06, 7.7933e-06, 1.2302e-04, 7.9649e-05, 1.2732e-05,\n",
      "         2.9496e-05, 7.0272e-06, 1.4118e-06, 6.2513e-06],\n",
      "        [1.9240e-05, 1.5085e-04, 3.8959e-04, 3.3276e-04, 1.1255e-05, 1.9723e-04,\n",
      "         1.9374e-05, 7.8118e-06, 2.2988e-05, 2.5236e-04, 1.1753e-04, 2.4555e-05,\n",
      "         1.0400e-04, 1.2717e-05, 3.3032e-06, 1.7005e-05],\n",
      "        [5.5355e-05, 4.2745e-04, 1.5171e-03, 6.3716e-04, 3.1556e-05, 6.0895e-04,\n",
      "         6.3716e-05, 2.4117e-05, 6.0876e-05, 8.8467e-04, 3.2322e-04, 5.6039e-05,\n",
      "         2.5126e-04, 4.3655e-05, 9.9213e-06, 4.1904e-05],\n",
      "        [1.9814e-06, 1.3132e-05, 4.1637e-05, 1.8991e-05, 1.1090e-06, 1.6836e-05,\n",
      "         1.7199e-06, 6.4563e-07, 1.7683e-06, 2.5505e-05, 9.5845e-06, 1.7901e-06,\n",
      "         6.8876e-06, 1.3545e-06, 2.6001e-07, 1.3119e-06],\n",
      "        [1.8932e-05, 1.2841e-04, 5.4450e-04, 1.8071e-04, 1.1775e-05, 2.4984e-04,\n",
      "         2.5329e-05, 1.0188e-05, 2.0528e-05, 2.8147e-04, 1.1300e-04, 2.3356e-05,\n",
      "         9.1926e-05, 1.5471e-05, 3.5853e-06, 1.3312e-05],\n",
      "        [3.4133e-06, 1.7970e-05, 6.2924e-05, 3.3962e-05, 2.0357e-06, 2.2699e-05,\n",
      "         2.3338e-06, 9.3774e-07, 3.8100e-06, 3.3265e-05, 1.7598e-05, 2.4136e-06,\n",
      "         1.4984e-05, 3.1743e-06, 3.9697e-07, 2.5171e-06],\n",
      "        [1.4434e-05, 9.3754e-05, 3.5619e-04, 1.3795e-04, 8.3482e-06, 1.2512e-04,\n",
      "         1.4261e-05, 5.9474e-06, 1.4981e-05, 1.9299e-04, 7.1710e-05, 1.2241e-05,\n",
      "         6.5074e-05, 1.1961e-05, 2.1871e-06, 9.1008e-06],\n",
      "        [6.1674e-05, 3.7413e-04, 1.7095e-03, 5.6061e-04, 3.7780e-05, 7.7771e-04,\n",
      "         8.0270e-05, 3.3168e-05, 6.2533e-05, 8.2593e-04, 3.4445e-04, 7.6054e-05,\n",
      "         3.0084e-04, 4.5535e-05, 1.0604e-05, 3.8175e-05],\n",
      "        [7.4281e-06, 5.0281e-05, 1.3897e-04, 6.9984e-05, 4.3632e-06, 6.3345e-05,\n",
      "         5.5138e-06, 1.7342e-06, 6.3528e-06, 9.3653e-05, 4.3219e-05, 6.3094e-06,\n",
      "         2.0996e-05, 4.6606e-06, 8.6634e-07, 4.7304e-06],\n",
      "        [5.5570e-05, 3.6533e-04, 1.0522e-03, 9.0278e-04, 3.7961e-05, 8.5187e-04,\n",
      "         6.2179e-05, 2.3578e-05, 6.0834e-05, 5.0316e-04, 5.0961e-04, 1.1065e-04,\n",
      "         3.1344e-04, 2.8915e-05, 9.6844e-06, 3.5544e-05],\n",
      "        [1.6250e-06, 1.0376e-05, 3.2903e-05, 1.0424e-05, 8.8978e-07, 1.1568e-05,\n",
      "         1.3246e-06, 4.0534e-07, 1.0523e-06, 2.0807e-05, 7.0479e-06, 1.1028e-06,\n",
      "         4.1016e-06, 1.0643e-06, 1.8789e-07, 7.8006e-07],\n",
      "        [2.6790e-05, 2.2768e-04, 6.5932e-04, 6.2998e-04, 1.5908e-05, 4.4604e-04,\n",
      "         3.3194e-05, 1.1262e-05, 4.6515e-05, 3.3478e-04, 2.4666e-04, 5.5455e-05,\n",
      "         2.0585e-04, 1.8751e-05, 5.8386e-06, 2.7623e-05],\n",
      "        [3.4562e-06, 2.1760e-05, 6.1949e-05, 3.7733e-05, 2.0419e-06, 1.7781e-05,\n",
      "         2.2189e-06, 8.0352e-07, 3.5150e-06, 4.1562e-05, 1.4937e-05, 1.7763e-06,\n",
      "         1.2510e-05, 2.9200e-06, 3.8814e-07, 2.7620e-06],\n",
      "        [9.7049e-06, 5.7117e-05, 1.8381e-04, 1.0377e-04, 5.8296e-06, 8.7649e-05,\n",
      "         7.4209e-06, 2.7050e-06, 1.0442e-05, 9.5734e-05, 6.7832e-05, 9.7304e-06,\n",
      "         3.9842e-05, 8.1246e-06, 1.4397e-06, 5.7179e-06],\n",
      "        [6.5246e-06, 4.4645e-05, 1.3707e-04, 8.4473e-05, 3.7884e-06, 4.3315e-05,\n",
      "         4.6875e-06, 1.7728e-06, 7.3473e-06, 8.5333e-05, 2.9177e-05, 4.4874e-06,\n",
      "         2.8463e-05, 5.2582e-06, 7.7626e-07, 5.5523e-06],\n",
      "        [1.2919e-05, 1.0238e-04, 3.5581e-04, 1.2269e-04, 7.7263e-06, 1.6215e-04,\n",
      "         1.6638e-05, 5.9672e-06, 1.1213e-05, 2.2550e-04, 7.1670e-05, 1.3806e-05,\n",
      "         4.5658e-05, 8.6527e-06, 2.3655e-06, 1.0653e-05],\n",
      "        [3.5861e-05, 3.1538e-04, 8.3070e-04, 6.0971e-04, 2.0380e-05, 4.0862e-04,\n",
      "         3.6041e-05, 1.2548e-05, 4.3412e-05, 5.4579e-04, 2.6137e-04, 4.7022e-05,\n",
      "         1.8330e-04, 2.5282e-05, 6.3641e-06, 3.1796e-05],\n",
      "        [1.8754e-06, 1.3860e-05, 3.4461e-05, 1.5585e-05, 1.0747e-06, 1.4054e-05,\n",
      "         1.6146e-06, 5.5877e-07, 1.4584e-06, 2.5872e-05, 8.3207e-06, 1.3934e-06,\n",
      "         4.7883e-06, 1.1580e-06, 3.9231e-07, 1.3925e-06],\n",
      "        [6.8778e-06, 4.6233e-05, 1.5791e-04, 7.0240e-05, 4.0728e-06, 6.5601e-05,\n",
      "         6.9786e-06, 2.7796e-06, 6.6426e-06, 8.9074e-05, 3.3906e-05, 7.1742e-06,\n",
      "         2.8987e-05, 4.5589e-06, 1.0332e-06, 4.3000e-06],\n",
      "        [3.6794e-06, 2.3628e-05, 7.3455e-05, 3.0361e-05, 2.0069e-06, 2.4814e-05,\n",
      "         2.5612e-06, 7.7902e-07, 2.7646e-06, 4.6311e-05, 1.6992e-05, 2.2617e-06,\n",
      "         9.9944e-06, 2.1934e-06, 4.5314e-07, 2.0340e-06],\n",
      "        [1.4128e-05, 7.7476e-05, 2.5665e-04, 1.1613e-04, 8.9067e-06, 1.3565e-04,\n",
      "         1.0869e-05, 3.4982e-06, 1.2113e-05, 1.3940e-04, 9.3830e-05, 1.4378e-05,\n",
      "         4.7668e-05, 1.0278e-05, 1.7608e-06, 7.4240e-06],\n",
      "        [4.2262e-06, 2.7496e-05, 7.5035e-05, 2.9377e-05, 2.4518e-06, 2.8097e-05,\n",
      "         2.9740e-06, 9.0829e-07, 3.1431e-06, 5.1823e-05, 2.1038e-05, 2.6515e-06,\n",
      "         9.8532e-06, 2.9188e-06, 4.6872e-07, 2.3045e-06],\n",
      "        [2.4047e-06, 1.8132e-05, 5.0606e-05, 2.1861e-05, 1.3142e-06, 1.6368e-05,\n",
      "         1.8721e-06, 5.5250e-07, 1.9438e-06, 3.7524e-05, 9.4251e-06, 1.3703e-06,\n",
      "         6.3082e-06, 1.4599e-06, 3.0447e-07, 1.6709e-06],\n",
      "        [1.4373e-05, 7.8185e-05, 2.0607e-04, 1.0315e-04, 9.5919e-06, 9.1404e-05,\n",
      "         7.0914e-06, 2.7368e-06, 1.1182e-05, 1.3715e-04, 9.2804e-05, 9.6683e-06,\n",
      "         3.6132e-05, 1.1285e-05, 1.3817e-06, 8.3426e-06],\n",
      "        [5.8221e-06, 3.1445e-05, 1.0917e-04, 5.1852e-05, 3.4966e-06, 4.6659e-05,\n",
      "         3.8978e-06, 1.3803e-06, 5.8772e-06, 5.4786e-05, 3.6474e-05, 5.1559e-06,\n",
      "         2.2328e-05, 5.2671e-06, 7.7269e-07, 3.2687e-06],\n",
      "        [7.7549e-07, 5.1112e-06, 1.8621e-05, 6.0609e-06, 4.6897e-07, 7.3061e-06,\n",
      "         7.8942e-07, 2.5266e-07, 6.6693e-07, 1.0594e-05, 3.6726e-06, 6.7032e-07,\n",
      "         2.6266e-06, 5.4693e-07, 1.2235e-07, 4.4666e-07],\n",
      "        [7.0634e-05, 4.6348e-04, 1.4248e-03, 1.1948e-03, 4.7512e-05, 1.1632e-03,\n",
      "         8.6412e-05, 3.4311e-05, 8.1732e-05, 6.5678e-04, 6.4491e-04, 1.4851e-04,\n",
      "         4.2570e-04, 3.6709e-05, 1.3021e-05, 4.8269e-05]], device='cuda:0')}, 18: {'step': tensor(19100.), 'exp_avg': tensor([[ 5.2547e-04,  4.6670e-04, -2.6091e-03,  ..., -4.8760e-05,\n",
      "          3.3371e-04,  3.7079e-03],\n",
      "        [ 1.5425e-03, -5.9156e-04,  1.4373e-03,  ...,  1.2245e-04,\n",
      "          3.6920e-04,  1.3926e-03],\n",
      "        [ 3.6712e-04,  7.3540e-04, -3.2426e-03,  ..., -2.4176e-04,\n",
      "          3.5946e-04,  4.6708e-03],\n",
      "        ...,\n",
      "        [-5.4096e-04,  3.2500e-04, -3.2486e-03,  ...,  1.1774e-03,\n",
      "         -3.2417e-04, -3.8863e-03],\n",
      "        [-7.6293e-07, -9.4861e-07,  3.0778e-06,  ...,  3.0284e-07,\n",
      "         -5.2011e-07, -6.4057e-06],\n",
      "        [-5.6052e-45,  5.6052e-45, -5.6052e-45,  ...,  5.6052e-45,\n",
      "         -5.6052e-45, -5.6052e-45]], device='cuda:0'), 'exp_avg_sq': tensor([[3.0063e-05, 2.5681e-05, 6.4462e-04,  ..., 1.8580e-05, 3.5286e-06,\n",
      "         3.5438e-04],\n",
      "        [5.6235e-05, 5.8816e-05, 7.7008e-04,  ..., 4.3370e-05, 4.7690e-06,\n",
      "         5.2057e-04],\n",
      "        [4.0394e-05, 3.6777e-05, 7.9995e-04,  ..., 3.6751e-05, 5.3468e-06,\n",
      "         6.5076e-04],\n",
      "        ...,\n",
      "        [4.2843e-05, 2.9331e-05, 8.1009e-04,  ..., 3.0846e-05, 4.6388e-06,\n",
      "         4.8827e-04],\n",
      "        [5.0155e-06, 1.8565e-06, 9.9375e-05,  ..., 3.8386e-06, 6.2432e-07,\n",
      "         3.3656e-05],\n",
      "        [3.4343e-12, 2.6983e-13, 4.1312e-12,  ..., 4.1885e-13, 2.4323e-13,\n",
      "         1.5408e-11]], device='cuda:0')}, 19: {'step': tensor(19100.), 'exp_avg': tensor([ 1.3903e-03,  8.2082e-03, -8.7702e-04,  5.6052e-45,  5.6052e-45,\n",
      "         1.4907e-41,  5.6052e-45, -1.2369e-02,  3.6082e-02,  5.6052e-45,\n",
      "         5.6052e-45,  0.0000e+00, -2.7722e-03,  4.3836e-06,  5.6052e-45,\n",
      "         5.6052e-45, -4.1887e-03,  5.6052e-45,  5.6052e-45, -7.7462e-03,\n",
      "        -6.7509e-03,  5.6052e-45,  5.6052e-45, -1.2253e-04, -2.7198e-10,\n",
      "        -3.3334e-06,  5.6052e-45,  1.6806e-03,  6.2910e-41,  5.6052e-45,\n",
      "         5.6052e-45,  1.7890e-38,  5.6052e-45,  5.6052e-45, -1.1210e-44,\n",
      "         5.6052e-45,  5.6052e-45,  6.8025e-03,  5.6052e-45,  5.6052e-45,\n",
      "        -3.1065e-06,  5.6052e-45,  8.9898e-11,  3.1428e-41,  1.1634e-02,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.1371e-03,  5.6052e-45, -1.2842e-16,\n",
      "         5.6052e-45, -7.2138e-03,  5.6052e-45,  2.5092e-09, -2.0829e-06,\n",
      "         5.6052e-45,  1.4671e-02, -2.7311e-06,  5.6052e-45], device='cuda:0'), 'exp_avg_sq': tensor([4.4452e-03, 7.7149e-03, 6.1482e-03, 1.2932e-10, 2.1344e-14, 5.7625e-09,\n",
      "        1.1956e-11, 3.2787e-03, 1.1950e-02, 6.7004e-13, 1.2177e-13, 0.0000e+00,\n",
      "        4.8060e-03, 6.1943e-06, 3.5730e-09, 4.4668e-09, 4.4897e-03, 3.6888e-09,\n",
      "        4.7857e-11, 1.9025e-03, 1.0785e-03, 1.4894e-07, 6.1197e-11, 1.4988e-04,\n",
      "        1.5906e-04, 1.2291e-02, 1.5221e-11, 6.4392e-03, 1.1281e-06, 2.3458e-11,\n",
      "        3.9060e-08, 1.3283e-05, 6.1308e-10, 5.3702e-10, 2.5067e-05, 7.1484e-07,\n",
      "        2.3428e-07, 2.2845e-03, 2.7294e-09, 3.8027e-15, 2.9505e-03, 3.9005e-10,\n",
      "        2.3166e-05, 6.8765e-07, 5.8191e-03, 1.5520e-07, 9.3185e-09, 1.4037e-06,\n",
      "        8.5584e-10, 3.2772e-13, 7.7840e-11, 6.0125e-07, 2.0120e-03, 5.2525e-08,\n",
      "        4.3002e-05, 2.7325e-09, 9.6809e-04, 4.4583e-11, 2.9894e-08, 8.9829e-05,\n",
      "        3.6705e-14, 3.5223e-03, 1.2941e-04, 3.5301e-11], device='cuda:0')}}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}]\n"
     ]
    }
   ],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{model_name}_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.5506249070167542,\n",
       "  0.3625558614730835,\n",
       "  0.5023398995399475,\n",
       "  0.4647738039493561,\n",
       "  0.6308514475822449,\n",
       "  0.5235710740089417,\n",
       "  0.4559338688850403,\n",
       "  0.9096137881278992,\n",
       "  0.45957571268081665,\n",
       "  0.9312061667442322,\n",
       "  0.8990527391433716,\n",
       "  0.3837440609931946,\n",
       "  0.3531661033630371,\n",
       "  1.039180874824524,\n",
       "  0.830642819404602,\n",
       "  0.5895458459854126,\n",
       "  0.5877319574356079,\n",
       "  0.9040868282318115,\n",
       "  0.7702378630638123,\n",
       "  0.6242498159408569,\n",
       "  0.3281988501548767,\n",
       "  0.41760891675949097,\n",
       "  0.6043224334716797,\n",
       "  0.08879508823156357,\n",
       "  0.3928443193435669,\n",
       "  0.7712334990501404,\n",
       "  0.5679090023040771,\n",
       "  0.5958870649337769,\n",
       "  0.5310583114624023,\n",
       "  0.8480932116508484,\n",
       "  0.3382696509361267,\n",
       "  0.9316542744636536,\n",
       "  0.6998307704925537,\n",
       "  0.780989944934845,\n",
       "  0.1630752682685852,\n",
       "  0.36225390434265137,\n",
       "  0.22552724182605743,\n",
       "  0.6935703158378601,\n",
       "  0.035873640328645706,\n",
       "  0.6778784394264221,\n",
       "  0.9580119252204895,\n",
       "  0.4871569275856018,\n",
       "  0.8037409782409668,\n",
       "  0.35711926221847534,\n",
       "  0.354701429605484,\n",
       "  0.5904563069343567,\n",
       "  0.4387834072113037,\n",
       "  0.4813379645347595,\n",
       "  0.7408961057662964,\n",
       "  1.0270888805389404,\n",
       "  0.35927942395210266,\n",
       "  0.8443410992622375,\n",
       "  0.7776339650154114,\n",
       "  0.8800849914550781,\n",
       "  0.2550559341907501,\n",
       "  0.33095723390579224,\n",
       "  0.5431606769561768,\n",
       "  0.4081604480743408,\n",
       "  0.601028323173523,\n",
       "  0.5783920288085938,\n",
       "  0.606498658657074,\n",
       "  0.5197433829307556,\n",
       "  0.41171029210090637,\n",
       "  0.5904960036277771,\n",
       "  0.4133762717247009,\n",
       "  0.5608035326004028,\n",
       "  1.024776816368103,\n",
       "  0.15492217242717743,\n",
       "  0.652828574180603,\n",
       "  0.0743958055973053,\n",
       "  0.924129843711853,\n",
       "  0.45257440209388733,\n",
       "  0.5836438536643982,\n",
       "  0.8135527968406677,\n",
       "  0.423240065574646,\n",
       "  0.19243519008159637,\n",
       "  0.528469443321228,\n",
       "  0.6461855173110962,\n",
       "  0.769391655921936,\n",
       "  0.7595245838165283,\n",
       "  0.6404323577880859,\n",
       "  0.5577169060707092,\n",
       "  0.33099794387817383,\n",
       "  0.32603588700294495,\n",
       "  0.28103476762771606,\n",
       "  0.6539925336837769,\n",
       "  0.5633567571640015,\n",
       "  0.8072153925895691,\n",
       "  0.8856512904167175,\n",
       "  0.775860071182251,\n",
       "  0.6367133259773254,\n",
       "  0.33260756731033325,\n",
       "  0.36307746171951294,\n",
       "  0.22767716646194458,\n",
       "  1.1593077182769775,\n",
       "  0.5853279232978821,\n",
       "  0.7251138091087341,\n",
       "  0.3851856291294098,\n",
       "  0.4526776075363159,\n",
       "  0.521509051322937,\n",
       "  0.6661285161972046,\n",
       "  0.19215472042560577,\n",
       "  0.0640895739197731,\n",
       "  0.6438112854957581,\n",
       "  0.47641026973724365,\n",
       "  0.9332574009895325,\n",
       "  0.6361080408096313,\n",
       "  0.7570843696594238,\n",
       "  0.513678252696991,\n",
       "  0.15092381834983826,\n",
       "  0.9151621460914612,\n",
       "  0.34550607204437256,\n",
       "  0.6561132073402405,\n",
       "  0.3285773992538452,\n",
       "  0.8712097406387329,\n",
       "  0.4739619791507721,\n",
       "  0.46750178933143616,\n",
       "  0.4214279353618622,\n",
       "  0.2827761173248291,\n",
       "  0.4907233417034149,\n",
       "  0.28342196345329285,\n",
       "  0.6441274881362915,\n",
       "  0.44826120138168335,\n",
       "  0.8851902484893799,\n",
       "  0.39435333013534546,\n",
       "  0.3695254325866699,\n",
       "  0.09950635582208633,\n",
       "  0.35641783475875854,\n",
       "  0.6120521426200867,\n",
       "  0.40477651357650757,\n",
       "  0.46968358755111694,\n",
       "  0.564924418926239,\n",
       "  0.46842917799949646,\n",
       "  0.3738764226436615,\n",
       "  0.6393686532974243,\n",
       "  0.8080548048019409,\n",
       "  0.4465389847755432,\n",
       "  0.49626079201698303,\n",
       "  0.30847278237342834,\n",
       "  0.4271041750907898,\n",
       "  0.4915196895599365,\n",
       "  0.7396118640899658,\n",
       "  0.45539119839668274,\n",
       "  0.6716005802154541,\n",
       "  0.4459204375743866,\n",
       "  0.4649275243282318,\n",
       "  0.337804913520813,\n",
       "  0.4026726186275482,\n",
       "  0.5302964448928833,\n",
       "  0.7449266910552979,\n",
       "  0.5048359036445618,\n",
       "  0.6811687350273132,\n",
       "  0.6074630618095398,\n",
       "  0.9222522377967834,\n",
       "  0.5496415495872498,\n",
       "  0.37487974762916565,\n",
       "  0.16285623610019684,\n",
       "  0.8497141599655151,\n",
       "  0.3638080060482025,\n",
       "  0.7783513069152832,\n",
       "  0.11766830831766129,\n",
       "  0.43475499749183655,\n",
       "  0.12872418761253357,\n",
       "  0.48046982288360596,\n",
       "  0.4114951193332672,\n",
       "  0.13446922600269318,\n",
       "  0.565091609954834,\n",
       "  0.9974916577339172,\n",
       "  0.7085499167442322,\n",
       "  0.6367709636688232,\n",
       "  0.5610774159431458,\n",
       "  0.5738833546638489,\n",
       "  0.5170602798461914,\n",
       "  0.15847985446453094,\n",
       "  0.6821572184562683,\n",
       "  0.5115287899971008,\n",
       "  0.6820048093795776,\n",
       "  0.6388485431671143,\n",
       "  0.6727949976921082,\n",
       "  0.5061013698577881,\n",
       "  0.860154926776886,\n",
       "  0.31925061345100403,\n",
       "  0.5967314839363098,\n",
       "  0.1707133650779724,\n",
       "  0.3721265494823456,\n",
       "  0.4128311276435852,\n",
       "  0.6203123927116394,\n",
       "  0.5266651511192322],\n",
       " [0.3641791343688965,\n",
       "  0.18950647115707397,\n",
       "  0.3386979401111603,\n",
       "  0.13080556690692902,\n",
       "  0.10992200672626495,\n",
       "  0.14277657866477966,\n",
       "  0.3911733627319336,\n",
       "  0.48970505595207214,\n",
       "  0.11522272229194641,\n",
       "  0.04780856519937515,\n",
       "  0.06069041043519974,\n",
       "  0.10531090199947357,\n",
       "  0.10654495656490326,\n",
       "  0.029311571270227432,\n",
       "  0.40168705582618713,\n",
       "  0.03957190364599228,\n",
       "  0.3893974721431732,\n",
       "  0.19224441051483154,\n",
       "  0.3745339810848236,\n",
       "  0.13079234957695007,\n",
       "  0.020691290497779846,\n",
       "  0.07795564085245132,\n",
       "  0.18934917449951172,\n",
       "  0.5151379704475403,\n",
       "  0.37491732835769653,\n",
       "  0.46591466665267944,\n",
       "  0.1548267900943756,\n",
       "  0.17919160425662994,\n",
       "  0.3630206882953644,\n",
       "  0.4787862300872803,\n",
       "  0.03836249187588692,\n",
       "  0.2799610197544098,\n",
       "  0.8633388876914978,\n",
       "  0.042182162404060364,\n",
       "  0.07144898921251297,\n",
       "  0.13976606726646423,\n",
       "  0.18780291080474854,\n",
       "  0.17129354178905487,\n",
       "  0.20922182500362396,\n",
       "  0.014505860395729542,\n",
       "  0.07404379546642303,\n",
       "  0.09329215437173843,\n",
       "  0.3175671398639679,\n",
       "  0.10800153762102127,\n",
       "  0.34658727049827576,\n",
       "  0.4258102476596832,\n",
       "  0.3314739167690277,\n",
       "  0.24848856031894684,\n",
       "  0.1880263090133667,\n",
       "  0.2483368068933487,\n",
       "  0.6101535558700562,\n",
       "  0.05361560732126236,\n",
       "  0.47528359293937683,\n",
       "  0.46650704741477966,\n",
       "  0.12555184960365295,\n",
       "  0.057713672518730164,\n",
       "  0.2690832018852234,\n",
       "  0.09419777989387512,\n",
       "  0.545545756816864,\n",
       "  0.5951347351074219,\n",
       "  0.14417915046215057,\n",
       "  0.33800598978996277,\n",
       "  0.015642428770661354,\n",
       "  0.023557698354125023,\n",
       "  0.09824704378843307,\n",
       "  0.18500098586082458,\n",
       "  0.7093138694763184,\n",
       "  0.05250788852572441,\n",
       "  0.28697165846824646,\n",
       "  0.24311478435993195,\n",
       "  0.08155710250139236,\n",
       "  0.0701184943318367,\n",
       "  0.2366137057542801,\n",
       "  0.34401071071624756,\n",
       "  0.5282818675041199,\n",
       "  0.4688704311847687,\n",
       "  0.2900888919830322,\n",
       "  0.08876116573810577,\n",
       "  0.35669270157814026,\n",
       "  0.19877159595489502,\n",
       "  0.4640851616859436,\n",
       "  0.4677085876464844,\n",
       "  0.09302505850791931,\n",
       "  0.09573877602815628,\n",
       "  0.18883700668811798,\n",
       "  0.6460456848144531,\n",
       "  0.27871981263160706,\n",
       "  0.40433669090270996,\n",
       "  0.46426472067832947,\n",
       "  0.1856354922056198,\n",
       "  0.13440850377082825,\n",
       "  0.06859181076288223,\n",
       "  0.17557598650455475,\n",
       "  0.4240095615386963,\n",
       "  0.3292172849178314,\n",
       "  0.229666605591774,\n",
       "  0.0796479880809784,\n",
       "  0.21612493693828583,\n",
       "  0.043749913573265076,\n",
       "  0.30238765478134155,\n",
       "  0.04682264104485512,\n",
       "  0.171608567237854,\n",
       "  0.02473622001707554,\n",
       "  0.212998166680336,\n",
       "  0.9529367685317993,\n",
       "  0.2982703149318695,\n",
       "  0.3679628074169159,\n",
       "  0.296261727809906,\n",
       "  0.15102528035640717,\n",
       "  0.682015061378479,\n",
       "  0.34784385561943054,\n",
       "  0.10549341142177582,\n",
       "  0.24392001330852509,\n",
       "  0.2766619622707367,\n",
       "  0.3328080475330353,\n",
       "  0.5100787281990051,\n",
       "  0.3067450523376465,\n",
       "  0.21304471790790558,\n",
       "  0.30053630471229553,\n",
       "  0.6412656903266907,\n",
       "  0.12903642654418945,\n",
       "  0.30368122458457947,\n",
       "  0.430308997631073,\n",
       "  0.6756930947303772,\n",
       "  0.205927312374115,\n",
       "  0.15135374665260315,\n",
       "  0.31911125779151917,\n",
       "  0.506207287311554,\n",
       "  0.32922258973121643,\n",
       "  0.22962602972984314,\n",
       "  0.10301952064037323,\n",
       "  0.17602969706058502,\n",
       "  0.11520218849182129,\n",
       "  0.31741979718208313,\n",
       "  0.2037741094827652,\n",
       "  0.03879483789205551,\n",
       "  0.15222197771072388,\n",
       "  0.05602550879120827,\n",
       "  0.3442572057247162,\n",
       "  0.14267794787883759,\n",
       "  0.3506467640399933,\n",
       "  0.32742825150489807,\n",
       "  0.20741145312786102,\n",
       "  0.06300429999828339,\n",
       "  0.021938364952802658,\n",
       "  0.326797217130661,\n",
       "  0.06437403708696365,\n",
       "  0.7498039603233337,\n",
       "  0.301766037940979,\n",
       "  0.27796387672424316,\n",
       "  0.2424740046262741,\n",
       "  0.4509250521659851,\n",
       "  0.2979251444339752,\n",
       "  0.19791196286678314,\n",
       "  0.060849253088235855,\n",
       "  0.32225701212882996,\n",
       "  0.24627864360809326,\n",
       "  0.19216947257518768,\n",
       "  0.9682157039642334,\n",
       "  0.012822114862501621,\n",
       "  0.3144994080066681,\n",
       "  0.5313104391098022,\n",
       "  0.338740736246109,\n",
       "  0.13588622212409973,\n",
       "  0.13447828590869904,\n",
       "  0.2822749614715576,\n",
       "  0.7501599788665771,\n",
       "  0.4045155644416809,\n",
       "  0.6395319700241089,\n",
       "  0.10145550966262817,\n",
       "  0.022401899099349976,\n",
       "  0.36697110533714294,\n",
       "  0.028854047879576683,\n",
       "  0.2998141348361969,\n",
       "  0.8774710297584534,\n",
       "  0.37578144669532776,\n",
       "  0.15237464010715485,\n",
       "  0.03696594759821892,\n",
       "  0.1137569323182106,\n",
       "  0.7963438034057617,\n",
       "  0.5176157355308533,\n",
       "  0.1181081011891365,\n",
       "  0.14237144589424133,\n",
       "  0.3572565019130707,\n",
       "  0.2972048223018646,\n",
       "  0.17541398108005524,\n",
       "  0.37394431233406067,\n",
       "  0.005445925053209066,\n",
       "  0.13997019827365875,\n",
       "  0.29167813062667847,\n",
       "  0.04497368633747101,\n",
       "  0.3989962935447693,\n",
       "  0.5896850824356079,\n",
       "  0.4049370586872101],\n",
       " [0.06631440669298172,\n",
       "  0.01795666106045246,\n",
       "  0.100970059633255,\n",
       "  0.05735740065574646,\n",
       "  0.20316942036151886,\n",
       "  0.008554985746741295,\n",
       "  0.12383222579956055,\n",
       "  0.14323528110980988,\n",
       "  0.006041277199983597,\n",
       "  0.010192367248237133,\n",
       "  0.07650727033615112,\n",
       "  0.11990420520305634,\n",
       "  0.06813420355319977,\n",
       "  0.11349328607320786,\n",
       "  0.14800430834293365,\n",
       "  0.006638287100940943,\n",
       "  0.004084747284650803,\n",
       "  0.14602743089199066,\n",
       "  0.001142889610491693,\n",
       "  0.001841629040427506,\n",
       "  0.005545261316001415,\n",
       "  0.0023660948500037193,\n",
       "  0.005095054395496845,\n",
       "  0.18988551199436188,\n",
       "  0.005676093976944685,\n",
       "  0.00042957745608873665,\n",
       "  0.20919927954673767,\n",
       "  0.08067607879638672,\n",
       "  0.0007829188252799213,\n",
       "  0.0,\n",
       "  0.01434046309441328,\n",
       "  0.07581421732902527,\n",
       "  0.0842377245426178,\n",
       "  0.018478868529200554,\n",
       "  0.08498367667198181,\n",
       "  0.07013721764087677,\n",
       "  0.004599965643137693,\n",
       "  0.026395976543426514,\n",
       "  0.07059526443481445,\n",
       "  0.22566018998622894,\n",
       "  0.16959069669246674,\n",
       "  0.008553139865398407,\n",
       "  0.0002140420547220856,\n",
       "  0.0030386559665203094,\n",
       "  0.017926137894392014,\n",
       "  0.07828038930892944,\n",
       "  0.41514700651168823,\n",
       "  0.1326843947172165,\n",
       "  0.18432064354419708,\n",
       "  0.026166442781686783,\n",
       "  0.09335260093212128,\n",
       "  0.08165293186903,\n",
       "  0.0702817365527153,\n",
       "  0.1099516749382019,\n",
       "  0.10853692144155502,\n",
       "  0.011537356302142143,\n",
       "  0.21894238889217377,\n",
       "  0.011985457502305508,\n",
       "  0.01605435088276863,\n",
       "  0.0023354266304522753,\n",
       "  0.0450516939163208,\n",
       "  0.023983066901564598,\n",
       "  0.06589187681674957,\n",
       "  0.35022005438804626,\n",
       "  0.11461875587701797,\n",
       "  0.20336133241653442,\n",
       "  0.0007357914000749588,\n",
       "  0.2999038100242615,\n",
       "  0.039188601076602936,\n",
       "  0.046950507909059525,\n",
       "  0.4647706151008606,\n",
       "  0.05188013240695,\n",
       "  0.0008813640451990068,\n",
       "  0.1315038502216339,\n",
       "  0.3726743459701538,\n",
       "  0.019258813932538033,\n",
       "  0.0008896142826415598,\n",
       "  0.002552435966208577,\n",
       "  0.2066490799188614,\n",
       "  0.009767076931893826,\n",
       "  0.20820629596710205,\n",
       "  0.08386252820491791,\n",
       "  0.01763504557311535,\n",
       "  0.1574835330247879,\n",
       "  0.13450537621974945,\n",
       "  0.01467055268585682,\n",
       "  0.03356688842177391,\n",
       "  0.0,\n",
       "  0.021887119859457016,\n",
       "  0.20526206493377686,\n",
       "  0.0001052223306032829,\n",
       "  0.0027411237824708223,\n",
       "  0.012114531360566616,\n",
       "  0.004351806826889515,\n",
       "  0.05042409524321556,\n",
       "  0.005832017865031958,\n",
       "  0.02472379058599472,\n",
       "  0.00718951178714633,\n",
       "  0.27747011184692383,\n",
       "  0.2238112986087799,\n",
       "  0.06006051227450371,\n",
       "  0.10435032099485397,\n",
       "  0.17513836920261383,\n",
       "  0.09065772593021393,\n",
       "  0.07958931475877762,\n",
       "  0.05493830889463425,\n",
       "  0.08887505531311035,\n",
       "  0.030872752889990807,\n",
       "  0.07742045819759369,\n",
       "  0.11532272398471832,\n",
       "  0.17304037511348724,\n",
       "  0.017676984891295433,\n",
       "  0.08384718000888824,\n",
       "  0.1720624715089798,\n",
       "  0.03083505854010582,\n",
       "  0.09644825756549835,\n",
       "  0.18614539504051208,\n",
       "  0.0014372357400134206,\n",
       "  0.11294591426849365,\n",
       "  0.0,\n",
       "  0.35707905888557434,\n",
       "  0.10881509631872177,\n",
       "  0.007881375029683113,\n",
       "  0.06026364862918854,\n",
       "  0.0016653417842462659,\n",
       "  0.42837169766426086,\n",
       "  0.03620191290974617,\n",
       "  0.004436461254954338,\n",
       "  0.14880849421024323,\n",
       "  0.0028779187705367804,\n",
       "  0.17709459364414215,\n",
       "  0.010393691249191761,\n",
       "  0.057122986763715744,\n",
       "  0.00012236044858582318,\n",
       "  0.14983920753002167,\n",
       "  0.00027751619927585125,\n",
       "  0.08667723834514618,\n",
       "  0.01738174632191658,\n",
       "  0.00482624676078558,\n",
       "  0.16632436215877533,\n",
       "  0.01711251214146614,\n",
       "  0.32608285546302795,\n",
       "  0.25156185030937195,\n",
       "  0.0013785753399133682,\n",
       "  0.041175853461027145,\n",
       "  0.11116927117109299,\n",
       "  0.02955273911356926,\n",
       "  0.003325663274154067,\n",
       "  0.002458214294165373,\n",
       "  0.06259257346391678,\n",
       "  0.02799341455101967,\n",
       "  0.02658943086862564,\n",
       "  0.059172146022319794,\n",
       "  0.028914986178278923,\n",
       "  0.1395403891801834,\n",
       "  0.06464465707540512,\n",
       "  0.0978071391582489,\n",
       "  0.2237837165594101,\n",
       "  0.10991840064525604,\n",
       "  0.227115198969841,\n",
       "  0.042076777666807175,\n",
       "  0.25845596194267273,\n",
       "  0.059861842542886734,\n",
       "  0.0039391471073031425,\n",
       "  0.09532772749662399,\n",
       "  0.06361360102891922,\n",
       "  0.01858271099627018,\n",
       "  0.019755030050873756,\n",
       "  0.10768599808216095,\n",
       "  0.10937681794166565,\n",
       "  0.004326623398810625,\n",
       "  0.006537459325045347,\n",
       "  0.025120021775364876,\n",
       "  0.004582828842103481,\n",
       "  0.017829211428761482,\n",
       "  0.20868588984012604,\n",
       "  0.0659874975681305,\n",
       "  0.038842085748910904,\n",
       "  0.08174274116754532,\n",
       "  0.10777056217193604,\n",
       "  0.01722979173064232,\n",
       "  0.22270835936069489,\n",
       "  0.20283512771129608,\n",
       "  0.00903263222426176,\n",
       "  0.2982424199581146,\n",
       "  0.0023523287381976843,\n",
       "  0.015413293614983559,\n",
       "  0.0,\n",
       "  0.0898926705121994,\n",
       "  0.054191138595342636,\n",
       "  0.0263728778809309,\n",
       "  0.0031720588449388742,\n",
       "  0.023354418575763702,\n",
       "  0.08597639203071594,\n",
       "  0.0009571043774485588,\n",
       "  0.0377822071313858,\n",
       "  0.04571856930851936,\n",
       "  0.18899914622306824,\n",
       "  0.0010962560772895813,\n",
       "  0.14977960288524628,\n",
       "  0.11447807401418686,\n",
       "  0.05573591962456703,\n",
       "  0.01472431793808937,\n",
       "  0.00030597770819440484,\n",
       "  0.32630473375320435,\n",
       "  0.022684313356876373,\n",
       "  0.45404133200645447,\n",
       "  0.044482793658971786,\n",
       "  0.0676989033818245,\n",
       "  0.04388554021716118,\n",
       "  0.011404085904359818,\n",
       "  0.4379647970199585,\n",
       "  0.06343613564968109,\n",
       "  0.23257450759410858,\n",
       "  0.06049889698624611,\n",
       "  0.005564553197473288,\n",
       "  0.02974780648946762,\n",
       "  0.13707491755485535,\n",
       "  0.0022274982184171677,\n",
       "  0.06620776653289795,\n",
       "  0.038270220160484314,\n",
       "  0.029503291472792625,\n",
       "  0.055379875004291534,\n",
       "  0.11825546622276306,\n",
       "  0.36046651005744934,\n",
       "  0.003598943119868636,\n",
       "  0.13009148836135864,\n",
       "  0.047046903520822525,\n",
       "  0.2141801416873932,\n",
       "  0.059128180146217346,\n",
       "  0.22540283203125,\n",
       "  0.022693907842040062,\n",
       "  0.008293051272630692,\n",
       "  0.04516135901212692,\n",
       "  0.1383596509695053,\n",
       "  0.20561237633228302,\n",
       "  0.1417790949344635,\n",
       "  0.008325018920004368,\n",
       "  0.046110037714242935,\n",
       "  0.09258132427930832,\n",
       "  0.16737417876720428,\n",
       "  0.22828243672847748,\n",
       "  0.02120274305343628,\n",
       "  0.25720691680908203,\n",
       "  0.011453649029135704,\n",
       "  0.05091601610183716,\n",
       "  0.1296827346086502,\n",
       "  0.2567422389984131,\n",
       "  0.06332351267337799,\n",
       "  0.1281212866306305,\n",
       "  0.05419301986694336,\n",
       "  0.026363365352153778,\n",
       "  0.1522097885608673,\n",
       "  0.006590304430574179,\n",
       "  0.18340268731117249,\n",
       "  0.19874796271324158,\n",
       "  0.00530636589974165,\n",
       "  0.015493287704885006,\n",
       "  0.006635790225118399,\n",
       "  0.050376735627651215,\n",
       "  0.40544185042381287,\n",
       "  0.020761804655194283,\n",
       "  0.0007524830289185047,\n",
       "  0.011585497297346592,\n",
       "  0.0015693549066781998,\n",
       "  0.05925538390874863,\n",
       "  0.010178668424487114,\n",
       "  0.061476901173591614,\n",
       "  0.053603701293468475,\n",
       "  0.20709910988807678,\n",
       "  0.07525089383125305,\n",
       "  0.1771472692489624,\n",
       "  0.021509995684027672,\n",
       "  0.0019847769290208817,\n",
       "  0.00024061487056314945,\n",
       "  0.1406196802854538,\n",
       "  0.05339730530977249,\n",
       "  0.0946444496512413,\n",
       "  0.1412840038537979,\n",
       "  0.19601519405841827,\n",
       "  0.06502851843833923,\n",
       "  0.0020720770116895437,\n",
       "  0.2811007499694824,\n",
       "  0.018420953303575516,\n",
       "  0.15316016972064972,\n",
       "  0.04553168639540672,\n",
       "  0.1268751621246338,\n",
       "  0.03863261267542839,\n",
       "  0.029396681115031242,\n",
       "  0.23910298943519592,\n",
       "  0.16410501301288605,\n",
       "  0.12927699089050293,\n",
       "  0.03390001133084297,\n",
       "  0.10166656970977783,\n",
       "  0.044378913938999176,\n",
       "  0.14830127358436584,\n",
       "  0.019584959372878075,\n",
       "  0.001851486274972558,\n",
       "  0.053924206644296646,\n",
       "  0.15350015461444855,\n",
       "  0.14315010607242584,\n",
       "  0.05192388966679573,\n",
       "  0.03032621368765831,\n",
       "  0.21924982964992523,\n",
       "  0.17840182781219482,\n",
       "  0.11031113564968109,\n",
       "  0.01846449263393879,\n",
       "  0.4687177538871765,\n",
       "  8.221049210987985e-05,\n",
       "  0.03253200277686119,\n",
       "  0.04945432022213936,\n",
       "  0.14114362001419067,\n",
       "  0.12259504199028015,\n",
       "  0.05082596465945244,\n",
       "  0.07704372704029083,\n",
       "  0.05737143009901047,\n",
       "  0.003022093093022704,\n",
       "  0.10140983760356903,\n",
       "  0.009232348762452602,\n",
       "  0.1953900009393692,\n",
       "  0.009042022749781609,\n",
       "  0.03983957692980766,\n",
       "  0.3504025638103485,\n",
       "  0.28137195110321045,\n",
       "  0.011293401010334492,\n",
       "  0.20236897468566895,\n",
       "  0.08181536197662354,\n",
       "  0.02456350438296795,\n",
       "  0.38925325870513916,\n",
       "  0.2045009583234787,\n",
       "  0.15975157916545868,\n",
       "  0.005146440118551254,\n",
       "  0.3795611560344696,\n",
       "  0.00025094422744587064,\n",
       "  0.06733492761850357,\n",
       "  0.1349548101425171,\n",
       "  0.1731705516576767,\n",
       "  0.3745743930339813,\n",
       "  0.09457371383905411,\n",
       "  0.00041624854202382267,\n",
       "  3.154118758175173e-06,\n",
       "  0.04494483023881912,\n",
       "  0.3849749267101288,\n",
       "  0.07060641795396805,\n",
       "  0.042470552027225494,\n",
       "  0.011608913540840149,\n",
       "  0.06596693396568298,\n",
       "  0.09632798284292221,\n",
       "  0.09078830480575562,\n",
       "  0.0006832911749370396,\n",
       "  0.006470168009400368,\n",
       "  0.11661385744810104,\n",
       "  0.3540794253349304,\n",
       "  0.3170812427997589,\n",
       "  0.05051315203309059,\n",
       "  0.1339617669582367,\n",
       "  0.00697455694898963,\n",
       "  0.11930232495069504,\n",
       "  0.010135030373930931,\n",
       "  0.06381522864103317,\n",
       "  0.04416561871767044,\n",
       "  0.050559598952531815,\n",
       "  0.0652160719037056,\n",
       "  0.05353110656142235,\n",
       "  0.015385348349809647,\n",
       "  0.06991804391145706,\n",
       "  1.482221887272317e-05,\n",
       "  0.12196625024080276,\n",
       "  0.009778180159628391,\n",
       "  0.00979561172425747,\n",
       "  0.042537275701761246,\n",
       "  0.0010113054886460304,\n",
       "  0.07959912717342377,\n",
       "  0.23170924186706543,\n",
       "  0.0813136100769043,\n",
       "  0.3438594937324524,\n",
       "  0.1738629788160324,\n",
       "  0.08198569715023041,\n",
       "  0.19711177051067352,\n",
       "  0.17238466441631317,\n",
       "  0.07208258658647537,\n",
       "  0.11202212423086166],\n",
       " [1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199106/3520072505.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tr_results = np.asarray(train_results)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tr_results = np.asarray(train_results)\n",
    "tr_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tr_results[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "tr_results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "w = 0.8    # bar width\n",
    "x = [1, 2] # x-coordinates of your bars\n",
    "colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "y = [np.random.random(30) * 2 + 5,       # data series\n",
    "    np.random.random(10) * 3 + 8]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x,\n",
    "       height=[np.mean(yi) for yi in y],\n",
    "       yerr=[np.std(yi) for yi in y],    # error bars\n",
    "       capsize=12, # error bar cap width in points\n",
    "       width=w,    # bar width\n",
    "       tick_label=[\"control\", \"test\"],\n",
    "       color=(0,0,0,0),  # face color transparent\n",
    "       edgecolor=colors,\n",
    "       #ecolor=colors,    # error bar colors; setting this raises an error for whatever reason.\n",
    "       )\n",
    "\n",
    "for i in range(len(x)):\n",
    "    # distribute scatter randomly across whole width of bar\n",
    "    ax.scatter(x[i] + np.random.random(y[i].size) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(f'{model_name}_training_loss.npy', tr_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_training_accuracy.npy', tr_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_validation_loss.npy', v_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_validation_accuracy.npy', v_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_test_loss.npy', tst_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_test_accuracy.npy', tst_acc, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_loss = np.load(f'{model_name}_training_loss.npy', allow_pickle=True)\n",
    "training_accuracy = np.load(f'{model_name}_training_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "validation_loss = np.load(f'{model_name}_validation_loss.npy', allow_pickle=True)\n",
    "validation_accuracy = np.load(f'{model_name}_validation_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "test_loss = np.load(f'{model_name}_test_loss.npy', allow_pickle=True)\n",
    "test_accuracy = np.load(f'{model_name}_test_accuracy.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_loss, label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Training and Validation Loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, label='Validation Accuracy')\n",
    "plt.plot(epochs, test_accuracy, label='Test Accuracy')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    "plt.ylim(0,1)\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Accuracy vs. Epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fraction = [0,0]\n",
    "\n",
    "train_fraction = [0,0]\n",
    "val_fraction = [0,0]\n",
    "test_fraction = [0,0]\n",
    "\n",
    "for grph in train_dataset: \n",
    "    if grph.y == 1: \n",
    "        train_fraction[1] +=1\n",
    "        dataset_fraction[1] +=1 \n",
    "    else: \n",
    "        train_fraction[0] +=1\n",
    "        dataset_fraction[0] +=1 \n",
    "\n",
    "for grph in val_dataset: \n",
    "    if grph.y == 1:\n",
    "         val_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1  \n",
    "    else:\n",
    "         val_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "for grph in test_dataset: \n",
    "    if grph.y == 1:\n",
    "         test_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1 \n",
    "    else:\n",
    "         test_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "print(f'Overall dataset percentage of label 1 = {dataset_fraction[1]/len(dataset)})')\n",
    "print(f'Training dataset percentage of label 1 = {train_fraction} = {train_fraction[1]/len(train_dataset)}')\n",
    "print(f'Validation dataset percentage of label 1 = {val_fraction} = {val_fraction[1]/len(val_dataset)}')\n",
    "print(f'Test dataset percentage of label 1 = {test_fraction} = {test_fraction[1]/len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, pos0, adj0 = torch.load(f'{model_name}_img0_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN\n",
    "print(x0[0].shape)\n",
    "x0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos0[0].shape)\n",
    "pos0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj0[0].shape)\n",
    "adj0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj0[0])\n",
    "visualize_points(pos0[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph After 1st Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_emb, x1_pool, pos1, adj1, s1= torch.load(f'{model_name}_img1_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj0 @ x_0 @ w_gnn_emb)\n",
    "print(x1_emb[0].shape)\n",
    "x1_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj_0 @ x_0 @ w_gnn_pool\n",
    "print(s1[0].shape)\n",
    "s1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s).t() @ pos_in)\n",
    "print(pos1[0].shape)\n",
    "pos1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s).t() @ x_in)\n",
    "print(x1_pool[0].shape)\n",
    "x1_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix = softmax(adj_out = softmax(s.t()) @ adj_in @ softmax(s))\n",
    "print(adj1[0].shape)\n",
    "adj1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj1[0])\n",
    "visualize_points(pos1[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 2nd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_emb, x2_pool, pos2, adj2, s2 = torch.load(f'{model_name}_img2_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj1 @ x1_pool @ w_gnn_emb)\n",
    "print(x2_emb[0].shape)\n",
    "x2_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj1 @ x1_pool @ w_gnn_pool), dim=1\n",
    "print(s2[0].shape)\n",
    "s2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos2[0].shape)\n",
    "pos2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s2).t() @ x2_emb)\n",
    "print(x2_pool[0].shape)\n",
    "x2_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s).T @ adj @ softmax(s)\n",
    "print(adj2[0].shape)\n",
    "adj2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj2[0])\n",
    "visualize_points(pos2[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 3rd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_emb, x3_pool, pos3, adj3, s3 = torch.load(f'{model_name}_img3_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj_0 @ x_0 @ w_gnn_emb)\n",
    "print(x3_emb[0].shape)\n",
    "x3_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: torch.softmax(adj_0 @ x_0 @ w_gnn_pool), dim=1)\n",
    "print(s3[0].shape)\n",
    "s3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos3[0].shape)\n",
    "pos3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s.t()) @ x_0)\n",
    "print(x3_pool[0].shape)\n",
    "x3_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s.t()) @ adj @ softmax(s)\n",
    "print(adj3[0].shape)\n",
    "adj3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj3[0])\n",
    "visualize_points(pos3[0].cpu(), edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2 (main, Jan 15 2022, 19:56:27) [GCC 11.1.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fec49b2b4a08384e01fd851531c21ff749e56c9a3b01e22f757b2c66b74daf11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
