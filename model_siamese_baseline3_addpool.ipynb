{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize features? \n",
    "## Invert h-bond and charge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'adl_model_siamese_baseline'\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grabeda2/pyg_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader #To make use of this data loader, all graph attributes in the dataset need to have the same shape. In particular, this data loader should only be used when working with dense adjacency matrices.\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool, global_max_pool, global_add_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir_1 = 'C:/Users/david/pyproj/pyg/adl/patch_label_1'\n",
    "#data_dir_0 = 'C:/Users/david/pyproj/pyg/adl/patch_label_0'\n",
    "data_dir_1 = 'adl_data_1'\n",
    "data_dir_0 = 'adl_data_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from c_PatchDataset import PatchDataset\n",
    "dataset = PatchDataset(data_dir_label_0 = data_dir_0,  data_dir_label_1=data_dir_1,  neg_pos_ratio=1)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(572):\n",
      "====================\n",
      "Number of graphs pairs: 572\n",
      "\n",
      "PairData(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=0)\n",
      "=============================================================\n",
      "Number of nodes in each: None\n",
      "Number of node features: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grabeda2/pyg_env/lib/python3.10/site-packages/torch_geometric/data/storage.py:280: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'adj2', 'y', 'x2', 'adj1', 'x1'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs pairs: {len(dataset)}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes in each: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0600e-01,  0.0000e+00, -9.9437e-01],\n",
       "        [-1.2153e-01,  0.0000e+00, -9.9259e-01],\n",
       "        [ 8.9937e-02,  1.7801e-01, -9.7991e-01],\n",
       "        [ 1.0526e-01,  4.6009e-02, -9.9338e-01],\n",
       "        [-1.7111e-02,  0.0000e+00, -9.9985e-01],\n",
       "        [-5.7591e-02,  0.0000e+00, -9.9834e-01],\n",
       "        [-2.4579e-01,  0.0000e+00, -9.6932e-01],\n",
       "        [ 7.2272e-02,  0.0000e+00, -9.9738e-01],\n",
       "        [ 5.8896e-02,  0.0000e+00, -9.9826e-01],\n",
       "        [-1.7830e-02,  0.0000e+00, -9.9984e-01],\n",
       "        [-6.9150e-01, -1.9468e-01, -6.9565e-01],\n",
       "        [-3.6645e-02,  0.0000e+00, -9.9933e-01],\n",
       "        [ 3.8248e-02,  0.0000e+00, -9.9927e-01],\n",
       "        [ 9.8237e-02,  0.0000e+00, -9.9516e-01],\n",
       "        [-2.0637e-01, -1.1561e-01, -9.7162e-01],\n",
       "        [-3.4071e-01, -7.9453e-05, -9.4017e-01],\n",
       "        [-2.5418e-01,  0.0000e+00,  9.6716e-01],\n",
       "        [-1.7896e-01,  0.0000e+00, -9.8386e-01],\n",
       "        [-2.1511e-01,  0.0000e+00, -9.7659e-01],\n",
       "        [ 1.0053e-02,  0.0000e+00, -9.9995e-01],\n",
       "        [ 1.4815e-01,  0.0000e+00, -9.8897e-01],\n",
       "        [ 3.5345e-02,  0.0000e+00, -9.9938e-01],\n",
       "        [ 1.0261e-02,  0.0000e+00, -9.9995e-01],\n",
       "        [-6.2121e-02, -3.7915e-03, -9.9806e-01],\n",
       "        [-1.7922e-01,  0.0000e+00,  9.8381e-01],\n",
       "        [-3.9855e-01, -8.8496e-02, -9.1287e-01],\n",
       "        [-1.3664e-01, -8.0280e-02, -9.8736e-01],\n",
       "        [-1.0319e-01, -1.4396e-01, -9.8419e-01],\n",
       "        [ 1.8289e-01,  0.0000e+00, -9.8313e-01],\n",
       "        [-5.5297e-01, -2.2185e-01, -8.0312e-01],\n",
       "        [-4.0540e-01, -1.6576e-01, -8.9898e-01],\n",
       "        [ 2.3937e-02,  1.7787e-01, -9.8376e-01],\n",
       "        [-1.2264e-02,  0.0000e+00, -9.9992e-01],\n",
       "        [ 3.5205e-02,  0.0000e+00, -9.9938e-01],\n",
       "        [-1.8897e-01, -6.8150e-02, -9.7961e-01],\n",
       "        [ 2.3484e-02,  0.0000e+00, -9.9972e-01],\n",
       "        [ 1.0768e-03,  0.0000e+00, -1.0000e+00],\n",
       "        [-3.5248e-01, -1.6766e-01, -9.2068e-01],\n",
       "        [-1.3978e-01,  0.0000e+00,  9.9018e-01],\n",
       "        [ 1.0091e-01,  0.0000e+00, -9.9490e-01],\n",
       "        [-5.4904e-02,  0.0000e+00, -9.9849e-01],\n",
       "        [ 4.9372e-02,  0.0000e+00, -9.9878e-01],\n",
       "        [-2.5101e-01,  0.0000e+00, -9.6799e-01],\n",
       "        [-3.6825e-03,  0.0000e+00, -9.9999e-01],\n",
       "        [-1.5273e-01, -8.7561e-02, -9.8438e-01],\n",
       "        [-5.9957e-01, -6.5046e-01,  4.6627e-01],\n",
       "        [-1.8501e-01,  0.0000e+00,  9.8274e-01],\n",
       "        [ 1.3431e-02,  0.0000e+00, -9.9991e-01],\n",
       "        [-4.1538e-01, -2.2084e-03, -9.0965e-01],\n",
       "        [-1.6014e-01, -1.6855e-01,  9.7260e-01],\n",
       "        [-9.9008e-02, -7.4319e-02, -9.9231e-01],\n",
       "        [-1.0587e-02,  0.0000e+00, -9.9994e-01],\n",
       "        [-1.2200e-01, -2.5273e-01, -9.5981e-01],\n",
       "        [-1.1100e-01,  0.0000e+00, -9.9382e-01],\n",
       "        [-3.7525e-01, -1.8115e-01, -9.0905e-01],\n",
       "        [ 6.6957e-02,  0.0000e+00, -9.9776e-01],\n",
       "        [ 2.3067e-01,  1.9835e-02, -9.7283e-01],\n",
       "        [-1.5573e-01,  0.0000e+00, -9.8780e-01],\n",
       "        [ 1.0502e-02,  0.0000e+00, -9.9994e-01],\n",
       "        [-3.9993e-02,  0.0000e+00, -9.9920e-01],\n",
       "        [-6.2304e-02, -8.5667e-02, -9.9437e-01],\n",
       "        [-2.9103e-01, -1.0943e-01, -9.5043e-01],\n",
       "        [ 1.8720e-03,  0.0000e+00, -1.0000e+00],\n",
       "        [-4.6419e-03,  0.0000e+00, -9.9999e-01],\n",
       "        [ 9.7480e-02,  0.0000e+00, -9.9524e-01],\n",
       "        [-3.0399e-01,  0.0000e+00, -9.5268e-01],\n",
       "        [ 9.9523e-02,  0.0000e+00, -9.9504e-01],\n",
       "        [-8.8500e-03,  0.0000e+00, -9.9996e-01],\n",
       "        [ 1.1578e-02,  0.0000e+00, -9.9993e-01],\n",
       "        [-3.0335e-02,  1.8049e-02, -9.9938e-01],\n",
       "        [ 2.5570e-02,  0.0000e+00, -9.9967e-01],\n",
       "        [ 5.0492e-02,  0.0000e+00, -9.9872e-01],\n",
       "        [ 1.1646e-01,  0.0000e+00, -9.9319e-01],\n",
       "        [ 1.3122e-01,  0.0000e+00, -9.9135e-01],\n",
       "        [-2.3599e-01, -1.7082e-01, -9.5662e-01],\n",
       "        [ 2.2802e-02, -1.0683e-01, -9.9402e-01],\n",
       "        [ 7.5846e-02,  2.4053e-01, -9.6767e-01],\n",
       "        [ 1.6419e-02,  0.0000e+00, -9.9987e-01],\n",
       "        [ 7.7712e-03,  7.4215e-02, -9.9721e-01],\n",
       "        [ 5.3435e-02,  6.9652e-02, -9.9614e-01],\n",
       "        [-2.1961e-02,  0.0000e+00, -9.9976e-01],\n",
       "        [ 4.2905e-02,  0.0000e+00, -9.9908e-01],\n",
       "        [ 4.9525e-03,  0.0000e+00, -9.9999e-01],\n",
       "        [-1.2347e-01,  0.0000e+00, -9.9235e-01],\n",
       "        [-1.0276e-01,  0.0000e+00, -9.9471e-01],\n",
       "        [-1.5774e-01, -4.2917e-06, -9.8748e-01],\n",
       "        [ 1.1343e-01,  0.0000e+00, -9.9355e-01],\n",
       "        [-1.6194e-01, -5.4964e-02, -9.8527e-01],\n",
       "        [-8.8353e-01,  0.0000e+00, -4.6838e-01],\n",
       "        [ 3.7655e-02,  0.0000e+00, -9.9929e-01],\n",
       "        [-1.8954e-02,  0.0000e+00, -9.9982e-01],\n",
       "        [-6.7574e-01,  0.0000e+00,  7.3714e-01],\n",
       "        [-2.7485e-01,  0.0000e+00, -9.6149e-01],\n",
       "        [ 1.5111e-01,  1.6594e-01, -9.7449e-01],\n",
       "        [-1.1391e-01,  0.0000e+00,  9.9349e-01],\n",
       "        [-2.8710e-01,  0.0000e+00,  9.5790e-01],\n",
       "        [-4.6969e-02,  0.0000e+00, -9.9890e-01],\n",
       "        [-3.2606e-03, -4.4762e-02, -9.9899e-01],\n",
       "        [ 3.6763e-02,  0.0000e+00, -9.9932e-01],\n",
       "        [-2.7141e-02,  0.0000e+00,  9.9963e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work we do not have pos\n",
    "#visualize_points(data.pos, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs pairs: 382\n",
      "Number of validation graphs: 95\n",
      "Number of test graphs: 95\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "n_train = math.ceil((4/6) * len(dataset))\n",
    "n_val = math.ceil((len(dataset) - n_train)/2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f'Number of training graphs pairs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size= batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairDataBatch(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=[1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = next(iter(train_loader))\n",
    "databatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_nodes, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        # Each instance of this GNN will have 3 convolutional layers and three batch norm layers        \n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        #self.bns1 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        #self.bns2 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        #self.bns3 = torch.nn.BatchNorm1d(in_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        \n",
    "        #Step 1\n",
    "        x = self.conv1(x, adj, mask)\n",
    "        #x = self.bns1(x)\n",
    "        \n",
    "        #Step 2\n",
    "        x = self.conv2(x, adj, mask)\n",
    "        #x = self.bns2(x)\n",
    "\n",
    "        #Step 3\n",
    "        x = self.conv3(x, adj, mask)\n",
    "        #if x.shape[2] != 1: \n",
    "        #    x = self.bns3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        in_nodes = num_nodes\n",
    "        out_nodes = 25 # Number of clusters / nodes in the next layer\n",
    "        #self.gnn1_pool = GNN(in_nodes, dataset.num_features, 16, out_nodes) # PoolGNN --> Cluster Assignment Matrix to reduce to num_nodes\n",
    "        self.gnn1_embed = GNN(in_nodes, dataset.num_features, 8, 8) # EmbGNN --> Convolutions to create new node embedding\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 10\n",
    "        #self.gnn2_pool = GNN(in_nodes, 8, 8, out_nodes)\n",
    "        self.gnn2_embed = GNN(in_nodes, 8, 12, 16, lin=False)\n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 1\n",
    "        #self.gnn3_pool = GNN(in_nodes, 16, 16, out_nodes)\n",
    "        self.gnn3_embed = GNN(in_nodes, 16, 16, 32, lin=False)\n",
    "\n",
    "        # Final Classifier\n",
    "        self.lin1 = torch.nn.Linear(32, 64) \n",
    "        #self.lin2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, batch, mask=None):\n",
    "        \n",
    "        #if batch == 0: print('Shape of input data batch:')\n",
    "        #if batch == 0: print(f'Feature Matrix: {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'Adjacency Matrix: {tuple(adj.shape)}')\n",
    "       \n",
    "\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        #if batch == 0: print('Hierarchical Step #1')\n",
    "        x = self.gnn1_embed(x, adj, mask) # node feature embedding\n",
    "        #s = self.gnn1_pool(x, adj, mask) # cluster assignment matrix\n",
    "\n",
    "        #if batch == 0: print(f'X1 = {tuple(x1.shape)}    S1: {tuple(s.shape)}')\n",
    "\n",
    "        #x, adj, l1, e1 = dense_diff_pool(x1, adj, s, mask) # does the necessary matrix multiplications\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "   \n",
    "\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        #if batch == 0: print('Hierarchical Step #2')\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        #s = self.gnn2_pool(x, adj)\n",
    "\n",
    "        #if batch == 0: print(f'X2: {tuple(x2.shape)}    S2: {tuple(s.shape)}')\n",
    "        \n",
    "        #x, adj, l2, e2 = dense_diff_pool(x2, adj, s)\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "      \n",
    "        \n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        #if batch == 0: print('Hierarchical Step #3')\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        #s = self.gnn3_pool(x, adj)\n",
    "        \n",
    "        #if batch == 0: print(f'X3: {tuple(x3.shape)}    S3: {tuple(s.shape)}')\n",
    "\n",
    "        #x, adj, l3, e3 = dense_diff_pool(x3, adj, s)\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "     \n",
    "        \n",
    "\n",
    "        # Final Classification\n",
    "        #if batch == 0: print('Final Output')\n",
    "        x = global_add_pool(x, batch=None) # Pool the features of all nodes (global mean pool)  dim = 1 refers to columns\n",
    "        #if batch == 0: print(f'---X Output after mean= {tuple(x.shape)}')\n",
    "\n",
    "        x = F.relu(self.lin1(x)) # Fully connected layer + relu\n",
    "        #if batch == 0: print(f'------ X Output 3 after lin= {tuple(x.shape)}')\n",
    "\n",
    "        \n",
    "        return x #, l1 + l2 + l3, e1 + e2 + e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt of a contrastive loss function\n",
    "#   pairs with label 1 --> should get small euclid dist = small loss\n",
    "#   pairs with label 0 --> should get large euclid dist = large loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        # euclidian distance\n",
    "        #print(x0)\n",
    "        #print(x1)\n",
    "        #print(y)\n",
    "        diff = x0 - x1\n",
    "        #print(diff)\n",
    "        pow = torch.pow(diff, 2)\n",
    "        #print(pow)\n",
    "        dist_sq = torch.sum(pow, 1)\n",
    "        #print(dist_sq) # sum of squared distance = 0.5 = 9\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "        #print(dist) # euclidean distance = 0.7 = 3\n",
    "\n",
    "        mdist = self.margin - dist #negative euclidean distance - margin = 0.3 = -2\n",
    "        #print(mdist)\n",
    "        dist_marg = torch.clamp(mdist, min=0.0) # only distances <margin will be still positive here = 0.3 = 0\n",
    "        #print(dist)\n",
    "        loss =  y * torch.pow(dist, 2) + (1-y) * torch.pow(dist_marg,2)\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 0.5\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0.3^2\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 0.5\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 9\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 9\n",
    "\n",
    "        #print(loss)\n",
    "        #loss = torch.sum(loss) / 2.0 \n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        #print(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 001, Train Loss: 4.955\n",
      "Epoch: 002, Train Loss: 0.121\n",
      "Epoch: 003, Train Loss: 0.146\n",
      "Epoch: 004, Train Loss: 0.158\n",
      "Epoch: 005, Train Loss: 0.135\n",
      "Epoch: 006, Train Loss: 0.139\n",
      "Epoch: 007, Train Loss: 0.137\n",
      "Epoch: 008, Train Loss: 0.140\n",
      "Epoch: 009, Train Loss: 0.129\n",
      "Epoch: 010, Train Loss: 0.137\n",
      "Epoch: 011, Train Loss: 0.138\n",
      "Epoch: 012, Train Loss: 0.131\n",
      "Epoch: 013, Train Loss: 0.125\n",
      "Epoch: 014, Train Loss: 0.125\n",
      "Epoch: 015, Train Loss: 0.121\n",
      "Epoch: 016, Train Loss: 0.122\n",
      "Epoch: 017, Train Loss: 0.134\n",
      "Epoch: 018, Train Loss: 0.127\n",
      "Epoch: 019, Train Loss: 0.131\n",
      "Epoch: 020, Train Loss: 0.126\n",
      "Epoch: 021, Train Loss: 0.130\n",
      "Epoch: 022, Train Loss: 0.134\n",
      "Epoch: 023, Train Loss: 0.132\n",
      "Epoch: 024, Train Loss: 0.137\n",
      "Epoch: 025, Train Loss: 0.132\n",
      "Epoch: 026, Train Loss: 0.130\n",
      "Epoch: 027, Train Loss: 0.119\n",
      "Epoch: 028, Train Loss: 0.126\n",
      "Epoch: 029, Train Loss: 0.126\n",
      "Epoch: 030, Train Loss: 0.131\n",
      "Epoch: 031, Train Loss: 0.127\n",
      "Epoch: 032, Train Loss: 0.124\n",
      "Epoch: 033, Train Loss: 0.124\n",
      "Epoch: 034, Train Loss: 0.134\n",
      "Epoch: 035, Train Loss: 0.136\n",
      "Epoch: 036, Train Loss: 0.130\n",
      "Epoch: 037, Train Loss: 0.143\n",
      "Epoch: 038, Train Loss: 0.134\n",
      "Epoch: 039, Train Loss: 0.127\n",
      "Epoch: 040, Train Loss: 0.128\n",
      "Epoch: 041, Train Loss: 0.125\n",
      "Epoch: 042, Train Loss: 0.125\n",
      "Epoch: 043, Train Loss: 0.128\n",
      "Epoch: 044, Train Loss: 0.120\n",
      "Epoch: 045, Train Loss: 0.127\n",
      "Epoch: 046, Train Loss: 0.125\n",
      "Epoch: 047, Train Loss: 0.126\n",
      "Epoch: 048, Train Loss: 0.122\n",
      "Epoch: 049, Train Loss: 0.140\n",
      "Epoch: 050, Train Loss: 0.123\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = DiffPool(num_nodes = 100).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    batch = 0\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1 = model(data.x1, data.adj1, batch)\n",
    "        output2 = model(data.x2, data.adj2, batch = None)\n",
    "        \n",
    "        #Contrastive Loss\n",
    "        loss_contrastive = criterion(output1,output2,data.y)\n",
    "        loss_contrastive.backward()\n",
    "        loss_all += data.y.size(0) * loss_contrastive.item()\n",
    "        optimizer.step()\n",
    "        batch +=1\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    distances_lab1 = []\n",
    "    distances_lab0 = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    \n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output1 = model(data.x1, data.adj2, batch=None)\n",
    "        output2 = model(data.x2, data.adj2, batch=None)\n",
    "\n",
    "        test_loss_contrastive = criterion(output1, output2, data.y)\n",
    "        \n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        label = data.y\n",
    "\n",
    "        if int(label) == 1: \n",
    "            distances_lab1.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "        else:\n",
    "            distances_lab0.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "\n",
    "    return  distances_lab0, distances_lab1, losses, labels\n",
    "\n",
    "\n",
    "\n",
    "train_distances_lab0 = []\n",
    "train_distances_lab1 = []\n",
    "train_losses = []\n",
    "train_labels = []\n",
    "\n",
    "validation_distances_lab0 = []\n",
    "validation_distances_lab1 = []\n",
    "validation_losses = []\n",
    "validation_labels = []\n",
    "\n",
    "test_distances_lab0 = []\n",
    "test_distances_lab1 = []\n",
    "test_losses = []\n",
    "test_labels = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "\n",
    "    train_results = test(train_loader)\n",
    "    train_distances_lab0.append(train_results[0])\n",
    "    train_distances_lab1.append(train_results[1])\n",
    "    train_losses.append(train_results[2])\n",
    "    train_labels.append(train_results[3])\n",
    "\n",
    "\n",
    "    validation_results = test(val_loader)\n",
    "    validation_distances_lab0.append(validation_results[0])\n",
    "    validation_distances_lab1.append(validation_results[1])\n",
    "    validation_losses.append(validation_results[2])\n",
    "    validation_labels.append(validation_results[3])\n",
    "\n",
    "    test_results = test(test_loader)\n",
    "    test_distances_lab0.append(test_results[0])\n",
    "    test_distances_lab1.append(test_results[1])\n",
    "    test_losses.append(test_results[2])\n",
    "    test_labels.append(test_results[3])\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}')\n",
    "    #Train Acc: {train_acc:.3f}, f'Val Acc: {val_acc:.3f}, Test Acc: {test_acc:.3f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_euclid_distances(distances_lab0, distances_lab1):\n",
    "\n",
    "    w = 0.8    # bar width\n",
    "    x = [1, 2] # x-coordinates of your bars\n",
    "    colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "\n",
    "    # Epoch 0\n",
    "    y = [distances_lab0, distances_lab1]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x,\n",
    "        height=[np.mean(yi) for yi in y],\n",
    "        yerr=[np.std(yi) for yi in y],    # error bars\n",
    "        capsize=12, # error bar cap width in points\n",
    "        width=w,    # bar width\n",
    "        tick_label=[\"Label 0\", \"Label 1\"],\n",
    "        color=(0,0,0,0),  # face color transparent\n",
    "        edgecolor=colors,\n",
    "        )\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # distribute scatter randomly across whole width of bar\n",
    "        ax.scatter(x[i] + np.random.random(len(y[i])) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "    plt.ylabel = 'Euclidean Distance'\n",
    "    plt.ylim(top=2, bottom=0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_euclid_distances(train_distances_lab0[49], train_distances_lab1[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "gnn1_embed.conv1.bias \t torch.Size([8])\n",
      "gnn1_embed.conv1.lin.weight \t torch.Size([8, 3])\n",
      "gnn1_embed.conv2.bias \t torch.Size([8])\n",
      "gnn1_embed.conv2.lin.weight \t torch.Size([8, 8])\n",
      "gnn1_embed.conv3.bias \t torch.Size([8])\n",
      "gnn1_embed.conv3.lin.weight \t torch.Size([8, 8])\n",
      "gnn2_embed.conv1.bias \t torch.Size([12])\n",
      "gnn2_embed.conv1.lin.weight \t torch.Size([12, 8])\n",
      "gnn2_embed.conv2.bias \t torch.Size([12])\n",
      "gnn2_embed.conv2.lin.weight \t torch.Size([12, 12])\n",
      "gnn2_embed.conv3.bias \t torch.Size([16])\n",
      "gnn2_embed.conv3.lin.weight \t torch.Size([16, 12])\n",
      "gnn3_embed.conv1.bias \t torch.Size([16])\n",
      "gnn3_embed.conv1.lin.weight \t torch.Size([16, 16])\n",
      "gnn3_embed.conv2.bias \t torch.Size([16])\n",
      "gnn3_embed.conv2.lin.weight \t torch.Size([16, 16])\n",
      "gnn3_embed.conv3.bias \t torch.Size([32])\n",
      "gnn3_embed.conv3.lin.weight \t torch.Size([32, 16])\n",
      "lin1.weight \t torch.Size([64, 32])\n",
      "lin1.bias \t torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {0: {'step': tensor(19100.), 'exp_avg': tensor([-3.0850e-04,  6.9096e-04,  6.9337e-05,  1.1891e-04,  2.5720e-04,\n",
      "         3.8928e-04,  3.3160e-04, -2.8970e-04], device='cuda:0'), 'exp_avg_sq': tensor([5.6945e-05, 8.8209e-05, 3.3029e-06, 1.3381e-05, 1.6873e-05, 2.5372e-05,\n",
      "        5.3217e-05, 2.6366e-05], device='cuda:0')}, 1: {'step': tensor(19100.), 'exp_avg': tensor([[-7.4811e-02, -2.6035e-02,  6.8167e-01],\n",
      "        [ 8.1621e-02,  2.4978e-02, -6.8109e-01],\n",
      "        [-1.0659e-02, -5.2125e-03,  1.3215e-01],\n",
      "        [-2.0789e-03, -1.7219e-03,  4.0748e-02],\n",
      "        [ 3.9408e-02,  1.4563e-02, -4.1311e-01],\n",
      "        [ 5.4628e-03, -3.8389e-03,  1.0793e-01],\n",
      "        [-5.1151e-04, -2.8712e-03,  3.3120e-02],\n",
      "        [-3.6792e-02, -1.2116e-02,  3.4521e-01]], device='cuda:0'), 'exp_avg_sq': tensor([[3.0435e-02, 2.1356e-03, 4.1164e-01],\n",
      "        [7.6764e-02, 5.6197e-03, 7.1527e-01],\n",
      "        [1.6943e-03, 9.7227e-05, 1.8424e-02],\n",
      "        [7.2155e-03, 2.2297e-04, 5.2036e-02],\n",
      "        [1.6190e-02, 8.2881e-04, 2.1192e-01],\n",
      "        [2.8768e-02, 1.4184e-03, 1.7653e-01],\n",
      "        [4.0469e-02, 2.6503e-03, 2.9178e-01],\n",
      "        [2.0993e-02, 1.4032e-03, 2.2404e-01]], device='cuda:0')}, 2: {'step': tensor(19100.), 'exp_avg': tensor([ 4.4142e-04,  1.3679e-05,  1.0496e-04, -2.4364e-04, -1.4691e-04,\n",
      "         3.0379e-04, -5.1951e-04,  3.2308e-05], device='cuda:0'), 'exp_avg_sq': tensor([5.0222e-05, 1.7167e-05, 5.9319e-06, 3.0929e-05, 1.1059e-05, 3.7133e-05,\n",
      "        5.8091e-05, 3.1068e-05], device='cuda:0')}, 3: {'step': tensor(19100.), 'exp_avg': tensor([[-0.1214, -0.0677, -0.0379,  0.1100,  0.0199,  0.0801, -0.0738, -0.0406],\n",
      "        [-0.0146, -0.0082, -0.0042,  0.0129,  0.0027,  0.0093, -0.0083, -0.0045],\n",
      "        [-0.0660, -0.0376, -0.0190,  0.0580,  0.0120,  0.0416, -0.0374, -0.0202],\n",
      "        [ 0.3022,  0.1692,  0.0875, -0.2667, -0.0543, -0.1921,  0.1728,  0.0942],\n",
      "        [-0.0814, -0.0449, -0.0223,  0.0708,  0.0154,  0.0508, -0.0448, -0.0245],\n",
      "        [-0.0297, -0.0217, -0.0080,  0.0241,  0.0066,  0.0162, -0.0146, -0.0063],\n",
      "        [ 0.0604,  0.0329,  0.0219, -0.0579, -0.0076, -0.0431,  0.0416,  0.0234],\n",
      "        [-0.3020, -0.1706, -0.0846,  0.2634,  0.0565,  0.1886, -0.1680, -0.0909]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[0.0677, 0.0386, 0.0071, 0.0469, 0.0048, 0.0239, 0.0223, 0.0058],\n",
      "        [0.0133, 0.0073, 0.0016, 0.0092, 0.0011, 0.0049, 0.0047, 0.0012],\n",
      "        [0.0087, 0.0048, 0.0009, 0.0059, 0.0007, 0.0030, 0.0028, 0.0007],\n",
      "        [0.0690, 0.0309, 0.0049, 0.0457, 0.0044, 0.0228, 0.0176, 0.0050],\n",
      "        [0.0120, 0.0075, 0.0012, 0.0078, 0.0011, 0.0037, 0.0035, 0.0009],\n",
      "        [0.0911, 0.0569, 0.0099, 0.0607, 0.0059, 0.0295, 0.0282, 0.0054],\n",
      "        [0.0933, 0.0521, 0.0093, 0.0624, 0.0074, 0.0326, 0.0288, 0.0084],\n",
      "        [0.0668, 0.0345, 0.0062, 0.0454, 0.0040, 0.0225, 0.0198, 0.0044]],\n",
      "       device='cuda:0')}, 4: {'step': tensor(19100.), 'exp_avg': tensor([-2.8369e-04, -1.7149e-05, -1.2798e-04,  9.1125e-04, -1.0644e-03,\n",
      "         4.1482e-04,  4.3180e-06,  1.1729e-04], device='cuda:0'), 'exp_avg_sq': tensor([8.9742e-05, 5.0061e-05, 1.1640e-04, 1.2607e-04, 1.4674e-04, 1.8260e-04,\n",
      "        7.6925e-05, 3.2737e-04], device='cuda:0')}, 5: {'step': tensor(19100.), 'exp_avg': tensor([[-1.7180e-01,  7.1166e-04, -6.7654e-03,  2.3550e-01,  1.1678e-02,\n",
      "          2.2576e-01,  3.4531e-02, -4.1363e-02],\n",
      "        [ 2.1807e-03,  4.0023e-03, -1.8567e-03, -2.7903e-03, -1.2939e-03,\n",
      "          1.1810e-03, -2.0726e-03, -1.8583e-03],\n",
      "        [-1.8210e-01,  4.6156e-06, -7.3133e-03,  2.4982e-01,  1.2366e-02,\n",
      "          2.3781e-01,  3.7463e-02, -4.3739e-02],\n",
      "        [ 1.3786e-01,  7.6714e-04,  2.2366e-03, -1.8860e-01, -1.0489e-02,\n",
      "         -1.8425e-01, -2.4891e-02,  3.1276e-02],\n",
      "        [-1.9596e-02, -3.9550e-03,  5.3600e-03,  2.6311e-02,  3.6136e-03,\n",
      "          2.9103e-02, -2.6122e-05, -5.7650e-04],\n",
      "        [ 2.5778e-01, -5.8271e-03,  1.2306e-02, -3.5361e-01, -1.6181e-02,\n",
      "         -3.4385e-01, -4.9639e-02,  6.4861e-02],\n",
      "        [-1.5860e-01,  2.2423e-03, -7.8254e-03,  2.1774e-01,  1.0032e-02,\n",
      "          2.0873e-01,  3.2220e-02, -3.9581e-02],\n",
      "        [-5.4778e-02, -8.3196e-03,  1.0811e-03,  7.4764e-02,  5.9180e-03,\n",
      "          6.1770e-02,  1.5697e-02, -8.4952e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[0.0224, 0.0063, 0.0034, 0.0378, 0.0007, 0.0486, 0.0023, 0.0031],\n",
      "        [0.0120, 0.0055, 0.0028, 0.0169, 0.0006, 0.0300, 0.0017, 0.0016],\n",
      "        [0.0258, 0.0101, 0.0047, 0.0392, 0.0010, 0.0550, 0.0031, 0.0036],\n",
      "        [0.0346, 0.0185, 0.0085, 0.0573, 0.0022, 0.0794, 0.0105, 0.0057],\n",
      "        [0.0237, 0.0186, 0.0095, 0.0351, 0.0025, 0.0553, 0.0124, 0.0052],\n",
      "        [0.0392, 0.0167, 0.0084, 0.0620, 0.0018, 0.0877, 0.0080, 0.0059],\n",
      "        [0.0184, 0.0092, 0.0040, 0.0288, 0.0008, 0.0445, 0.0025, 0.0027],\n",
      "        [0.0378, 0.0212, 0.0128, 0.0558, 0.0026, 0.1286, 0.0095, 0.0059]],\n",
      "       device='cuda:0')}, 6: {'step': tensor(19100.), 'exp_avg': tensor([ 4.5601e-04, -4.8811e-04, -3.9293e-05, -5.7603e-04,  3.1463e-04,\n",
      "        -1.3331e-04, -2.9700e-04,  2.9300e-04, -8.0692e-06,  2.0036e-04,\n",
      "        -1.3002e-03, -1.2704e-03], device='cuda:0'), 'exp_avg_sq': tensor([9.5941e-05, 2.1563e-04, 8.5259e-05, 2.6302e-04, 3.2500e-05, 8.3686e-05,\n",
      "        3.2620e-05, 9.4476e-05, 4.0516e-05, 2.6955e-04, 1.9635e-04, 3.2931e-04],\n",
      "       device='cuda:0')}, 7: {'step': tensor(19100.), 'exp_avg': tensor([[ 0.0101,  0.0004,  0.0123, -0.0086,  0.0077, -0.0187,  0.0101,  0.0188],\n",
      "        [-0.0137, -0.0021, -0.0137,  0.0139, -0.0140,  0.0235, -0.0086, -0.0228],\n",
      "        [-0.0111,  0.0007, -0.0142,  0.0078, -0.0070,  0.0206, -0.0120, -0.0223],\n",
      "        [-0.0160,  0.0003, -0.0205,  0.0122, -0.0103,  0.0303, -0.0180, -0.0312],\n",
      "        [-0.0058,  0.0005, -0.0069,  0.0038, -0.0039,  0.0103, -0.0052, -0.0119],\n",
      "        [ 0.0185, -0.0011,  0.0232, -0.0130,  0.0122, -0.0341,  0.0191,  0.0373],\n",
      "        [ 0.0018, -0.0005,  0.0027, -0.0007,  0.0008, -0.0033,  0.0022,  0.0045],\n",
      "        [-0.0202,  0.0006, -0.0242,  0.0151, -0.0148,  0.0365, -0.0187, -0.0398],\n",
      "        [-0.0289,  0.0010, -0.0361,  0.0216, -0.0201,  0.0531, -0.0292, -0.0573],\n",
      "        [-0.0379, -0.0002, -0.0441,  0.0302, -0.0298,  0.0676, -0.0329, -0.0724],\n",
      "        [ 0.0056, -0.0025,  0.0072, -0.0004,  0.0016, -0.0099,  0.0053,  0.0147],\n",
      "        [ 0.0439, -0.0019,  0.0517, -0.0318,  0.0324, -0.0782,  0.0384,  0.0871]],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([[0.0002, 0.0004, 0.0011, 0.0009, 0.0007, 0.0004, 0.0011, 0.0018],\n",
      "        [0.0014, 0.0025, 0.0080, 0.0091, 0.0031, 0.0020, 0.0068, 0.0201],\n",
      "        [0.0001, 0.0001, 0.0004, 0.0006, 0.0003, 0.0002, 0.0005, 0.0012],\n",
      "        [0.0006, 0.0005, 0.0013, 0.0011, 0.0009, 0.0005, 0.0019, 0.0032],\n",
      "        [0.0002, 0.0004, 0.0007, 0.0007, 0.0007, 0.0003, 0.0006, 0.0014],\n",
      "        [0.0002, 0.0003, 0.0009, 0.0011, 0.0007, 0.0004, 0.0008, 0.0018],\n",
      "        [0.0001, 0.0002, 0.0006, 0.0012, 0.0004, 0.0002, 0.0007, 0.0020],\n",
      "        [0.0004, 0.0005, 0.0023, 0.0015, 0.0015, 0.0008, 0.0018, 0.0034],\n",
      "        [0.0004, 0.0003, 0.0014, 0.0006, 0.0006, 0.0012, 0.0012, 0.0026],\n",
      "        [0.0014, 0.0016, 0.0057, 0.0026, 0.0023, 0.0029, 0.0038, 0.0102],\n",
      "        [0.0010, 0.0022, 0.0050, 0.0108, 0.0052, 0.0013, 0.0051, 0.0125],\n",
      "        [0.0021, 0.0017, 0.0072, 0.0074, 0.0068, 0.0039, 0.0062, 0.0137]],\n",
      "       device='cuda:0')}, 8: {'step': tensor(19100.), 'exp_avg': tensor([ 1.2798e-03,  1.2622e-04,  5.9427e-04,  1.7387e-04, -1.6721e-03,\n",
      "        -8.0468e-04,  3.5795e-04,  2.3689e-04,  7.9852e-04, -6.1617e-04,\n",
      "         2.0963e-04,  8.4951e-05], device='cuda:0'), 'exp_avg_sq': tensor([3.9050e-04, 5.6409e-05, 1.3912e-04, 5.4565e-05, 5.0600e-04, 1.8117e-04,\n",
      "        4.4954e-04, 5.4275e-05, 4.1750e-04, 2.2105e-04, 1.2602e-04, 6.0280e-04],\n",
      "       device='cuda:0')}, 9: {'step': tensor(19100.), 'exp_avg': tensor([[-3.3577e-02,  7.2043e-02, -3.4513e-04,  5.3331e-03, -7.1484e-03,\n",
      "         -5.8915e-02, -2.6191e-03,  1.5672e-02,  6.6773e-02,  8.2077e-03,\n",
      "         -2.8113e-02, -4.8332e-02],\n",
      "        [-1.6623e-03,  3.7790e-03, -5.1332e-05,  3.4520e-04, -3.2199e-04,\n",
      "         -2.9543e-03, -8.3739e-05,  7.3163e-04,  3.3048e-03,  4.3946e-04,\n",
      "         -1.3657e-03, -2.4520e-03],\n",
      "        [ 4.6822e-03, -7.9073e-03, -2.2698e-04, -2.0272e-03,  1.0394e-03,\n",
      "          7.0432e-03,  7.0184e-04, -2.7187e-03, -7.7020e-03, -5.9337e-04,\n",
      "          4.0646e-03,  5.6660e-03],\n",
      "        [-6.0335e-03,  1.2620e-02, -4.7439e-05,  1.2553e-03, -1.2610e-03,\n",
      "         -1.0399e-02, -6.2491e-04,  2.9843e-03,  1.1683e-02,  1.3541e-03,\n",
      "         -5.1976e-03, -8.6388e-03],\n",
      "        [-2.8000e-02,  5.3470e-02,  1.9524e-04,  8.1271e-03, -5.7617e-03,\n",
      "         -4.6082e-02, -5.6002e-03,  1.6316e-02,  5.0759e-02,  4.6906e-03,\n",
      "         -2.6673e-02, -3.9909e-02],\n",
      "        [-2.2839e-02,  4.5222e-02, -3.3177e-05,  5.8831e-03, -4.6558e-03,\n",
      "         -3.8359e-02, -4.1973e-03,  1.2835e-02,  4.2445e-02,  4.2470e-03,\n",
      "         -2.1506e-02, -3.3174e-02],\n",
      "        [-8.0445e-03,  1.7553e-02, -3.3376e-04,  1.7249e-03, -1.4465e-03,\n",
      "         -1.4272e-02, -1.6646e-03,  4.4848e-03,  1.5676e-02,  1.7405e-03,\n",
      "         -7.9923e-03, -1.2952e-02],\n",
      "        [ 1.0244e-02, -2.0542e-02,  4.8074e-05, -2.6741e-03,  2.0667e-03,\n",
      "          1.7264e-02,  1.7665e-03, -5.6593e-03, -1.9094e-02, -1.9630e-03,\n",
      "          9.5429e-03,  1.4891e-02],\n",
      "        [ 2.7734e-02, -5.6344e-02,  8.1418e-04, -6.3243e-03,  5.0535e-03,\n",
      "          4.8064e-02,  8.5113e-03, -1.7701e-02, -5.2341e-02, -4.7970e-03,\n",
      "          3.0257e-02,  4.5389e-02],\n",
      "        [ 3.1838e-02, -6.7711e-02,  7.4997e-04, -5.2680e-03,  6.3192e-03,\n",
      "          5.6209e-02,  5.7396e-03, -1.7146e-02, -6.2755e-02, -7.0122e-03,\n",
      "          3.0388e-02,  4.9369e-02],\n",
      "        [-4.1641e-02,  8.7173e-02, -6.7828e-04,  7.6407e-03, -8.4030e-03,\n",
      "         -7.2644e-02, -7.1288e-03,  2.2321e-02,  8.1107e-02,  8.9699e-03,\n",
      "         -3.9122e-02, -6.3133e-02],\n",
      "        [-5.8043e-02,  1.2120e-01, -1.3504e-03,  1.1252e-02, -1.1268e-02,\n",
      "         -1.0145e-01, -1.2468e-02,  3.2954e-02,  1.1237e-01,  1.1848e-02,\n",
      "         -5.7474e-02, -9.0815e-02]], device='cuda:0'), 'exp_avg_sq': tensor([[2.0497e-03, 1.4725e-02, 5.3664e-04, 5.0410e-03, 3.5983e-04, 3.6040e-03,\n",
      "         1.3718e-02, 5.6621e-03, 5.9752e-03, 1.3856e-03, 9.7375e-03, 7.2631e-03],\n",
      "        [7.1541e-05, 4.6009e-04, 1.2325e-05, 2.8509e-04, 1.3521e-05, 1.3276e-04,\n",
      "         1.4778e-04, 1.0830e-04, 1.3435e-04, 3.6323e-05, 1.4072e-04, 1.8275e-04],\n",
      "        [3.6550e-04, 2.5139e-03, 1.0543e-04, 1.0822e-03, 7.1398e-05, 7.7589e-04,\n",
      "         8.5671e-04, 6.3885e-04, 8.1596e-04, 2.1659e-04, 1.0729e-03, 1.5439e-03],\n",
      "        [2.1859e-04, 1.6640e-03, 4.5911e-05, 3.9135e-04, 3.2284e-05, 6.6766e-04,\n",
      "         2.9010e-04, 2.3128e-04, 8.7018e-04, 7.0388e-05, 3.9253e-04, 7.0055e-04],\n",
      "        [1.4001e-03, 1.1545e-02, 2.7521e-04, 3.5613e-03, 2.1699e-04, 3.8946e-03,\n",
      "         3.4381e-03, 1.4759e-03, 4.6872e-03, 4.8791e-04, 2.5776e-03, 3.9645e-03],\n",
      "        [4.0878e-04, 2.7194e-03, 1.2246e-04, 7.4497e-04, 6.5781e-05, 8.3783e-04,\n",
      "         1.7326e-03, 5.6684e-04, 1.1741e-03, 1.9011e-04, 9.7303e-04, 8.1316e-04],\n",
      "        [5.8946e-04, 5.1189e-03, 1.1233e-04, 1.4829e-03, 1.0530e-04, 1.4768e-03,\n",
      "         1.3562e-03, 9.7961e-04, 1.8684e-03, 4.4471e-04, 1.1240e-03, 1.4533e-03],\n",
      "        [7.6103e-05, 4.5845e-04, 2.5633e-05, 2.1824e-04, 1.1531e-05, 1.9275e-04,\n",
      "         1.5644e-04, 1.7945e-04, 2.6642e-04, 6.3648e-05, 2.4250e-04, 2.7801e-04],\n",
      "        [1.4892e-03, 1.4708e-02, 5.9584e-04, 3.3973e-03, 2.7930e-04, 4.1571e-03,\n",
      "         4.2326e-03, 3.2536e-03, 5.0198e-03, 1.8581e-03, 3.2800e-03, 3.1285e-03],\n",
      "        [1.0822e-03, 6.8127e-03, 1.3297e-04, 2.1358e-03, 1.3845e-04, 2.5542e-03,\n",
      "         2.5381e-03, 1.4458e-03, 2.1278e-03, 3.1434e-04, 2.8884e-03, 4.0052e-03],\n",
      "        [7.3623e-04, 4.5681e-03, 1.1931e-04, 1.1381e-03, 8.1547e-05, 1.9156e-03,\n",
      "         1.6179e-03, 8.7558e-04, 2.2749e-03, 2.9828e-04, 1.6320e-03, 2.2212e-03],\n",
      "        [2.6942e-03, 2.4141e-02, 5.0030e-04, 3.9950e-03, 4.5348e-04, 8.1098e-03,\n",
      "         4.8196e-03, 2.7534e-03, 9.9040e-03, 1.4236e-03, 4.0377e-03, 7.2941e-03]],\n",
      "       device='cuda:0')}, 10: {'step': tensor(19100.), 'exp_avg': tensor([-3.5882e-04, -2.1561e-03,  7.1239e-04,  1.7196e-03, -1.9509e-04,\n",
      "         1.6004e-03, -1.1087e-03,  5.6200e-05, -1.6787e-05,  5.8694e-04,\n",
      "        -5.0832e-04,  5.6960e-04, -1.5317e-03,  1.1184e-03, -2.0473e-03,\n",
      "        -1.3325e-03], device='cuda:0'), 'exp_avg_sq': tensor([1.3815e-04, 1.0044e-03, 4.3977e-04, 3.2415e-04, 8.0846e-04, 4.2243e-04,\n",
      "        1.1707e-04, 1.3246e-04, 3.4339e-04, 1.6068e-04, 1.4759e-04, 1.6114e-04,\n",
      "        1.3185e-03, 8.8205e-05, 1.0895e-03, 1.5033e-03], device='cuda:0')}, 11: {'step': tensor(19100.), 'exp_avg': tensor([[ 0.0124,  0.0034, -0.0008,  0.0020,  0.0040,  0.0087,  0.0091,  0.0052,\n",
      "         -0.0009, -0.0118,  0.0057,  0.0165],\n",
      "        [-0.0117, -0.0037,  0.0011, -0.0026, -0.0041, -0.0092, -0.0115, -0.0042,\n",
      "          0.0034,  0.0148, -0.0067, -0.0233],\n",
      "        [-0.0218, -0.0066,  0.0023, -0.0041, -0.0089, -0.0187, -0.0175, -0.0082,\n",
      "          0.0038,  0.0216, -0.0103, -0.0326],\n",
      "        [-0.1023, -0.0302,  0.0095, -0.0187, -0.0401, -0.0842, -0.0816, -0.0400,\n",
      "          0.0159,  0.1025, -0.0480, -0.1520],\n",
      "        [-0.0384, -0.0107,  0.0025, -0.0065, -0.0119, -0.0263, -0.0297, -0.0159,\n",
      "          0.0039,  0.0388, -0.0185, -0.0550],\n",
      "        [ 0.0196,  0.0066, -0.0026,  0.0044,  0.0092,  0.0189,  0.0187,  0.0065,\n",
      "         -0.0063, -0.0229,  0.0103,  0.0371],\n",
      "        [ 0.0429,  0.0127, -0.0040,  0.0078,  0.0171,  0.0357,  0.0341,  0.0167,\n",
      "         -0.0067, -0.0426,  0.0200,  0.0632],\n",
      "        [ 0.0141,  0.0044, -0.0016,  0.0028,  0.0061,  0.0126,  0.0119,  0.0052,\n",
      "         -0.0030, -0.0146,  0.0068,  0.0225],\n",
      "        [-0.0584, -0.0174,  0.0056, -0.0109, -0.0232, -0.0487, -0.0477, -0.0225,\n",
      "          0.0100,  0.0599, -0.0278, -0.0897],\n",
      "        [-0.0113, -0.0033,  0.0010, -0.0020, -0.0045, -0.0094, -0.0087, -0.0045,\n",
      "          0.0016,  0.0109, -0.0051, -0.0159],\n",
      "        [ 0.0336,  0.0098, -0.0029,  0.0060,  0.0125,  0.0266,  0.0265,  0.0133,\n",
      "         -0.0047, -0.0335,  0.0158,  0.0491],\n",
      "        [-0.0352, -0.0106,  0.0035, -0.0066, -0.0146, -0.0304, -0.0285, -0.0135,\n",
      "          0.0062,  0.0355, -0.0165, -0.0534],\n",
      "        [-0.0074, -0.0023,  0.0008, -0.0017, -0.0030, -0.0064, -0.0074, -0.0026,\n",
      "          0.0023,  0.0096, -0.0041, -0.0151],\n",
      "        [-0.0262, -0.0076,  0.0023, -0.0046, -0.0101, -0.0212, -0.0201, -0.0104,\n",
      "          0.0033,  0.0252, -0.0120, -0.0367],\n",
      "        [ 0.0502,  0.0146, -0.0046,  0.0089,  0.0196,  0.0411,  0.0388,  0.0199,\n",
      "         -0.0068, -0.0486,  0.0230,  0.0712],\n",
      "        [-0.0374, -0.0124,  0.0050, -0.0082, -0.0181, -0.0370, -0.0342, -0.0126,\n",
      "          0.0110,  0.0412, -0.0188, -0.0665]], device='cuda:0'), 'exp_avg_sq': tensor([[8.2283e-04, 2.7410e-05, 3.2256e-05, 2.0161e-05, 9.2683e-05, 1.4512e-04,\n",
      "         1.6282e-04, 2.9072e-04, 2.6940e-04, 3.4011e-04, 5.5707e-05, 7.8038e-04],\n",
      "        [1.0012e-02, 3.1813e-04, 3.1925e-04, 3.8280e-04, 1.1792e-03, 1.9855e-03,\n",
      "         2.2286e-03, 3.1617e-03, 1.6037e-03, 5.2295e-03, 8.2735e-04, 9.4156e-03],\n",
      "        [5.9438e-03, 9.2091e-05, 1.9839e-04, 8.3100e-05, 4.6552e-04, 7.7973e-04,\n",
      "         9.8044e-04, 1.9523e-03, 7.6014e-04, 1.2907e-03, 3.6052e-04, 3.6087e-03],\n",
      "        [4.5558e-03, 2.3357e-04, 1.9170e-04, 1.1676e-04, 1.2239e-03, 2.4619e-03,\n",
      "         1.8662e-03, 1.2973e-03, 1.0390e-03, 3.6908e-03, 5.8531e-04, 7.6981e-03],\n",
      "        [8.9750e-03, 5.3998e-04, 4.0439e-04, 4.4443e-04, 2.0678e-03, 2.8833e-03,\n",
      "         2.9944e-03, 3.1514e-03, 4.2831e-03, 7.4437e-03, 1.0134e-03, 1.5743e-02],\n",
      "        [1.0894e-02, 2.6603e-04, 4.2499e-04, 3.0674e-04, 1.0983e-03, 1.8010e-03,\n",
      "         1.7671e-03, 4.1085e-03, 2.1900e-03, 4.5287e-03, 7.5328e-04, 6.3265e-03],\n",
      "        [1.7698e-03, 4.4354e-05, 5.6499e-05, 2.7343e-05, 1.9066e-04, 4.3246e-04,\n",
      "         4.4016e-04, 4.8097e-04, 2.2773e-04, 6.4894e-04, 1.3380e-04, 1.7608e-03],\n",
      "        [6.3756e-04, 3.1438e-05, 1.8160e-05, 1.8737e-05, 1.2446e-04, 1.7970e-04,\n",
      "         1.2927e-04, 2.6828e-04, 2.7738e-04, 3.1630e-04, 5.1551e-05, 5.2237e-04],\n",
      "        [4.1245e-03, 1.5338e-04, 1.9315e-04, 7.6223e-05, 9.3023e-04, 1.3158e-03,\n",
      "         1.2011e-03, 1.6058e-03, 1.1252e-03, 4.0290e-03, 3.4071e-04, 4.9086e-03],\n",
      "        [3.5729e-04, 1.5737e-05, 1.6169e-05, 1.3264e-05, 8.3863e-05, 1.4240e-04,\n",
      "         9.9627e-05, 1.6862e-04, 1.6141e-04, 2.6111e-04, 3.4953e-05, 3.7093e-04],\n",
      "        [1.4970e-03, 6.7762e-05, 8.3178e-05, 3.5584e-05, 6.1182e-04, 6.8009e-04,\n",
      "         4.8322e-04, 4.6512e-04, 3.2899e-04, 1.5358e-03, 1.4835e-04, 2.6273e-03],\n",
      "        [2.3307e-03, 5.8000e-05, 6.5449e-05, 2.7140e-05, 2.7532e-04, 4.8137e-04,\n",
      "         4.6927e-04, 8.0366e-04, 5.6774e-04, 6.6849e-04, 1.5972e-04, 1.7358e-03],\n",
      "        [1.2196e-02, 1.8032e-04, 3.3766e-04, 1.5472e-04, 9.7313e-04, 1.2999e-03,\n",
      "         2.2812e-03, 3.6342e-03, 1.2151e-03, 3.9104e-03, 7.4298e-04, 8.3636e-03],\n",
      "        [1.3187e-03, 3.5681e-05, 4.7479e-05, 1.6129e-05, 3.5394e-04, 4.2996e-04,\n",
      "         2.9370e-04, 3.8504e-04, 1.9209e-04, 6.6760e-04, 1.0496e-04, 1.4834e-03],\n",
      "        [6.2152e-03, 1.8397e-04, 1.9698e-04, 1.3398e-04, 1.3738e-03, 1.9881e-03,\n",
      "         1.2580e-03, 1.9403e-03, 1.0482e-03, 2.1777e-03, 4.6566e-04, 5.6340e-03],\n",
      "        [2.3931e-02, 4.8638e-04, 6.9666e-04, 2.8059e-04, 2.3298e-03, 3.7141e-03,\n",
      "         2.8016e-03, 1.0643e-02, 8.4067e-03, 7.3895e-03, 1.2526e-03, 1.0492e-02]],\n",
      "       device='cuda:0')}, 12: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0003, -0.0003, -0.0002,  0.0005, -0.0011, -0.0008,  0.0031,  0.0007,\n",
      "         0.0006, -0.0007, -0.0006,  0.0023, -0.0009,  0.0011,  0.0007,  0.0032],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0003, 0.0003, 0.0003, 0.0002, 0.0016, 0.0003, 0.0005, 0.0014, 0.0010,\n",
      "        0.0017, 0.0005, 0.0009, 0.0006, 0.0002, 0.0006, 0.0011],\n",
      "       device='cuda:0')}, 13: {'step': tensor(19100.), 'exp_avg': tensor([[ 5.0765e-03,  1.1110e-03,  2.5008e-03, -7.3537e-03, -8.8995e-03,\n",
      "          4.5090e-03, -1.9784e-04,  1.4240e-03, -2.7852e-03, -8.8240e-04,\n",
      "          2.1497e-03, -2.9251e-03, -4.6633e-03, -6.2965e-04,  4.4310e-03,\n",
      "          8.1026e-04],\n",
      "        [ 1.7468e-02,  1.3931e-03,  9.5236e-03, -2.3191e-02, -3.0479e-02,\n",
      "          1.6807e-02, -6.2416e-04,  6.8523e-03, -1.2615e-02, -4.3843e-03,\n",
      "          5.6872e-03, -9.6885e-03, -1.5439e-02, -1.3332e-03,  1.1837e-02,\n",
      "         -2.9755e-03],\n",
      "        [-3.6198e-03, -3.3726e-04, -1.9565e-03,  4.9208e-03,  6.4000e-03,\n",
      "         -3.3820e-03,  2.0855e-04, -1.3105e-03,  2.4717e-03,  7.7138e-04,\n",
      "         -1.2532e-03,  1.9982e-03,  3.1933e-03,  3.1790e-04, -2.6468e-03,\n",
      "          2.4521e-04],\n",
      "        [-2.6802e-02, -2.3422e-03, -1.4638e-02,  3.5776e-02,  4.6699e-02,\n",
      "         -2.5914e-02,  1.0464e-03, -1.0433e-02,  1.9392e-02,  6.7690e-03,\n",
      "         -8.9204e-03,  1.4885e-02,  2.3916e-02,  2.1777e-03, -1.8447e-02,\n",
      "          4.6091e-03],\n",
      "        [-1.1222e-03,  1.8193e-03, -1.0184e-03,  3.1614e-04,  2.6708e-03,\n",
      "         -8.5865e-04,  2.7012e-04, -1.2563e-03,  1.7734e-03,  1.2609e-04,\n",
      "          7.9575e-04,  2.7093e-04, -1.5225e-04, -5.4259e-04,  1.0392e-03,\n",
      "          1.3708e-03],\n",
      "        [-1.1493e-02,  2.5344e-04, -6.4725e-03,  1.4398e-02,  2.0457e-02,\n",
      "         -1.0733e-02,  3.6591e-04, -5.0274e-03,  8.5676e-03,  2.6866e-03,\n",
      "         -2.7610e-03,  6.2683e-03,  9.2221e-03,  3.4996e-04, -6.3926e-03,\n",
      "          2.2659e-03],\n",
      "        [-2.2005e-02, -2.9036e-03, -1.2142e-02,  2.9905e-02,  3.7432e-02,\n",
      "         -2.2679e-02,  9.0698e-04, -8.6488e-03,  1.7107e-02,  6.7282e-03,\n",
      "         -8.3028e-03,  1.2248e-02,  2.1178e-02,  2.3771e-03, -1.5987e-02,\n",
      "          6.4509e-03],\n",
      "        [-1.2586e-02, -2.0738e-03, -6.6704e-03,  1.7427e-02,  2.1603e-02,\n",
      "         -1.2244e-02,  4.1783e-04, -4.4432e-03,  8.5632e-03,  3.1973e-03,\n",
      "         -4.7867e-03,  7.1542e-03,  1.1804e-02,  1.3427e-03, -9.6355e-03,\n",
      "          1.4272e-03],\n",
      "        [-2.2134e-02, -2.8903e-03, -1.1849e-02,  3.0162e-02,  3.8299e-02,\n",
      "         -2.1336e-02,  7.5076e-04, -8.1322e-03,  1.5305e-02,  5.5033e-03,\n",
      "         -7.9063e-03,  1.2489e-02,  2.0203e-02,  2.0848e-03, -1.6173e-02,\n",
      "          2.7449e-03],\n",
      "        [ 5.2907e-02,  2.0410e-03,  2.9647e-02, -6.9089e-02, -9.2881e-02,\n",
      "          5.1663e-02, -2.5788e-03,  2.1955e-02, -4.0686e-02, -1.3801e-02,\n",
      "          1.6417e-02, -2.8757e-02, -4.6324e-02, -3.6824e-03,  3.4129e-02,\n",
      "         -1.2702e-02],\n",
      "        [-3.5955e-02, -2.9813e-03, -1.9455e-02,  4.7863e-02,  6.2982e-02,\n",
      "         -3.3963e-02,  1.2329e-03, -1.3802e-02,  2.5049e-02,  8.4462e-03,\n",
      "         -1.1553e-02,  2.0062e-02,  3.1370e-02,  2.6544e-03, -2.4513e-02,\n",
      "          4.3484e-03],\n",
      "        [-7.3176e-02, -7.7118e-03, -3.9743e-02,  9.8625e-02,  1.2704e-01,\n",
      "         -7.1087e-02,  2.8895e-03, -2.7904e-02,  5.2577e-02,  1.8671e-02,\n",
      "         -2.5391e-02,  4.0786e-02,  6.6310e-02,  6.5091e-03, -5.1880e-02,\n",
      "          1.2140e-02],\n",
      "        [-2.6376e-02, -2.0177e-03, -1.4255e-02,  3.5116e-02,  4.6418e-02,\n",
      "         -2.4704e-02,  1.0101e-03, -1.0056e-02,  1.8260e-02,  5.9822e-03,\n",
      "         -8.4549e-03,  1.4644e-02,  2.2833e-02,  1.9180e-03, -1.8040e-02,\n",
      "          2.7675e-03],\n",
      "        [-1.1213e-02, -9.2808e-04, -6.2976e-03,  1.4943e-02,  1.9332e-02,\n",
      "         -1.1441e-02,  5.6925e-04, -4.6004e-03,  8.9437e-03,  3.3189e-03,\n",
      "         -3.9260e-03,  6.1290e-03,  1.0445e-02,  1.0458e-03, -7.7031e-03,\n",
      "          3.4697e-03],\n",
      "        [ 8.6621e-03, -4.6961e-05,  4.7827e-03, -1.1225e-02, -1.5628e-02,\n",
      "          7.9620e-03, -5.1368e-04,  3.5058e-03, -6.3466e-03, -1.8269e-03,\n",
      "          2.5343e-03, -4.6076e-03, -7.1047e-03, -4.7514e-04,  5.5257e-03,\n",
      "         -1.1568e-03],\n",
      "        [-1.5937e-02, -1.1336e-03, -9.2396e-03,  2.0927e-02,  2.7015e-02,\n",
      "         -1.7255e-02,  7.9813e-04, -7.1020e-03,  1.4074e-02,  5.6318e-03,\n",
      "         -5.5970e-03,  8.6303e-03,  1.5399e-02,  1.5467e-03, -1.0434e-02,\n",
      "          7.7142e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.7113e-04, 1.9090e-04, 1.0434e-04, 3.0484e-04, 7.9831e-04, 3.4584e-04,\n",
      "         4.4506e-04, 8.5604e-05, 5.3820e-04, 7.3766e-05, 1.7023e-04, 1.5774e-04,\n",
      "         4.8922e-04, 1.3606e-04, 4.5148e-04, 7.8426e-04],\n",
      "        [3.9332e-04, 1.8918e-04, 2.6112e-04, 6.1154e-04, 1.5728e-03, 6.3975e-04,\n",
      "         4.6397e-04, 9.7793e-05, 4.3772e-04, 6.3160e-05, 2.5539e-04, 2.6361e-04,\n",
      "         8.8495e-04, 2.4380e-04, 4.1757e-04, 8.2562e-04],\n",
      "        [2.3388e-04, 1.9573e-04, 2.6878e-04, 3.0084e-04, 9.1125e-04, 6.0172e-04,\n",
      "         5.7814e-04, 8.2340e-05, 5.4303e-04, 6.7714e-05, 2.1699e-04, 2.7354e-04,\n",
      "         7.2063e-04, 1.6460e-04, 3.3553e-04, 9.4474e-04],\n",
      "        [3.1563e-04, 2.1586e-04, 1.5507e-04, 5.4366e-04, 1.3736e-03, 5.9786e-04,\n",
      "         4.2448e-04, 1.1120e-04, 5.2797e-04, 7.5605e-05, 2.7107e-04, 2.3426e-04,\n",
      "         7.8898e-04, 1.8058e-04, 4.5105e-04, 1.0757e-03],\n",
      "        [1.4762e-03, 8.0361e-04, 1.1138e-03, 1.9092e-03, 6.1331e-03, 1.1781e-03,\n",
      "         1.1259e-03, 2.9610e-04, 1.3088e-03, 1.9712e-04, 7.8073e-04, 3.9076e-04,\n",
      "         9.7577e-04, 3.4688e-04, 1.1937e-03, 1.4383e-03],\n",
      "        [7.7584e-04, 4.8091e-04, 7.5181e-04, 1.1076e-03, 2.8262e-03, 9.7871e-04,\n",
      "         9.2512e-04, 1.7225e-04, 1.2029e-03, 9.1294e-05, 3.8991e-04, 2.9115e-04,\n",
      "         7.1043e-04, 2.0386e-04, 7.1307e-04, 1.1028e-03],\n",
      "        [6.4949e-04, 4.6122e-04, 6.1990e-04, 9.4995e-04, 2.1885e-03, 1.1937e-03,\n",
      "         7.9084e-04, 2.1414e-04, 8.9497e-04, 1.3772e-04, 5.1052e-04, 4.3131e-04,\n",
      "         1.3313e-03, 3.5818e-04, 7.2851e-04, 1.8790e-03],\n",
      "        [3.4600e-04, 3.9737e-04, 2.7714e-04, 8.0721e-04, 1.7887e-03, 5.4280e-04,\n",
      "         8.5650e-04, 1.7974e-04, 9.7157e-04, 1.1087e-04, 5.0481e-04, 1.9356e-04,\n",
      "         7.2745e-04, 2.6766e-04, 9.3619e-04, 8.7539e-04],\n",
      "        [4.9420e-04, 2.6888e-04, 3.0894e-04, 7.2288e-04, 1.7969e-03, 6.2629e-04,\n",
      "         8.4002e-04, 1.9008e-04, 8.2172e-04, 1.1149e-04, 4.4178e-04, 2.7874e-04,\n",
      "         6.1931e-04, 2.1190e-04, 8.0779e-04, 1.3740e-03],\n",
      "        [3.4247e-03, 1.4687e-03, 1.8616e-03, 5.1356e-03, 1.3924e-02, 2.6412e-03,\n",
      "         2.0415e-03, 5.8900e-04, 2.7865e-03, 4.2839e-04, 1.5320e-03, 7.8743e-04,\n",
      "         2.6232e-03, 9.5962e-04, 3.0762e-03, 3.2423e-03],\n",
      "        [7.1442e-04, 6.4167e-04, 6.3242e-04, 1.1463e-03, 2.7720e-03, 1.3450e-03,\n",
      "         1.0919e-03, 2.9912e-04, 1.6968e-03, 1.8673e-04, 5.1069e-04, 4.2257e-04,\n",
      "         1.2439e-03, 3.0308e-04, 9.2051e-04, 2.8962e-03],\n",
      "        [1.5725e-03, 7.2832e-04, 5.9468e-04, 2.6559e-03, 5.7579e-03, 2.2413e-03,\n",
      "         1.2327e-03, 4.3338e-04, 1.8191e-03, 2.4075e-04, 9.4398e-04, 9.5170e-04,\n",
      "         3.0769e-03, 7.1953e-04, 1.7314e-03, 3.8941e-03],\n",
      "        [9.2059e-04, 5.0733e-04, 9.8292e-04, 1.3861e-03, 2.8445e-03, 1.1511e-03,\n",
      "         8.0018e-04, 2.8030e-04, 1.0729e-03, 1.4488e-04, 6.1297e-04, 2.7958e-04,\n",
      "         6.8152e-04, 2.5042e-04, 8.5622e-04, 1.5308e-03],\n",
      "        [1.5598e-04, 1.5001e-04, 9.7590e-05, 2.7399e-04, 6.8064e-04, 2.9283e-04,\n",
      "         1.9387e-04, 5.8763e-05, 2.4424e-04, 5.2697e-05, 1.8681e-04, 9.0079e-05,\n",
      "         3.4299e-04, 1.0800e-04, 2.6652e-04, 5.5633e-04],\n",
      "        [7.7549e-04, 3.1467e-04, 6.0322e-04, 8.3784e-04, 2.2772e-03, 6.1015e-04,\n",
      "         4.6800e-04, 1.6636e-04, 3.8846e-04, 6.9889e-05, 2.5765e-04, 1.9647e-04,\n",
      "         3.8485e-04, 2.0523e-04, 4.2303e-04, 6.1944e-04],\n",
      "        [1.3072e-03, 1.1217e-03, 9.1760e-04, 1.9306e-03, 5.4952e-03, 3.4484e-03,\n",
      "         2.1580e-03, 5.1463e-04, 2.1755e-03, 4.4648e-04, 1.4304e-03, 1.4583e-03,\n",
      "         5.2401e-03, 1.3609e-03, 2.0282e-03, 6.6732e-03]], device='cuda:0')}, 14: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0051, -0.0037, -0.0040,  0.0061,  0.0033,  0.0003,  0.0033, -0.0021,\n",
      "         0.0008, -0.0055,  0.0024,  0.0025,  0.0027,  0.0031,  0.0054,  0.0051],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0020, 0.0011, 0.0005, 0.0012, 0.0013, 0.0014, 0.0009, 0.0003, 0.0008,\n",
      "        0.0024, 0.0035, 0.0019, 0.0024, 0.0007, 0.0009, 0.0011],\n",
      "       device='cuda:0')}, 15: {'step': tensor(19100.), 'exp_avg': tensor([[-1.9502e-02,  1.2065e-02,  1.3812e-03,  5.5867e-02,  3.3000e-02,\n",
      "         -3.3135e-03,  1.1883e-02, -1.7024e-02,  3.3756e-02, -8.3783e-02,\n",
      "          1.8105e-02,  5.3151e-02, -3.7314e-03,  2.6391e-03, -3.0061e-02,\n",
      "         -5.0188e-04],\n",
      "        [ 2.1471e-02, -1.3375e-02, -1.0796e-03, -6.2977e-02, -3.8700e-02,\n",
      "          3.6265e-03, -1.2041e-02,  1.9235e-02, -3.7396e-02,  9.4481e-02,\n",
      "         -2.1418e-02, -5.8298e-02,  3.0264e-03, -2.2970e-03,  3.4994e-02,\n",
      "          8.4275e-04],\n",
      "        [ 5.7186e-03, -3.5115e-03, -6.7623e-04, -1.5215e-02, -6.8354e-03,\n",
      "          7.9758e-04, -4.4128e-03,  4.7141e-03, -9.8292e-03,  2.1835e-02,\n",
      "         -4.7690e-03, -1.6138e-02,  1.4091e-03, -1.2051e-03,  7.1344e-03,\n",
      "          3.5500e-04],\n",
      "        [-2.5338e-02,  1.5697e-02,  1.6897e-03,  7.2965e-02,  4.3549e-02,\n",
      "         -4.3299e-03,  1.5181e-02, -2.2225e-02,  4.3905e-02, -1.0951e-01,\n",
      "          2.3845e-02,  6.8991e-02, -4.6156e-03,  3.2846e-03, -3.9559e-02,\n",
      "         -6.8098e-04],\n",
      "        [-1.1030e-02,  6.8681e-03,  5.9433e-04,  3.1935e-02,  1.8441e-02,\n",
      "         -1.7279e-03,  6.5568e-03, -9.8169e-03,  1.9208e-02, -4.7137e-02,\n",
      "          1.1093e-02,  3.0288e-02, -1.4178e-03,  1.3110e-03, -1.7316e-02,\n",
      "         -6.7297e-04],\n",
      "        [ 1.7836e-03, -1.0089e-03,  6.2283e-04, -5.7419e-03, -3.6991e-03,\n",
      "         -7.6045e-04,  6.5625e-04,  2.2453e-03, -3.0398e-03,  7.0876e-03,\n",
      "         -4.2752e-03, -3.7840e-03, -1.2754e-03,  6.2438e-04,  3.8486e-03,\n",
      "          1.1589e-03],\n",
      "        [-1.2716e-02,  7.9038e-03,  8.9464e-04,  3.6608e-02,  2.1612e-02,\n",
      "         -2.2558e-03,  7.8209e-03, -1.1124e-02,  2.2079e-02, -5.4874e-02,\n",
      "          1.1877e-02,  3.4847e-02, -2.3146e-03,  1.7134e-03, -1.9756e-02,\n",
      "         -3.3106e-04],\n",
      "        [ 9.0705e-03, -5.6580e-03, -6.5214e-04, -2.6256e-02, -1.5747e-02,\n",
      "          1.7025e-03, -5.5736e-03,  7.9386e-03, -1.5783e-02,  3.9556e-02,\n",
      "         -8.4122e-03, -2.4894e-02,  1.6824e-03, -1.2155e-03,  1.4264e-02,\n",
      "          1.5958e-04],\n",
      "        [-1.3063e-03,  8.1386e-04,  1.4604e-05,  3.8221e-03,  1.8774e-03,\n",
      "         -2.3819e-04,  8.8376e-04, -1.1500e-03,  2.2715e-03, -5.3676e-03,\n",
      "          1.4597e-03,  3.7546e-03,  1.5687e-05,  1.6433e-04, -2.0446e-03,\n",
      "         -1.2058e-04],\n",
      "        [ 1.9427e-02, -1.1935e-02,  2.7101e-04, -5.7115e-02, -3.2609e-02,\n",
      "          1.0682e-03, -8.7411e-03,  1.8480e-02, -3.3762e-02,  8.0983e-02,\n",
      "         -2.4321e-02, -5.1739e-02, -5.4630e-04, -8.5713e-04,  3.2004e-02,\n",
      "          3.3602e-03],\n",
      "        [-2.2308e-02,  1.3996e-02,  1.6384e-03,  6.5670e-02,  4.1836e-02,\n",
      "         -4.8381e-03,  1.3350e-02, -1.9564e-02,  3.8940e-02, -1.0079e-01,\n",
      "          2.0169e-02,  6.1031e-02, -4.5021e-03,  2.8560e-03, -3.6579e-02,\n",
      "          3.1742e-04],\n",
      "        [ 1.1188e-02, -7.1235e-03, -1.1134e-03, -3.4095e-02, -2.5096e-02,\n",
      "          3.6033e-03, -6.6829e-03,  9.6247e-03, -1.9640e-02,  5.5495e-02,\n",
      "         -8.4169e-03, -3.0459e-02,  3.4402e-03, -1.5302e-03,  1.9953e-02,\n",
      "         -1.5578e-03],\n",
      "        [-5.2914e-03,  3.0821e-03, -1.0884e-03,  1.5629e-02,  7.3871e-03,\n",
      "          1.5614e-03,  4.1077e-04, -5.9124e-03,  9.0726e-03, -1.8678e-02,\n",
      "          1.0522e-02,  1.2942e-02,  2.6274e-03, -8.0069e-04, -9.0750e-03,\n",
      "         -2.9377e-03],\n",
      "        [-1.2704e-02,  7.8504e-03,  3.2083e-04,  3.7106e-02,  2.1857e-02,\n",
      "         -1.4817e-03,  6.6139e-03, -1.1640e-02,  2.2073e-02, -5.4247e-02,\n",
      "          1.3964e-02,  3.4217e-02, -9.4365e-04,  1.0717e-03, -2.0556e-02,\n",
      "         -1.2431e-03],\n",
      "        [-2.0703e-02,  1.2794e-02,  8.8459e-04,  5.9937e-02,  3.5120e-02,\n",
      "         -2.8111e-03,  1.1545e-02, -1.8603e-02,  3.5905e-02, -8.8326e-02,\n",
      "          2.1423e-02,  5.6068e-02, -2.4445e-03,  2.1765e-03, -3.2764e-02,\n",
      "         -1.4925e-03],\n",
      "        [-1.2438e-02,  7.6872e-03,  7.3219e-04,  3.5261e-02,  1.9087e-02,\n",
      "         -1.6469e-03,  7.6879e-03, -1.0972e-02,  2.1554e-02, -5.1252e-02,\n",
      "          1.2464e-02,  3.4212e-02, -1.6583e-03,  1.6332e-03, -1.8543e-02,\n",
      "         -1.0134e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[5.2789e-04, 1.6809e-04, 1.1015e-04, 9.7282e-04, 9.3714e-04, 5.4670e-04,\n",
      "         2.1284e-04, 5.6382e-04, 6.8410e-04, 2.6168e-03, 8.4925e-04, 9.4089e-04,\n",
      "         7.5826e-04, 2.1572e-04, 3.0707e-04, 7.4061e-04],\n",
      "        [7.3014e-04, 1.6427e-04, 2.8233e-04, 1.9102e-03, 1.5822e-03, 7.1663e-04,\n",
      "         3.5357e-04, 7.7189e-04, 1.3039e-03, 4.9366e-03, 1.9110e-03, 1.3744e-03,\n",
      "         1.5135e-03, 4.0033e-04, 6.5113e-04, 1.3409e-03],\n",
      "        [2.6681e-04, 1.4201e-04, 1.5124e-04, 8.0938e-04, 1.5047e-03, 3.5939e-04,\n",
      "         2.3808e-04, 5.9002e-04, 5.4458e-04, 2.2280e-03, 1.0726e-03, 4.1695e-04,\n",
      "         1.0090e-03, 2.8295e-04, 7.1860e-04, 7.3899e-04],\n",
      "        [3.6335e-04, 1.0887e-04, 8.2310e-05, 1.1769e-03, 8.0000e-04, 2.8130e-04,\n",
      "         1.7445e-04, 3.4017e-04, 6.2432e-04, 3.2147e-03, 5.6018e-04, 1.0555e-03,\n",
      "         4.7832e-04, 1.3171e-04, 3.6057e-04, 5.5570e-04],\n",
      "        [3.4265e-04, 1.0130e-04, 1.0703e-04, 6.8560e-04, 1.2086e-03, 4.2879e-04,\n",
      "         1.6570e-04, 4.7658e-04, 6.1643e-04, 2.1472e-03, 7.0430e-04, 6.3470e-04,\n",
      "         7.9710e-04, 2.1035e-04, 4.3460e-04, 6.9385e-04],\n",
      "        [2.4361e-04, 1.2367e-04, 1.4788e-04, 7.2584e-04, 1.8298e-03, 3.9327e-04,\n",
      "         1.6137e-04, 4.1123e-04, 4.3455e-04, 3.3641e-03, 9.6112e-04, 3.6442e-04,\n",
      "         7.5909e-04, 1.7010e-04, 6.7537e-04, 8.0469e-04],\n",
      "        [1.3677e-04, 7.3685e-05, 8.6295e-05, 4.4610e-04, 6.1512e-04, 3.2052e-04,\n",
      "         1.5825e-04, 2.6911e-04, 3.6762e-04, 1.4559e-03, 6.8192e-04, 4.6797e-04,\n",
      "         4.8408e-04, 1.7616e-04, 2.0885e-04, 8.7048e-04],\n",
      "        [1.3271e-04, 4.5610e-05, 7.6097e-05, 3.1959e-04, 4.0538e-04, 2.8071e-04,\n",
      "         1.1218e-04, 2.2085e-04, 3.3288e-04, 9.4676e-04, 6.6571e-04, 3.0075e-04,\n",
      "         3.4193e-04, 1.2392e-04, 1.3645e-04, 5.9521e-04],\n",
      "        [8.0341e-05, 2.9408e-05, 3.7407e-05, 1.4819e-04, 1.7693e-04, 1.0609e-04,\n",
      "         4.7071e-05, 1.3887e-04, 1.3436e-04, 3.8484e-04, 2.2984e-04, 1.1036e-04,\n",
      "         2.2250e-04, 5.0418e-05, 5.5743e-05, 1.5837e-04],\n",
      "        [2.2802e-03, 4.5405e-04, 5.1985e-04, 4.2099e-03, 7.5718e-03, 1.4682e-03,\n",
      "         6.8929e-04, 1.5917e-03, 1.8118e-03, 1.8335e-02, 3.3848e-03, 2.6028e-03,\n",
      "         7.2799e-03, 1.3668e-03, 1.5512e-03, 3.8406e-03],\n",
      "        [2.1112e-03, 7.4608e-04, 4.1850e-04, 3.2563e-03, 3.0996e-03, 2.8203e-03,\n",
      "         9.4064e-04, 3.0896e-03, 3.0163e-03, 7.3407e-03, 5.6539e-03, 2.3815e-03,\n",
      "         3.2284e-03, 9.0220e-04, 9.5379e-04, 3.8646e-03],\n",
      "        [9.4138e-04, 4.2742e-04, 4.3137e-04, 2.3541e-03, 3.6500e-03, 1.7016e-03,\n",
      "         9.7834e-04, 1.9043e-03, 1.8436e-03, 5.9769e-03, 5.3694e-03, 1.4040e-03,\n",
      "         3.9646e-03, 1.0957e-03, 1.6857e-03, 3.1528e-03],\n",
      "        [1.5202e-03, 4.2362e-04, 4.5262e-04, 2.7282e-03, 6.8981e-03, 1.6907e-03,\n",
      "         6.8305e-04, 1.4415e-03, 1.5054e-03, 1.4285e-02, 4.3053e-03, 1.7182e-03,\n",
      "         5.7557e-03, 1.2165e-03, 1.4159e-03, 4.0274e-03],\n",
      "        [2.0406e-04, 7.1034e-05, 9.7716e-05, 6.9129e-04, 8.3631e-04, 2.0526e-04,\n",
      "         1.2507e-04, 2.1913e-04, 3.5161e-04, 2.3793e-03, 4.9814e-04, 4.1783e-04,\n",
      "         5.1118e-04, 1.2147e-04, 3.2620e-04, 5.7791e-04],\n",
      "        [2.3603e-04, 8.5860e-05, 1.1814e-04, 9.1537e-04, 8.2148e-04, 3.8551e-04,\n",
      "         2.2001e-04, 3.2928e-04, 6.1259e-04, 2.8101e-03, 7.3590e-04, 9.2793e-04,\n",
      "         6.8313e-04, 2.5796e-04, 3.0825e-04, 1.1901e-03],\n",
      "        [8.1192e-04, 2.7277e-04, 1.7201e-04, 1.2612e-03, 1.9743e-03, 8.8290e-04,\n",
      "         3.9427e-04, 9.8410e-04, 9.5931e-04, 3.9913e-03, 1.4741e-03, 1.0010e-03,\n",
      "         1.7684e-03, 5.0856e-04, 4.8349e-04, 1.9417e-03]], device='cuda:0')}, 16: {'step': tensor(19100.), 'exp_avg': tensor([ 3.1110e-13, -8.2698e-14,  2.1355e-13,  4.7072e-13,  2.1499e-13,\n",
      "        -2.4554e-14,  3.6820e-13, -4.1333e-13,  4.5090e-14, -2.3110e-13,\n",
      "        -2.9198e-13, -5.3933e-15, -4.0590e-13, -2.6353e-13,  2.5816e-13,\n",
      "         4.2728e-14,  3.1996e-14, -3.5623e-13,  8.8527e-14,  4.4173e-14,\n",
      "        -3.4782e-13,  3.1010e-13, -2.6906e-13,  2.9236e-13,  5.0258e-13,\n",
      "         7.5169e-14, -3.2361e-13,  5.3370e-13,  5.5135e-14, -3.5854e-13,\n",
      "        -3.7962e-13, -2.3135e-13], device='cuda:0'), 'exp_avg_sq': tensor([0.0014, 0.0013, 0.0017, 0.0015, 0.0018, 0.0028, 0.0026, 0.0010, 0.0030,\n",
      "        0.0024, 0.0010, 0.0012, 0.0016, 0.0008, 0.0024, 0.0016, 0.0011, 0.0027,\n",
      "        0.0014, 0.0018, 0.0030, 0.0015, 0.0016, 0.0004, 0.0016, 0.0039, 0.0030,\n",
      "        0.0028, 0.0016, 0.0030, 0.0051, 0.0044], device='cuda:0')}, 17: {'step': tensor(19100.), 'exp_avg': tensor([[-1.3662e-02,  2.3217e-02,  8.5867e-04, -1.8163e-02, -6.9193e-03,\n",
      "          9.5992e-04,  1.4353e-03,  1.3078e-02,  6.8610e-04,  3.2046e-02,\n",
      "         -2.8484e-02,  2.0340e-02, -1.5381e-02, -1.4872e-02, -2.1831e-02,\n",
      "          1.1727e-03],\n",
      "        [ 7.0232e-03, -1.2726e-02,  3.4281e-04,  8.0963e-03,  2.7209e-03,\n",
      "         -9.0278e-05, -8.9781e-04, -6.9171e-03,  3.7998e-05, -1.5277e-02,\n",
      "          1.5591e-02, -1.4030e-02,  7.5896e-03,  8.3231e-03,  1.0999e-02,\n",
      "         -1.6491e-03],\n",
      "        [ 3.6094e-03, -6.6547e-03,  2.7490e-04,  3.9446e-03,  1.2227e-03,\n",
      "          3.5933e-05, -4.5366e-04, -3.5543e-03,  8.6935e-05, -7.2474e-03,\n",
      "          7.8939e-03, -7.5636e-03,  3.5860e-03,  4.1453e-03,  5.5300e-03,\n",
      "         -7.4079e-04],\n",
      "        [-6.4787e-03,  1.1557e-02,  8.6724e-04, -9.6983e-03, -3.9385e-03,\n",
      "          3.5588e-05,  4.4512e-04,  6.3833e-03,  3.6024e-04,  1.4944e-02,\n",
      "         -1.3519e-02,  7.8707e-03, -6.8534e-03, -6.3146e-03, -1.0823e-02,\n",
      "         -1.0170e-03],\n",
      "        [-1.9656e-02,  3.4399e-02, -1.0312e-04, -2.3851e-02, -8.4238e-03,\n",
      "          9.0965e-04,  2.3684e-03,  1.9022e-02,  3.7181e-04,  4.4098e-02,\n",
      "         -4.2223e-02,  3.5168e-02, -2.1627e-02, -2.2527e-02, -3.0846e-02,\n",
      "          3.6504e-03],\n",
      "        [ 2.7668e-03, -4.8780e-03,  1.5598e-04,  3.1221e-03,  1.0396e-03,\n",
      "         -1.3384e-04, -3.8801e-04, -2.6904e-03, -1.0664e-05, -6.2268e-03,\n",
      "          6.1327e-03, -5.5892e-03,  3.1337e-03,  3.3905e-03,  4.2930e-03,\n",
      "         -8.7174e-04],\n",
      "        [ 9.7730e-03, -1.7653e-02,  4.7691e-04,  1.1272e-02,  3.8030e-03,\n",
      "         -1.7108e-04, -1.2705e-03, -9.6224e-03,  2.8983e-05, -2.1481e-02,\n",
      "          2.1766e-02, -1.9548e-02,  1.0695e-02,  1.1678e-02,  1.5320e-02,\n",
      "         -2.4342e-03],\n",
      "        [ 6.2185e-03, -7.6345e-03, -3.4187e-04,  7.0810e-03,  2.5338e-03,\n",
      "         -2.3287e-03, -1.0687e-03, -5.0752e-03, -8.6732e-04, -1.6951e-02,\n",
      "          1.1172e-02, -8.4817e-03,  8.5962e-03,  7.6272e-03,  8.8237e-03,\n",
      "         -3.2443e-03],\n",
      "        [-9.0135e-03,  1.6217e-02,  2.5247e-04, -1.1762e-02, -4.3934e-03,\n",
      "          1.1674e-04,  9.5171e-04,  8.8983e-03,  1.9297e-04,  2.0485e-02,\n",
      "         -1.9697e-02,  1.5098e-02, -9.8857e-03, -1.0063e-02, -1.4596e-02,\n",
      "          8.2287e-04],\n",
      "        [ 6.6789e-03, -1.2556e-02, -4.7610e-04,  9.4403e-03,  3.6869e-03,\n",
      "          3.1233e-04, -5.2475e-04, -6.7630e-03, -1.2811e-04, -1.4783e-02,\n",
      "          1.4688e-02, -1.0141e-02,  6.8942e-03,  6.9060e-03,  1.1163e-02,\n",
      "          5.8303e-04],\n",
      "        [-5.2906e-03,  1.0403e-02, -3.8173e-04, -6.1273e-03, -2.0191e-03,\n",
      "         -4.5205e-04,  5.9471e-04,  5.4364e-03, -2.3229e-04,  1.0580e-02,\n",
      "         -1.2224e-02,  1.1326e-02, -5.1870e-03, -6.1051e-03, -8.4675e-03,\n",
      "          7.0576e-04],\n",
      "        [ 1.0202e-02, -1.7911e-02,  1.5614e-04,  1.2233e-02,  4.2943e-03,\n",
      "         -4.6180e-04, -1.2785e-03, -9.9074e-03, -1.7429e-04, -2.3058e-02,\n",
      "          2.2190e-02, -1.8796e-02,  1.1379e-02,  1.1931e-02,  1.6018e-02,\n",
      "         -2.2284e-03],\n",
      "        [ 7.1646e-03, -1.2086e-02, -4.6056e-04,  9.5171e-03,  3.6327e-03,\n",
      "         -5.6287e-04, -7.7030e-04, -6.8411e-03, -3.9335e-04, -1.6974e-02,\n",
      "          1.4940e-02, -1.0635e-02,  8.1631e-03,  7.8533e-03,  1.1435e-02,\n",
      "         -7.2326e-04],\n",
      "        [ 3.0683e-03, -4.6568e-03, -2.2426e-04,  3.9858e-03,  1.5443e-03,\n",
      "         -5.8726e-04, -4.0755e-04, -2.7945e-03, -2.8402e-04, -8.0091e-03,\n",
      "          6.2179e-03, -4.3194e-03,  3.9530e-03,  3.6157e-03,  4.7935e-03,\n",
      "         -8.5701e-04],\n",
      "        [-1.9934e-03,  3.9570e-03,  4.8897e-05, -2.7344e-03, -1.0485e-03,\n",
      "         -2.0917e-04,  1.6674e-04,  2.0853e-03, -2.8580e-05,  4.3238e-03,\n",
      "         -4.6481e-03,  3.5206e-03, -2.0479e-03, -2.1830e-03, -3.3837e-03,\n",
      "         -7.6782e-05],\n",
      "        [-3.6741e-03,  4.1283e-03,  2.7736e-04, -4.2675e-03, -1.5949e-03,\n",
      "          1.6393e-03,  6.9003e-04,  2.9194e-03,  6.3243e-04,  1.0837e-02,\n",
      "         -6.5794e-03,  4.6649e-03, -5.5488e-03, -4.7382e-03, -5.2192e-03,\n",
      "          2.3185e-03],\n",
      "        [-4.1954e-03,  5.7947e-03, -1.4755e-04, -4.2635e-03, -1.3380e-03,\n",
      "          1.2033e-03,  7.5347e-04,  3.5853e-03,  3.3582e-04,  1.0514e-02,\n",
      "         -8.0762e-03,  7.4905e-03, -5.4274e-03, -5.3379e-03, -5.9162e-03,\n",
      "          2.3893e-03],\n",
      "        [ 1.8556e-02, -3.3263e-02, -5.9563e-04,  2.4172e-02,  8.9802e-03,\n",
      "         -2.6070e-04, -1.8971e-03, -1.8235e-02, -4.1309e-04, -4.1534e-02,\n",
      "          3.9988e-02, -3.0563e-02,  1.9929e-02,  2.0291e-02,  2.9877e-02,\n",
      "         -1.2379e-03],\n",
      "        [ 4.5072e-03, -8.7164e-03, -3.9751e-04,  6.5973e-03,  2.6110e-03,\n",
      "          3.8263e-04, -2.8202e-04, -4.6265e-03, -4.5150e-05, -9.6743e-03,\n",
      "          9.9146e-03, -6.5443e-03,  4.4190e-03,  4.4329e-03,  7.6461e-03,\n",
      "          8.6759e-04],\n",
      "        [ 5.7789e-03, -1.0674e-02, -1.5459e-04,  7.5725e-03,  2.8150e-03,\n",
      "          1.2173e-04, -5.5485e-04, -5.7712e-03, -5.5183e-05, -1.2651e-02,\n",
      "          1.2664e-02, -9.7625e-03,  6.0379e-03,  6.2713e-03,  9.4019e-03,\n",
      "         -1.7645e-04],\n",
      "        [ 1.8336e-02, -3.0163e-02, -2.9242e-04,  2.2294e-02,  7.9805e-03,\n",
      "         -2.0352e-03, -2.3416e-03, -1.7192e-02, -8.4452e-04, -4.2985e-02,\n",
      "          3.7972e-02, -3.0528e-02,  2.1161e-02,  2.1096e-02,  2.8315e-02,\n",
      "         -4.2494e-03],\n",
      "        [-3.1356e-03,  4.8228e-03, -1.9631e-04, -3.2937e-03, -1.0805e-03,\n",
      "          6.2623e-04,  5.6336e-04,  2.8613e-03,  1.5549e-04,  7.9751e-03,\n",
      "         -6.7204e-03,  6.2557e-03, -4.1504e-03, -4.2274e-03, -4.6729e-03,\n",
      "          1.8359e-03],\n",
      "        [-5.2793e-03,  9.7631e-03,  2.2642e-04, -7.0745e-03, -2.6558e-03,\n",
      "         -1.3492e-04,  4.6921e-04,  5.2695e-03,  7.0196e-05,  1.1445e-02,\n",
      "         -1.1435e-02,  8.5161e-03, -5.3987e-03, -5.5454e-03, -8.6109e-03,\n",
      "         -1.2615e-04],\n",
      "        [-2.9103e-03,  4.6865e-03, -4.2170e-05, -3.3508e-03, -1.1654e-03,\n",
      "          4.0742e-04,  4.2526e-04,  2.7062e-03,  1.3294e-04,  7.0236e-03,\n",
      "         -6.1190e-03,  5.2319e-03, -3.5355e-03, -3.5666e-03, -4.4399e-03,\n",
      "          1.0557e-03],\n",
      "        [-1.7531e-02,  2.9840e-02,  2.4956e-04, -2.1635e-02, -7.7987e-03,\n",
      "          1.3061e-03,  2.1119e-03,  1.6739e-02,  5.9786e-04,  4.0346e-02,\n",
      "         -3.6987e-02,  2.9667e-02, -1.9750e-02, -1.9980e-02, -2.7449e-02,\n",
      "          3.2873e-03],\n",
      "        [ 1.0788e-02, -1.5694e-02, -5.1034e-04,  1.2967e-02,  4.6845e-03,\n",
      "         -2.4529e-03, -1.5128e-03, -9.4951e-03, -9.6867e-04, -2.6869e-02,\n",
      "          2.0661e-02, -1.5751e-02,  1.3316e-02,  1.2433e-02,  1.6063e-02,\n",
      "         -3.3868e-03],\n",
      "        [ 7.4181e-04, -9.0676e-05, -7.4040e-04,  1.8503e-03,  9.6602e-04,\n",
      "         -6.9044e-04,  2.5510e-05, -3.7244e-04, -4.5484e-04, -2.8071e-03,\n",
      "          1.9975e-04,  2.3023e-03,  1.1451e-03,  1.5933e-04,  1.1302e-03,\n",
      "          6.3833e-04],\n",
      "        [-2.0160e-02,  3.3182e-02,  8.1074e-04, -2.5416e-02, -9.3386e-03,\n",
      "          2.1395e-03,  2.3845e-03,  1.8916e-02,  1.0624e-03,  4.7286e-02,\n",
      "         -4.1312e-02,  3.1456e-02, -2.3004e-02, -2.2492e-02, -3.1392e-02,\n",
      "          3.4167e-03],\n",
      "        [-8.0898e-03,  1.3435e-02, -4.2722e-05, -9.5083e-03, -3.2920e-03,\n",
      "          8.2442e-04,  1.0543e-03,  7.5959e-03,  2.7760e-04,  1.8492e-02,\n",
      "         -1.6783e-02,  1.4170e-02, -9.1444e-03, -9.3538e-03, -1.2378e-02,\n",
      "          2.0142e-03],\n",
      "        [ 9.7887e-03, -1.6605e-02, -3.8101e-04,  1.2579e-02,  4.6867e-03,\n",
      "         -7.5175e-04, -1.1288e-03, -9.3620e-03, -4.4083e-04, -2.3006e-02,\n",
      "          2.0637e-02, -1.5595e-02,  1.1179e-02,  1.1014e-02,  1.5513e-02,\n",
      "         -1.4831e-03],\n",
      "        [ 7.3177e-03, -1.0487e-02, -4.0780e-04,  8.8854e-03,  3.2448e-03,\n",
      "         -1.7662e-03, -1.0372e-03, -6.4037e-03, -7.1391e-04, -1.8512e-02,\n",
      "          1.3946e-02, -1.0387e-02,  9.1770e-03,  8.4545e-03,  1.0900e-02,\n",
      "         -2.3568e-03],\n",
      "        [ 1.1318e-02, -1.8864e-02,  6.3013e-05,  1.3328e-02,  4.6316e-03,\n",
      "         -1.1123e-03, -1.4760e-03, -1.0661e-02, -3.8732e-04, -2.5959e-02,\n",
      "          2.3620e-02, -1.9920e-02,  1.2843e-02,  1.3144e-02,  1.7368e-02,\n",
      "         -2.8487e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.8776e-04, 3.4163e-04, 2.7396e-05, 2.0190e-04, 6.4290e-05, 2.8804e-04,\n",
      "         4.0538e-05, 5.8117e-05, 2.7722e-05, 7.7902e-04, 5.3036e-04, 6.1072e-04,\n",
      "         3.0231e-04, 2.0242e-04, 1.7734e-04, 3.9534e-04],\n",
      "        [1.7933e-04, 2.2802e-04, 8.6433e-05, 1.5114e-04, 8.8397e-05, 3.9050e-04,\n",
      "         4.6123e-05, 7.1668e-05, 7.4772e-05, 1.6791e-03, 6.1467e-04, 1.0432e-03,\n",
      "         7.8494e-04, 6.7337e-04, 1.9256e-04, 7.4869e-04],\n",
      "        [8.9363e-05, 2.9428e-04, 3.8683e-05, 1.3962e-04, 5.9989e-05, 1.5968e-04,\n",
      "         3.7277e-05, 3.4367e-05, 5.7786e-05, 3.2213e-04, 4.0224e-04, 5.4146e-04,\n",
      "         1.7263e-04, 1.8679e-04, 1.4969e-04, 3.6656e-04],\n",
      "        [1.4820e-04, 2.2135e-04, 7.7411e-05, 1.3800e-04, 8.3913e-05, 2.3835e-04,\n",
      "         5.9975e-05, 5.7338e-05, 4.3744e-05, 8.7867e-04, 7.2782e-04, 1.2452e-03,\n",
      "         4.8942e-04, 5.3513e-04, 1.6258e-04, 7.3683e-04],\n",
      "        [4.6018e-04, 7.6490e-04, 1.9712e-04, 4.5840e-04, 2.1857e-04, 8.7505e-04,\n",
      "         1.0484e-04, 2.0326e-04, 1.9718e-04, 4.3777e-03, 1.7653e-03, 2.4820e-03,\n",
      "         1.7998e-03, 1.5475e-03, 6.5938e-04, 1.7293e-03],\n",
      "        [1.4351e-04, 2.9567e-04, 4.0771e-05, 1.6937e-04, 9.0790e-05, 2.1755e-04,\n",
      "         5.6711e-05, 5.5832e-05, 5.7141e-05, 7.1578e-04, 7.0486e-04, 8.3922e-04,\n",
      "         3.1114e-04, 3.2153e-04, 1.5212e-04, 6.8416e-04],\n",
      "        [3.6563e-04, 4.7944e-04, 1.2954e-04, 2.9683e-04, 1.5260e-04, 6.9901e-04,\n",
      "         7.9727e-05, 1.2966e-04, 1.1960e-04, 2.7201e-03, 1.1098e-03, 1.6908e-03,\n",
      "         1.2047e-03, 1.0786e-03, 3.8166e-04, 1.2877e-03],\n",
      "        [2.5421e-04, 4.9093e-04, 9.1742e-05, 2.9166e-04, 1.2964e-04, 5.4783e-04,\n",
      "         7.6657e-05, 1.3021e-04, 7.5361e-05, 1.2993e-03, 1.6666e-03, 2.8197e-03,\n",
      "         6.6584e-04, 4.2626e-04, 3.1659e-04, 6.1316e-04],\n",
      "        [1.1686e-04, 2.8008e-04, 2.9760e-05, 1.2266e-04, 8.2925e-05, 2.1211e-04,\n",
      "         3.3185e-05, 5.5155e-05, 3.7950e-05, 4.1593e-04, 4.6799e-04, 6.2121e-04,\n",
      "         2.3305e-04, 2.1779e-04, 1.7683e-04, 3.6339e-04],\n",
      "        [1.4131e-04, 2.8940e-04, 4.3839e-05, 1.7820e-04, 7.9548e-05, 1.7056e-04,\n",
      "         5.9693e-05, 4.6734e-05, 2.3992e-05, 3.9597e-04, 4.6184e-04, 8.8600e-04,\n",
      "         2.1490e-04, 2.0510e-04, 1.6287e-04, 4.2700e-04],\n",
      "        [2.1938e-04, 3.3336e-04, 6.8977e-05, 2.6281e-04, 1.2739e-04, 4.3066e-04,\n",
      "         6.3571e-05, 1.0161e-04, 8.5485e-05, 2.4380e-03, 9.8808e-04, 1.3943e-03,\n",
      "         9.5300e-04, 5.5317e-04, 3.0401e-04, 9.1851e-04],\n",
      "        [8.9489e-05, 1.9973e-04, 4.3507e-05, 1.2476e-04, 5.3324e-05, 1.6436e-04,\n",
      "         2.8435e-05, 4.9060e-05, 4.9930e-05, 6.1925e-04, 3.1589e-04, 5.2739e-04,\n",
      "         3.2611e-04, 3.5136e-04, 1.5025e-04, 4.1071e-04],\n",
      "        [1.1598e-04, 1.9214e-04, 2.0184e-05, 1.2997e-04, 5.3012e-05, 1.3950e-04,\n",
      "         3.1008e-05, 4.3500e-05, 3.2818e-05, 4.5599e-04, 5.4669e-04, 5.0102e-04,\n",
      "         1.7822e-04, 1.3591e-04, 1.0845e-04, 3.7065e-04],\n",
      "        [2.9453e-05, 1.1986e-04, 2.3472e-05, 6.3884e-05, 3.4742e-05, 6.9700e-05,\n",
      "         2.2339e-05, 1.2291e-05, 2.1190e-05, 1.6777e-04, 1.1866e-04, 3.0885e-04,\n",
      "         9.8056e-05, 1.1285e-04, 4.9410e-05, 1.9540e-04],\n",
      "        [5.3558e-05, 2.4400e-04, 1.6840e-05, 1.0276e-04, 4.7564e-05, 1.1699e-04,\n",
      "         3.5640e-05, 3.1027e-05, 2.5763e-05, 1.2592e-04, 1.8931e-04, 3.5401e-04,\n",
      "         5.9791e-05, 5.3637e-05, 9.5951e-05, 2.6210e-04],\n",
      "        [1.9139e-04, 3.2904e-04, 7.2462e-05, 2.3702e-04, 1.4304e-04, 3.8793e-04,\n",
      "         6.4602e-05, 8.2674e-05, 6.0222e-05, 1.8218e-03, 8.8495e-04, 1.9017e-03,\n",
      "         7.1888e-04, 3.6867e-04, 2.0205e-04, 7.2249e-04],\n",
      "        [1.9041e-04, 3.0732e-04, 7.4220e-05, 1.6811e-04, 7.6840e-05, 2.9004e-04,\n",
      "         5.9837e-05, 5.9738e-05, 5.6323e-05, 6.8650e-04, 1.0031e-03, 1.7749e-03,\n",
      "         3.2719e-04, 3.1507e-04, 1.6458e-04, 4.6738e-04],\n",
      "        [1.8193e-04, 5.3606e-04, 5.0426e-05, 3.2936e-04, 1.2705e-04, 2.1819e-04,\n",
      "         6.7277e-05, 1.0231e-04, 6.6765e-05, 8.6905e-04, 8.8501e-04, 1.2061e-03,\n",
      "         3.7568e-04, 3.2670e-04, 4.1776e-04, 5.2205e-04],\n",
      "        [1.0763e-04, 2.1914e-04, 2.4853e-05, 1.3003e-04, 4.9588e-05, 1.0753e-04,\n",
      "         5.0989e-05, 4.1152e-05, 1.7153e-05, 3.7358e-04, 6.8232e-04, 9.8927e-04,\n",
      "         2.0847e-04, 1.2429e-04, 2.0588e-04, 3.2345e-04],\n",
      "        [9.0373e-05, 2.1056e-04, 2.2433e-05, 1.0926e-04, 5.6475e-05, 1.5042e-04,\n",
      "         2.9858e-05, 3.3828e-05, 2.5815e-05, 2.4380e-04, 3.0724e-04, 4.2503e-04,\n",
      "         1.0912e-04, 1.0365e-04, 9.2768e-05, 2.4594e-04],\n",
      "        [2.9612e-04, 7.7527e-04, 1.4453e-04, 3.6885e-04, 1.8018e-04, 5.2331e-04,\n",
      "         8.8435e-05, 1.7850e-04, 1.3910e-04, 2.6540e-03, 1.7317e-03, 2.2326e-03,\n",
      "         9.6080e-04, 1.0232e-03, 4.1974e-04, 1.5110e-03],\n",
      "        [1.5325e-04, 1.9029e-04, 7.1884e-05, 1.0307e-04, 8.2922e-05, 2.3978e-04,\n",
      "         4.8877e-05, 2.8094e-05, 3.3564e-05, 5.0090e-04, 4.4412e-04, 9.7505e-04,\n",
      "         3.3897e-04, 3.8504e-04, 1.0419e-04, 4.6220e-04],\n",
      "        [1.0470e-04, 2.6580e-04, 3.4761e-05, 1.4104e-04, 6.1504e-05, 1.8788e-04,\n",
      "         4.7479e-05, 3.0695e-05, 2.7974e-05, 4.1611e-04, 3.3404e-04, 5.9490e-04,\n",
      "         2.0521e-04, 1.7217e-04, 1.2992e-04, 3.6787e-04],\n",
      "        [2.6269e-05, 3.4368e-05, 1.6745e-05, 2.1804e-05, 1.2554e-05, 2.7134e-05,\n",
      "         8.6301e-06, 8.6731e-06, 9.0615e-06, 1.1413e-04, 6.2225e-05, 1.7196e-04,\n",
      "         8.0238e-05, 8.0576e-05, 2.3150e-05, 8.6432e-05],\n",
      "        [1.6293e-04, 3.6428e-04, 8.3293e-05, 1.8540e-04, 8.6586e-05, 2.1484e-04,\n",
      "         4.0767e-05, 9.0899e-05, 7.5370e-05, 1.1169e-03, 7.0509e-04, 9.2753e-04,\n",
      "         4.9752e-04, 5.6602e-04, 2.7116e-04, 5.5544e-04],\n",
      "        [9.6401e-04, 1.2154e-03, 1.7843e-04, 5.8758e-04, 2.3390e-04, 1.4947e-03,\n",
      "         1.2807e-04, 2.6814e-04, 2.3197e-04, 2.1409e-03, 4.8552e-03, 4.3125e-03,\n",
      "         8.8442e-04, 8.3360e-04, 4.4083e-04, 1.4983e-03],\n",
      "        [4.6496e-04, 5.7408e-04, 1.2092e-04, 3.1738e-04, 1.6726e-04, 8.9918e-04,\n",
      "         8.3641e-05, 1.1332e-04, 1.2118e-04, 1.2744e-03, 1.4800e-03, 1.7116e-03,\n",
      "         7.4945e-04, 7.4579e-04, 2.2710e-04, 9.5578e-04],\n",
      "        [2.9343e-04, 9.0423e-04, 1.0380e-04, 3.3701e-04, 1.5253e-04, 6.6542e-04,\n",
      "         5.6503e-05, 1.7841e-04, 1.1009e-04, 1.2261e-03, 1.7296e-03, 1.6225e-03,\n",
      "         4.8207e-04, 4.7657e-04, 3.7750e-04, 6.9255e-04],\n",
      "        [8.4643e-05, 3.4129e-04, 6.6887e-05, 1.5025e-04, 7.4811e-05, 1.4934e-04,\n",
      "         4.4236e-05, 3.7843e-05, 6.5581e-05, 3.7688e-04, 3.2681e-04, 8.0505e-04,\n",
      "         2.3594e-04, 2.9722e-04, 1.4880e-04, 4.0164e-04],\n",
      "        [9.2205e-05, 2.9011e-04, 2.6763e-05, 1.3081e-04, 7.5506e-05, 1.8416e-04,\n",
      "         3.6674e-05, 4.6584e-05, 3.1623e-05, 6.1053e-04, 2.7721e-04, 5.4773e-04,\n",
      "         2.0279e-04, 2.0267e-04, 1.5903e-04, 4.3172e-04],\n",
      "        [4.2904e-04, 1.0054e-03, 1.1505e-04, 5.9860e-04, 2.6650e-04, 9.5887e-04,\n",
      "         1.7713e-04, 1.6741e-04, 1.5810e-04, 3.4646e-03, 1.9849e-03, 3.6965e-03,\n",
      "         1.2150e-03, 5.7282e-04, 5.4215e-04, 1.5484e-03],\n",
      "        [3.6781e-04, 7.2317e-04, 1.0722e-04, 3.0692e-04, 1.7659e-04, 5.2108e-04,\n",
      "         8.4297e-05, 1.1472e-04, 1.1753e-04, 1.2798e-03, 1.7660e-03, 1.5636e-03,\n",
      "         5.6692e-04, 5.4714e-04, 3.1383e-04, 1.0512e-03]], device='cuda:0')}, 18: {'step': tensor(19100.), 'exp_avg': tensor([[-3.5802e-02,  7.1263e-03, -1.0711e-02,  ...,  9.9097e-03,\n",
      "         -2.4984e-02,  5.1385e-02],\n",
      "        [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ..., -5.6052e-45,\n",
      "         -5.6052e-45,  5.6052e-45],\n",
      "        [-1.8020e-02,  3.0165e-03, -5.2042e-03,  ...,  4.4974e-03,\n",
      "         -1.1683e-02,  2.5739e-02],\n",
      "        ...,\n",
      "        [-5.6052e-45, -5.6052e-45,  5.6052e-45,  ..., -5.6052e-45,\n",
      "         -5.6052e-45, -5.6052e-45],\n",
      "        [ 5.6052e-45,  5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
      "          5.6052e-45, -5.6052e-45],\n",
      "        [ 5.6052e-45, -5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
      "         -5.6052e-45,  5.6052e-45]], device='cuda:0'), 'exp_avg_sq': tensor([[5.2385e-04, 8.2068e-04, 2.5347e-04,  ..., 1.0662e-03, 3.0864e-03,\n",
      "         1.9104e-03],\n",
      "        [6.7443e-05, 6.4049e-05, 1.3743e-05,  ..., 9.2808e-05, 1.5758e-05,\n",
      "         1.0462e-05],\n",
      "        [3.3617e-04, 1.3863e-03, 2.7725e-04,  ..., 3.8195e-03, 2.9312e-03,\n",
      "         7.8766e-04],\n",
      "        ...,\n",
      "        [3.1066e-05, 2.0298e-05, 3.7364e-05,  ..., 5.5422e-05, 1.9896e-04,\n",
      "         1.0408e-05],\n",
      "        [2.2637e-10, 6.5120e-10, 1.8838e-09,  ..., 5.5028e-09, 2.7089e-09,\n",
      "         4.1358e-09],\n",
      "        [2.7697e-08, 1.0261e-07, 1.2478e-07,  ..., 4.4224e-07, 2.7934e-08,\n",
      "         1.7955e-08]], device='cuda:0')}, 19: {'step': tensor(19100.), 'exp_avg': tensor([-5.6052e-45,  5.6052e-45, -5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45, -5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  1.4175e-14,\n",
      "         5.6052e-45,  5.6052e-45,  3.1808e-26,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45, -5.6052e-45,  5.6052e-45,  5.6052e-45,  2.9839e-28,\n",
      "        -5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45, -2.7754e-14,\n",
      "         5.6052e-45, -5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45, -4.6982e-27,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45, -5.6052e-45,  5.6052e-45,  5.6052e-45,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45,  5.6052e-45], device='cuda:0'), 'exp_avg_sq': tensor([6.6633e-08, 6.9849e-07, 6.8465e-06, 2.9861e-11, 7.6876e-11, 2.8497e-11,\n",
      "        3.4190e-06, 8.0914e-10, 1.8094e-11, 1.2678e-10, 2.7783e-12, 3.0140e-10,\n",
      "        5.3334e-09, 1.2561e-07, 4.2487e-11, 1.2169e-10, 3.7338e-10, 6.2552e-08,\n",
      "        6.1863e-11, 6.3155e-08, 7.3883e-11, 2.1632e-10, 5.3627e-06, 1.3469e-10,\n",
      "        5.6871e-10, 1.9203e-09, 2.5254e-09, 3.0720e-11, 3.8539e-10, 4.9930e-06,\n",
      "        8.0879e-07, 1.3443e-10, 2.4049e-10, 2.9186e-10, 6.8745e-07, 6.3993e-13,\n",
      "        8.2266e-07, 8.4351e-09, 2.9125e-11, 3.5078e-14, 8.0109e-10, 1.0994e-09,\n",
      "        2.0731e-12, 3.2717e-08, 6.9938e-11, 2.1672e-10, 8.6768e-08, 2.1246e-07,\n",
      "        1.4035e-11, 2.4423e-10, 8.5842e-12, 6.3906e-10, 1.4496e-06, 2.4229e-13,\n",
      "        5.2137e-11, 6.7079e-12, 8.2701e-08, 6.4342e-08, 4.5890e-08, 4.5083e-11,\n",
      "        3.6697e-09, 6.3975e-07, 1.0093e-11, 5.1962e-10], device='cuda:0')}}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}]\n"
     ]
    }
   ],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{model_name}_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.3467746675014496,\n",
       "  0.2670913338661194,\n",
       "  1.1573362350463867,\n",
       "  0.44076403975486755,\n",
       "  0.4068085849285126,\n",
       "  0.11405106633901596,\n",
       "  0.285125732421875,\n",
       "  0.2771705687046051,\n",
       "  0.7040592432022095,\n",
       "  0.3596470057964325,\n",
       "  0.7115749716758728,\n",
       "  0.9310901761054993,\n",
       "  0.2300594002008438,\n",
       "  1.076621174812317,\n",
       "  0.1976536214351654,\n",
       "  0.17268610000610352,\n",
       "  0.4044511020183563,\n",
       "  0.4731912314891815,\n",
       "  0.1544363796710968,\n",
       "  0.30338355898857117,\n",
       "  0.5527524352073669,\n",
       "  1.0228042602539062,\n",
       "  0.7744596004486084,\n",
       "  0.526511549949646,\n",
       "  0.49162915349006653,\n",
       "  0.5259588360786438,\n",
       "  0.529477059841156,\n",
       "  1.4394381046295166,\n",
       "  0.2684149146080017,\n",
       "  0.8374361395835876,\n",
       "  0.5147445201873779,\n",
       "  0.41023576259613037,\n",
       "  0.9108292460441589,\n",
       "  0.9882000088691711,\n",
       "  0.43487638235092163,\n",
       "  0.6840386986732483,\n",
       "  0.4992109537124634,\n",
       "  0.3937351107597351,\n",
       "  0.8564106225967407,\n",
       "  0.38367322087287903,\n",
       "  0.29733046889305115,\n",
       "  0.5036300420761108,\n",
       "  0.6489309072494507,\n",
       "  0.2389589250087738,\n",
       "  0.14308100938796997,\n",
       "  0.14352351427078247,\n",
       "  0.9294792413711548,\n",
       "  0.27203497290611267,\n",
       "  0.28901439905166626,\n",
       "  0.7642820477485657,\n",
       "  0.4961058497428894,\n",
       "  0.09157980978488922,\n",
       "  0.37292349338531494,\n",
       "  0.517790675163269,\n",
       "  0.1787271946668625,\n",
       "  0.9622074365615845,\n",
       "  0.620567798614502,\n",
       "  0.4244643449783325,\n",
       "  0.8322761654853821,\n",
       "  1.189214825630188,\n",
       "  0.18976755440235138,\n",
       "  0.29480859637260437,\n",
       "  0.43996572494506836,\n",
       "  0.5805277824401855,\n",
       "  0.9232132434844971,\n",
       "  0.6861429810523987,\n",
       "  0.1905493587255478,\n",
       "  1.543712854385376,\n",
       "  0.6319996118545532,\n",
       "  0.15672631561756134,\n",
       "  0.9735260605812073,\n",
       "  0.6552717089653015,\n",
       "  0.18631504476070404,\n",
       "  0.5024782419204712,\n",
       "  0.55250084400177,\n",
       "  0.28156334161758423,\n",
       "  0.2517218291759491,\n",
       "  0.5643267631530762,\n",
       "  0.7511431574821472,\n",
       "  0.7772307395935059,\n",
       "  0.26911842823028564,\n",
       "  0.3093051612377167,\n",
       "  0.1454755961894989,\n",
       "  0.8155606985092163,\n",
       "  0.21717469394207,\n",
       "  0.08549875020980835,\n",
       "  0.11360742896795273,\n",
       "  1.1158671379089355,\n",
       "  0.36930763721466064,\n",
       "  0.07485093176364899,\n",
       "  0.6680105328559875,\n",
       "  0.6290757656097412,\n",
       "  0.7159469127655029,\n",
       "  0.7042222023010254,\n",
       "  0.4196750223636627,\n",
       "  0.3635658323764801,\n",
       "  0.1531887948513031,\n",
       "  0.5989967584609985,\n",
       "  0.8877483606338501,\n",
       "  0.8059166073799133,\n",
       "  0.613347589969635,\n",
       "  0.676292359828949,\n",
       "  0.605251669883728,\n",
       "  0.5714694261550903,\n",
       "  0.47263479232788086,\n",
       "  0.5666651725769043,\n",
       "  0.290215402841568,\n",
       "  0.8272918462753296,\n",
       "  0.18751348555088043,\n",
       "  0.6920638084411621,\n",
       "  0.4291818141937256,\n",
       "  0.38659512996673584,\n",
       "  1.0721064805984497,\n",
       "  0.24351975321769714,\n",
       "  0.4377768933773041,\n",
       "  0.4126952588558197,\n",
       "  0.36414599418640137,\n",
       "  0.48716026544570923,\n",
       "  0.5091404318809509,\n",
       "  1.1707016229629517,\n",
       "  0.30253687500953674,\n",
       "  0.6198551654815674,\n",
       "  0.2640652358531952,\n",
       "  0.521278977394104,\n",
       "  0.490621417760849,\n",
       "  0.6286904215812683,\n",
       "  0.3729431629180908,\n",
       "  0.6333968639373779,\n",
       "  0.21290789544582367,\n",
       "  0.4874430000782013,\n",
       "  0.5294694304466248,\n",
       "  0.7789530158042908,\n",
       "  0.30542507767677307,\n",
       "  0.20081400871276855,\n",
       "  0.6861353516578674,\n",
       "  0.8408557772636414,\n",
       "  1.0908124446868896,\n",
       "  0.6227117776870728,\n",
       "  1.3338881731033325,\n",
       "  0.8175787925720215,\n",
       "  1.0563160181045532,\n",
       "  0.0832531675696373,\n",
       "  0.32237231731414795,\n",
       "  0.21580955386161804,\n",
       "  0.39909064769744873,\n",
       "  0.8097841143608093,\n",
       "  0.30168387293815613,\n",
       "  0.4740333557128906,\n",
       "  0.29767537117004395,\n",
       "  0.6828597187995911,\n",
       "  1.0842331647872925,\n",
       "  0.21382063627243042,\n",
       "  0.1907975822687149,\n",
       "  0.31827613711357117,\n",
       "  0.7170381546020508,\n",
       "  0.5608534216880798,\n",
       "  1.3313947916030884,\n",
       "  0.6368679404258728,\n",
       "  0.24951402842998505,\n",
       "  0.20530538260936737,\n",
       "  0.3388175964355469,\n",
       "  0.39195516705513,\n",
       "  0.6981106400489807,\n",
       "  0.6721596121788025,\n",
       "  0.551813542842865,\n",
       "  0.9259722828865051,\n",
       "  0.5169597864151001,\n",
       "  1.017811894416809,\n",
       "  0.22898223996162415,\n",
       "  0.6792811155319214,\n",
       "  0.7014994025230408,\n",
       "  0.40638527274131775,\n",
       "  0.9323453307151794,\n",
       "  0.25815632939338684,\n",
       "  0.6086989641189575,\n",
       "  0.9855819344520569,\n",
       "  0.8059670925140381,\n",
       "  0.5941830277442932,\n",
       "  0.23939526081085205,\n",
       "  1.6076009273529053,\n",
       "  0.9825881719589233],\n",
       " [0.790635883808136,\n",
       "  0.32113000750541687,\n",
       "  0.16012920439243317,\n",
       "  0.5376626253128052,\n",
       "  0.11560629308223724,\n",
       "  0.13433578610420227,\n",
       "  0.2597699463367462,\n",
       "  0.22735732793807983,\n",
       "  0.5261880159378052,\n",
       "  0.76579749584198,\n",
       "  0.30192092061042786,\n",
       "  0.78385990858078,\n",
       "  0.1802329421043396,\n",
       "  0.18345801532268524,\n",
       "  0.2810894846916199,\n",
       "  0.2522435486316681,\n",
       "  0.1056278869509697,\n",
       "  0.23897404968738556,\n",
       "  0.2493964582681656,\n",
       "  0.1466514766216278,\n",
       "  0.37235355377197266,\n",
       "  0.07719086110591888,\n",
       "  0.5879510641098022,\n",
       "  0.33100804686546326,\n",
       "  0.1356307566165924,\n",
       "  0.29636722803115845,\n",
       "  0.6215164661407471,\n",
       "  0.34107840061187744,\n",
       "  0.24117052555084229,\n",
       "  0.5421573519706726,\n",
       "  0.09461488574743271,\n",
       "  0.12135791033506393,\n",
       "  0.3459792137145996,\n",
       "  0.377106249332428,\n",
       "  0.38685619831085205,\n",
       "  0.2809063792228699,\n",
       "  0.547125518321991,\n",
       "  0.4433823525905609,\n",
       "  0.2824093699455261,\n",
       "  0.2697679102420807,\n",
       "  0.20141971111297607,\n",
       "  0.2069358080625534,\n",
       "  0.2797800898551941,\n",
       "  0.331001341342926,\n",
       "  0.38222190737724304,\n",
       "  0.18846465647220612,\n",
       "  0.4273380637168884,\n",
       "  0.2890203595161438,\n",
       "  0.3486132323741913,\n",
       "  0.021813027560710907,\n",
       "  0.41258934140205383,\n",
       "  0.3057594895362854,\n",
       "  0.5302947163581848,\n",
       "  0.40509504079818726,\n",
       "  0.8848779797554016,\n",
       "  0.3690764009952545,\n",
       "  0.26092633605003357,\n",
       "  0.5248659253120422,\n",
       "  0.14071807265281677,\n",
       "  0.23533980548381805,\n",
       "  0.30102357268333435,\n",
       "  0.15133894979953766,\n",
       "  0.06716728955507278,\n",
       "  0.14994581043720245,\n",
       "  0.38807573914527893,\n",
       "  0.5520119071006775,\n",
       "  0.5497124791145325,\n",
       "  1.3079906702041626,\n",
       "  0.2315816432237625,\n",
       "  0.28503385186195374,\n",
       "  0.1301908940076828,\n",
       "  0.4093571901321411,\n",
       "  0.27446144819259644,\n",
       "  0.12265798449516296,\n",
       "  0.1127457469701767,\n",
       "  0.4467422366142273,\n",
       "  0.32452237606048584,\n",
       "  0.15656571090221405,\n",
       "  0.2182874232530594,\n",
       "  0.13134783506393433,\n",
       "  0.35161301493644714,\n",
       "  0.3837653398513794,\n",
       "  0.2603701055049896,\n",
       "  0.41363653540611267,\n",
       "  0.30933699011802673,\n",
       "  0.25851818919181824,\n",
       "  0.6819688081741333,\n",
       "  0.9360420107841492,\n",
       "  0.4719427824020386,\n",
       "  0.22901102900505066,\n",
       "  0.14471647143363953,\n",
       "  0.2251046895980835,\n",
       "  0.3523401916027069,\n",
       "  0.3144277334213257,\n",
       "  0.3243633210659027,\n",
       "  0.15585732460021973,\n",
       "  0.9463226199150085,\n",
       "  0.08237819373607635,\n",
       "  0.3916168212890625,\n",
       "  0.5862354040145874,\n",
       "  0.36406078934669495,\n",
       "  0.08382730185985565,\n",
       "  0.24509930610656738,\n",
       "  0.22975796461105347,\n",
       "  0.6099345088005066,\n",
       "  0.15820123255252838,\n",
       "  0.5347318053245544,\n",
       "  0.16375386714935303,\n",
       "  0.18347667157649994,\n",
       "  0.3443332612514496,\n",
       "  0.08654000610113144,\n",
       "  0.3997061252593994,\n",
       "  0.30867716670036316,\n",
       "  0.5206068158149719,\n",
       "  0.06666882336139679,\n",
       "  0.2957168519496918,\n",
       "  0.6811664700508118,\n",
       "  0.22972998023033142,\n",
       "  0.20126543939113617,\n",
       "  0.42536619305610657,\n",
       "  0.04283048212528229,\n",
       "  0.4767581522464752,\n",
       "  0.1536860466003418,\n",
       "  0.3288460075855255,\n",
       "  0.3154199719429016,\n",
       "  0.13352161645889282,\n",
       "  0.14794999361038208,\n",
       "  0.178436741232872,\n",
       "  0.5482538342475891,\n",
       "  0.7075969576835632,\n",
       "  0.4475868344306946,\n",
       "  0.39757153391838074,\n",
       "  0.3464938700199127,\n",
       "  0.5203074812889099,\n",
       "  0.3005388677120209,\n",
       "  0.6869075894355774,\n",
       "  0.13589848577976227,\n",
       "  0.07506344467401505,\n",
       "  0.16389046609401703,\n",
       "  0.26038986444473267,\n",
       "  0.23408211767673492,\n",
       "  0.2324606031179428,\n",
       "  0.25711047649383545,\n",
       "  0.3490751385688782,\n",
       "  0.3858184814453125,\n",
       "  0.570793867111206,\n",
       "  0.4049322009086609,\n",
       "  0.10939760506153107,\n",
       "  0.526715099811554,\n",
       "  0.7970967292785645,\n",
       "  0.3554486632347107,\n",
       "  0.2021537572145462,\n",
       "  0.49875548481941223,\n",
       "  0.4594390094280243,\n",
       "  0.7876307368278503,\n",
       "  0.9330837726593018,\n",
       "  0.30139392614364624,\n",
       "  0.9043546319007874,\n",
       "  0.11648579686880112,\n",
       "  0.4967620074748993,\n",
       "  0.33559560775756836,\n",
       "  1.3489513397216797,\n",
       "  0.8297932744026184,\n",
       "  0.1704479455947876,\n",
       "  0.5083731412887573,\n",
       "  0.6362375020980835,\n",
       "  0.19613267481327057,\n",
       "  0.223294198513031,\n",
       "  0.5630258321762085,\n",
       "  0.30818286538124084,\n",
       "  1.278910517692566,\n",
       "  0.29686400294303894,\n",
       "  0.16681347787380219,\n",
       "  0.1488928496837616,\n",
       "  0.4070431888103485,\n",
       "  1.481390118598938,\n",
       "  0.3973411023616791,\n",
       "  0.19740290939807892,\n",
       "  0.32879000902175903,\n",
       "  0.22120511531829834,\n",
       "  0.10362310707569122,\n",
       "  0.360976904630661,\n",
       "  0.1714295893907547,\n",
       "  0.061172083020210266,\n",
       "  0.10117779672145844,\n",
       "  0.3623542785644531,\n",
       "  0.4242086708545685,\n",
       "  0.34205514192581177,\n",
       "  0.34340593218803406,\n",
       "  0.22439004480838776,\n",
       "  0.492028146982193,\n",
       "  0.3884720504283905,\n",
       "  0.5856660604476929,\n",
       "  0.13644558191299438,\n",
       "  0.22497360408306122,\n",
       "  0.8387644290924072,\n",
       "  0.4564201533794403,\n",
       "  0.602543830871582,\n",
       "  0.7788416743278503,\n",
       "  0.19908064603805542,\n",
       "  0.13720357418060303],\n",
       " [0.3125535845756531,\n",
       "  0.21335336565971375,\n",
       "  0.2685778737068176,\n",
       "  0.0,\n",
       "  0.05156233534216881,\n",
       "  0.012820998206734657,\n",
       "  0.15637311339378357,\n",
       "  0.1445394903421402,\n",
       "  0.17593687772750854,\n",
       "  0.39245161414146423,\n",
       "  0.006682378239929676,\n",
       "  0.2555232048034668,\n",
       "  0.009023083373904228,\n",
       "  0.033740848302841187,\n",
       "  0.025845786556601524,\n",
       "  0.26124224066734314,\n",
       "  0.1384357213973999,\n",
       "  0.2932237386703491,\n",
       "  0.0455779954791069,\n",
       "  0.30721649527549744,\n",
       "  0.016241583973169327,\n",
       "  0.016828393563628197,\n",
       "  0.043790996074676514,\n",
       "  0.20502465963363647,\n",
       "  0.03950504586100578,\n",
       "  0.0318138562142849,\n",
       "  0.041594430804252625,\n",
       "  0.002374187344685197,\n",
       "  0.005578706506639719,\n",
       "  0.0285536777228117,\n",
       "  0.031099464744329453,\n",
       "  0.01075312215834856,\n",
       "  0.06932415813207626,\n",
       "  0.296404629945755,\n",
       "  0.0,\n",
       "  0.0029792110435664654,\n",
       "  0.32187989354133606,\n",
       "  0.3422260284423828,\n",
       "  0.17733979225158691,\n",
       "  0.1728442758321762,\n",
       "  0.1387651115655899,\n",
       "  0.05478241294622421,\n",
       "  0.3574885427951813,\n",
       "  0.009198061190545559,\n",
       "  0.24263828992843628,\n",
       "  0.10001431405544281,\n",
       "  0.0,\n",
       "  0.025434397161006927,\n",
       "  0.11209503561258316,\n",
       "  0.04391719028353691,\n",
       "  0.12921999394893646,\n",
       "  0.19314143061637878,\n",
       "  0.05816754326224327,\n",
       "  0.11235855519771576,\n",
       "  0.029081014916300774,\n",
       "  0.11069698631763458,\n",
       "  0.0,\n",
       "  0.267610102891922,\n",
       "  0.14696772396564484,\n",
       "  0.004476000089198351,\n",
       "  0.00736371660605073,\n",
       "  0.013213329948484898,\n",
       "  0.11773736774921417,\n",
       "  0.059851087629795074,\n",
       "  0.07110477238893509,\n",
       "  0.1739114224910736,\n",
       "  0.07482784241437912,\n",
       "  0.0039755310863256454,\n",
       "  0.03945395350456238,\n",
       "  0.14967387914657593,\n",
       "  0.09829436242580414,\n",
       "  0.03987771272659302,\n",
       "  6.959668826311827e-05,\n",
       "  0.03638715296983719,\n",
       "  0.020284578204154968,\n",
       "  0.15968365967273712,\n",
       "  0.04991556331515312,\n",
       "  0.12539561092853546,\n",
       "  0.18377996981143951,\n",
       "  0.01030923705548048,\n",
       "  0.18992988765239716,\n",
       "  0.246874138712883,\n",
       "  0.021411139518022537,\n",
       "  0.12319124490022659,\n",
       "  0.061624523252248764,\n",
       "  0.2895929217338562,\n",
       "  0.03913779929280281,\n",
       "  0.05478034168481827,\n",
       "  0.07304689288139343,\n",
       "  0.017758991569280624,\n",
       "  0.3671543002128601,\n",
       "  0.3667772114276886,\n",
       "  0.002486723242327571,\n",
       "  0.0913095474243164,\n",
       "  0.26496583223342896,\n",
       "  0.2527509033679962,\n",
       "  0.04176684096455574,\n",
       "  0.060764897614717484,\n",
       "  0.00023785016674082726,\n",
       "  0.08511398732662201,\n",
       "  0.02778106927871704,\n",
       "  0.126955047249794,\n",
       "  0.4126160144805908,\n",
       "  0.046744205057621,\n",
       "  0.14060606062412262,\n",
       "  0.19661198556423187,\n",
       "  0.11626294255256653,\n",
       "  0.08204994350671768,\n",
       "  0.3372431695461273,\n",
       "  0.39150556921958923,\n",
       "  0.06810804456472397,\n",
       "  0.03404157981276512,\n",
       "  0.0007142064860090613,\n",
       "  0.13774290680885315,\n",
       "  0.07198387384414673,\n",
       "  0.16562016308307648,\n",
       "  0.009900928474962711,\n",
       "  0.027691885828971863,\n",
       "  0.01406555250287056,\n",
       "  0.04530791938304901,\n",
       "  0.011451407335698605,\n",
       "  0.0,\n",
       "  0.0022556723561137915,\n",
       "  0.011241475120186806,\n",
       "  0.328240305185318,\n",
       "  0.2486472725868225,\n",
       "  0.07530052214860916,\n",
       "  0.15682029724121094,\n",
       "  0.08797851949930191,\n",
       "  0.15235833823680878,\n",
       "  0.00294825853779912,\n",
       "  0.04925350844860077,\n",
       "  0.1510925590991974,\n",
       "  0.3276037573814392,\n",
       "  0.85542231798172,\n",
       "  0.026814566925168037,\n",
       "  0.04062260687351227,\n",
       "  0.0,\n",
       "  0.06771133095026016,\n",
       "  0.35555461049079895,\n",
       "  0.008474539965391159,\n",
       "  0.08378603309392929,\n",
       "  0.03766384720802307,\n",
       "  0.007522698026150465,\n",
       "  0.006355787627398968,\n",
       "  0.09978987276554108,\n",
       "  0.05265705659985542,\n",
       "  0.01225628238171339,\n",
       "  0.023824527859687805,\n",
       "  0.008625786751508713,\n",
       "  0.0003504173655528575,\n",
       "  0.06181630492210388,\n",
       "  0.07363689690828323,\n",
       "  0.03389569744467735,\n",
       "  0.0855468288064003,\n",
       "  0.05941859260201454,\n",
       "  0.33104315400123596,\n",
       "  0.12376487255096436,\n",
       "  0.10012756288051605,\n",
       "  0.2580775320529938,\n",
       "  0.27996179461479187,\n",
       "  0.04784483462572098,\n",
       "  0.09490667283535004,\n",
       "  0.033415287733078,\n",
       "  0.030964449048042297,\n",
       "  0.02481319196522236,\n",
       "  0.267095685005188,\n",
       "  0.2325417846441269,\n",
       "  0.4380875527858734,\n",
       "  0.11136579513549805,\n",
       "  0.23852978646755219,\n",
       "  0.026223117485642433,\n",
       "  0.01047163363546133,\n",
       "  0.025335729122161865,\n",
       "  0.36510762572288513,\n",
       "  0.06207149103283882,\n",
       "  0.049432896077632904,\n",
       "  0.017009323462843895,\n",
       "  0.3064086437225342,\n",
       "  0.05260597541928291,\n",
       "  0.4181554913520813,\n",
       "  0.012145631946623325,\n",
       "  0.39284655451774597,\n",
       "  0.44776448607444763,\n",
       "  0.0,\n",
       "  0.1988874077796936,\n",
       "  0.0033928717020899057,\n",
       "  0.07668238133192062,\n",
       "  0.17183656990528107,\n",
       "  0.0662698820233345,\n",
       "  0.0035133196506649256,\n",
       "  0.030036253854632378,\n",
       "  0.4279525578022003,\n",
       "  0.0263944324105978,\n",
       "  0.18601001799106598,\n",
       "  0.055108264088630676,\n",
       "  0.06879237294197083,\n",
       "  0.04034360125660896,\n",
       "  0.043742042034864426,\n",
       "  0.012513426132500172,\n",
       "  0.14296959340572357,\n",
       "  0.16838786005973816,\n",
       "  0.20252561569213867,\n",
       "  0.013407420367002487,\n",
       "  0.3585456907749176,\n",
       "  0.01683182083070278,\n",
       "  0.05928252264857292,\n",
       "  0.003744535380974412,\n",
       "  0.08040115237236023,\n",
       "  0.006300047971308231,\n",
       "  0.018833927810192108,\n",
       "  0.07474960386753082,\n",
       "  0.05239272117614746,\n",
       "  0.07791401445865631,\n",
       "  0.07988303154706955,\n",
       "  0.09181858599185944,\n",
       "  0.13905829191207886,\n",
       "  0.09388937801122665,\n",
       "  0.047640226781368256,\n",
       "  0.1355160027742386,\n",
       "  0.0022222173865884542,\n",
       "  0.04372372478246689,\n",
       "  0.231994166970253,\n",
       "  0.2518961429595947,\n",
       "  0.014913744293153286,\n",
       "  0.026387689635157585,\n",
       "  0.020254122093319893,\n",
       "  0.330068439245224,\n",
       "  0.04741301015019417,\n",
       "  0.1629166603088379,\n",
       "  0.18813379108905792,\n",
       "  0.09046843647956848,\n",
       "  0.0,\n",
       "  0.0009172439458779991,\n",
       "  0.11364985257387161,\n",
       "  0.011809389106929302,\n",
       "  0.28613224625587463,\n",
       "  0.054069094359874725,\n",
       "  0.15804746747016907,\n",
       "  0.04974517226219177,\n",
       "  0.17246274650096893,\n",
       "  0.20215530693531036,\n",
       "  0.00891406275331974,\n",
       "  0.010944277979433537,\n",
       "  0.13150137662887573,\n",
       "  0.12047255039215088,\n",
       "  0.0,\n",
       "  0.24322842061519623,\n",
       "  0.01591937057673931,\n",
       "  0.15029169619083405,\n",
       "  0.25034815073013306,\n",
       "  0.07225504517555237,\n",
       "  0.10016704350709915,\n",
       "  0.27079808712005615,\n",
       "  0.11458788067102432,\n",
       "  0.12973277270793915,\n",
       "  0.07903064042329788,\n",
       "  0.06002827733755112,\n",
       "  0.13536055386066437,\n",
       "  0.06893546879291534,\n",
       "  0.19659999012947083,\n",
       "  0.06719832122325897,\n",
       "  0.04516218975186348,\n",
       "  0.23591966927051544,\n",
       "  0.3097590506076813,\n",
       "  0.13135753571987152,\n",
       "  0.009233935736119747,\n",
       "  0.11069908738136292,\n",
       "  0.024430621415376663,\n",
       "  0.2412165254354477,\n",
       "  0.31935083866119385,\n",
       "  0.04925507679581642,\n",
       "  0.0028170656878501177,\n",
       "  0.012663479894399643,\n",
       "  0.013429793529212475,\n",
       "  0.0,\n",
       "  0.033900998532772064,\n",
       "  0.027397707104682922,\n",
       "  0.07117385417222977,\n",
       "  0.027018435299396515,\n",
       "  0.0,\n",
       "  0.016639184206724167,\n",
       "  0.03305312246084213,\n",
       "  0.0,\n",
       "  0.06092746555805206,\n",
       "  0.07442827522754669,\n",
       "  0.16290360689163208,\n",
       "  0.08198418468236923,\n",
       "  0.005983792245388031,\n",
       "  0.13871333003044128,\n",
       "  0.3176824152469635,\n",
       "  0.06317108124494553,\n",
       "  0.020432794466614723,\n",
       "  0.12437872588634491,\n",
       "  0.42021289467811584,\n",
       "  0.2295907735824585,\n",
       "  0.10554251074790955,\n",
       "  0.31018194556236267,\n",
       "  0.3074764609336853,\n",
       "  0.4353239834308624,\n",
       "  0.1805455982685089,\n",
       "  0.04541921988129616,\n",
       "  0.4089265465736389,\n",
       "  0.018090667203068733,\n",
       "  0.24382418394088745,\n",
       "  0.1383213996887207,\n",
       "  0.24663136899471283,\n",
       "  0.05028856173157692,\n",
       "  0.0,\n",
       "  0.3090394139289856,\n",
       "  0.3274049758911133,\n",
       "  0.0067846328020095825,\n",
       "  0.23237287998199463,\n",
       "  0.12338506430387497,\n",
       "  0.05631227418780327,\n",
       "  0.9098372459411621,\n",
       "  0.3442789912223816,\n",
       "  0.04003329947590828,\n",
       "  0.014526378363370895,\n",
       "  0.09642587602138519,\n",
       "  0.12922044098377228,\n",
       "  0.20239964127540588,\n",
       "  0.0,\n",
       "  0.06593333184719086,\n",
       "  0.28161588311195374,\n",
       "  0.3157711625099182,\n",
       "  0.019233647733926773,\n",
       "  0.02493014559149742,\n",
       "  0.1584976315498352,\n",
       "  0.04748796299099922,\n",
       "  0.21858282387256622,\n",
       "  0.8178033232688904,\n",
       "  0.04406457766890526,\n",
       "  0.013913664035499096,\n",
       "  0.011084815487265587,\n",
       "  0.08284129202365875,\n",
       "  1.097260594367981,\n",
       "  0.184859961271286,\n",
       "  0.07893992960453033,\n",
       "  0.019484099000692368,\n",
       "  0.05405154451727867,\n",
       "  0.04556819796562195,\n",
       "  0.05373932793736458,\n",
       "  0.02446611225605011,\n",
       "  0.00536864809691906,\n",
       "  0.10043533891439438,\n",
       "  0.0027399456594139338,\n",
       "  0.06515160202980042,\n",
       "  0.01469418965280056,\n",
       "  0.0018708615098148584,\n",
       "  0.005118475761264563,\n",
       "  0.11666380614042282,\n",
       "  0.0,\n",
       "  0.29723477363586426,\n",
       "  0.0514310859143734,\n",
       "  0.06564953178167343,\n",
       "  0.08997552841901779,\n",
       "  0.058500971645116806,\n",
       "  0.05896313115954399,\n",
       "  0.04455189034342766,\n",
       "  0.025175021961331367,\n",
       "  0.12104474753141403,\n",
       "  0.176190584897995,\n",
       "  0.0754544585943222,\n",
       "  0.17150267958641052,\n",
       "  0.0022885447833687067,\n",
       "  0.2751677930355072,\n",
       "  0.0765591636300087,\n",
       "  0.00930849276483059,\n",
       "  0.025306202471256256,\n",
       "  0.3517613410949707,\n",
       "  0.00010396952711744234,\n",
       "  0.01882421039044857,\n",
       "  0.1041603684425354,\n",
       "  0.08234336972236633,\n",
       "  0.18153010308742523,\n",
       "  0.30329740047454834,\n",
       "  0.019816024228930473,\n",
       "  0.009412101469933987,\n",
       "  0.2892600893974304,\n",
       "  0.0,\n",
       "  0.0001515630428912118],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_213169/3520072505.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tr_results = np.asarray(train_results)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tr_results = np.asarray(train_results)\n",
    "tr_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tr_results[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "tr_results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "w = 0.8    # bar width\n",
    "x = [1, 2] # x-coordinates of your bars\n",
    "colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "y = [np.random.random(30) * 2 + 5,       # data series\n",
    "    np.random.random(10) * 3 + 8]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x,\n",
    "       height=[np.mean(yi) for yi in y],\n",
    "       yerr=[np.std(yi) for yi in y],    # error bars\n",
    "       capsize=12, # error bar cap width in points\n",
    "       width=w,    # bar width\n",
    "       tick_label=[\"control\", \"test\"],\n",
    "       color=(0,0,0,0),  # face color transparent\n",
    "       edgecolor=colors,\n",
    "       #ecolor=colors,    # error bar colors; setting this raises an error for whatever reason.\n",
    "       )\n",
    "\n",
    "for i in range(len(x)):\n",
    "    # distribute scatter randomly across whole width of bar\n",
    "    ax.scatter(x[i] + np.random.random(y[i].size) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(f'{model_name}_training_loss.npy', tr_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_training_accuracy.npy', tr_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_validation_loss.npy', v_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_validation_accuracy.npy', v_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_test_loss.npy', tst_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_test_accuracy.npy', tst_acc, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_loss = np.load(f'{model_name}_training_loss.npy', allow_pickle=True)\n",
    "training_accuracy = np.load(f'{model_name}_training_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "validation_loss = np.load(f'{model_name}_validation_loss.npy', allow_pickle=True)\n",
    "validation_accuracy = np.load(f'{model_name}_validation_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "test_loss = np.load(f'{model_name}_test_loss.npy', allow_pickle=True)\n",
    "test_accuracy = np.load(f'{model_name}_test_accuracy.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_loss, label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Training and Validation Loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, label='Validation Accuracy')\n",
    "plt.plot(epochs, test_accuracy, label='Test Accuracy')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    "plt.ylim(0,1)\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Accuracy vs. Epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fraction = [0,0]\n",
    "\n",
    "train_fraction = [0,0]\n",
    "val_fraction = [0,0]\n",
    "test_fraction = [0,0]\n",
    "\n",
    "for grph in train_dataset: \n",
    "    if grph.y == 1: \n",
    "        train_fraction[1] +=1\n",
    "        dataset_fraction[1] +=1 \n",
    "    else: \n",
    "        train_fraction[0] +=1\n",
    "        dataset_fraction[0] +=1 \n",
    "\n",
    "for grph in val_dataset: \n",
    "    if grph.y == 1:\n",
    "         val_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1  \n",
    "    else:\n",
    "         val_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "for grph in test_dataset: \n",
    "    if grph.y == 1:\n",
    "         test_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1 \n",
    "    else:\n",
    "         test_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "print(f'Overall dataset percentage of label 1 = {dataset_fraction[1]/len(dataset)})')\n",
    "print(f'Training dataset percentage of label 1 = {train_fraction} = {train_fraction[1]/len(train_dataset)}')\n",
    "print(f'Validation dataset percentage of label 1 = {val_fraction} = {val_fraction[1]/len(val_dataset)}')\n",
    "print(f'Test dataset percentage of label 1 = {test_fraction} = {test_fraction[1]/len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, pos0, adj0 = torch.load(f'{model_name}_img0_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN\n",
    "print(x0[0].shape)\n",
    "x0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos0[0].shape)\n",
    "pos0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj0[0].shape)\n",
    "adj0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj0[0])\n",
    "visualize_points(pos0[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph After 1st Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_emb, x1_pool, pos1, adj1, s1= torch.load(f'{model_name}_img1_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj0 @ x_0 @ w_gnn_emb)\n",
    "print(x1_emb[0].shape)\n",
    "x1_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj_0 @ x_0 @ w_gnn_pool\n",
    "print(s1[0].shape)\n",
    "s1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s).t() @ pos_in)\n",
    "print(pos1[0].shape)\n",
    "pos1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s).t() @ x_in)\n",
    "print(x1_pool[0].shape)\n",
    "x1_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix = softmax(adj_out = softmax(s.t()) @ adj_in @ softmax(s))\n",
    "print(adj1[0].shape)\n",
    "adj1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj1[0])\n",
    "visualize_points(pos1[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 2nd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_emb, x2_pool, pos2, adj2, s2 = torch.load(f'{model_name}_img2_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj1 @ x1_pool @ w_gnn_emb)\n",
    "print(x2_emb[0].shape)\n",
    "x2_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj1 @ x1_pool @ w_gnn_pool), dim=1\n",
    "print(s2[0].shape)\n",
    "s2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos2[0].shape)\n",
    "pos2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s2).t() @ x2_emb)\n",
    "print(x2_pool[0].shape)\n",
    "x2_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s).T @ adj @ softmax(s)\n",
    "print(adj2[0].shape)\n",
    "adj2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj2[0])\n",
    "visualize_points(pos2[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 3rd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_emb, x3_pool, pos3, adj3, s3 = torch.load(f'{model_name}_img3_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj_0 @ x_0 @ w_gnn_emb)\n",
    "print(x3_emb[0].shape)\n",
    "x3_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: torch.softmax(adj_0 @ x_0 @ w_gnn_pool), dim=1)\n",
    "print(s3[0].shape)\n",
    "s3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos3[0].shape)\n",
    "pos3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s.t()) @ x_0)\n",
    "print(x3_pool[0].shape)\n",
    "x3_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s.t()) @ adj @ softmax(s)\n",
    "print(adj3[0].shape)\n",
    "adj3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj3[0])\n",
    "visualize_points(pos3[0].cpu(), edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (tags/v3.9.13:6de2ca5, May 17 2022, 16:36:42) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5295f743bc4e47f7cb4c7d5e484c5cf6bb52e824ff35c3fecf2642e2b62ae0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
