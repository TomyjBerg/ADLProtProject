{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required packages.\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from plyfile import PlyData, PlyElement\n",
    "from helper_functions import load_object, save_object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = 'C:/Users/thoma/Desktop/ZHAW MLS/David prod/ADLProtProject/surface'\n",
    "path = 'C:/Users/david/pyproj/pyg/adl/surfaces'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List file on so \n",
    "from os import walk\n",
    "def list_files(mpath):\n",
    "  f = []\n",
    "  for (dirpath, dirnames, filenames) in walk(mpath):\n",
    "      f.extend(filenames)\n",
    "      break\n",
    "  return(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'C:/Users/david/pyproj/pyg/adl/surfaces'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# I think this would do the trick\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m files_ply \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(path)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'C:/Users/david/pyproj/pyg/adl/surfaces'"
     ]
    }
   ],
   "source": [
    "# I think this would do the trick\n",
    "files_ply = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ply_file(folder_path,file_name,requirement):\n",
    "\n",
    "    plydata = PlyData.read(folder_path + '/' + file_name)\n",
    "    \n",
    "    feature = pd.DataFrame(plydata.elements[0].data)\n",
    "    vertex_indices = pd.DataFrame(plydata.elements[1].data)\n",
    "\n",
    "    if requirement == 'feature':\n",
    "        return feature\n",
    "    if requirement == 'vertex':\n",
    "        return vertex_indices\n",
    "    if requirement == 'plyfile':\n",
    "        return plydata\n",
    "    if requirement == 'all' :\n",
    "        return [feature,vertex_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "PlyHeaderParseError",
     "evalue": "line 1: expected one of {ply}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPlyHeaderParseError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 46\u001b[0m\n\u001b[0;32m     40\u001b[0m         complexes_names\u001b[39m.\u001b[39mappend(prot_names)\n\u001b[0;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m  complexes,complexes_names\n\u001b[1;32m---> 46\u001b[0m feature,vetex,ply_files \u001b[39m=\u001b[39m read_ply_folder(path)\n\u001b[0;32m     47\u001b[0m complexes_feature,complexes_names \u001b[39m=\u001b[39m complex_separation(ply_files,feature)\n\u001b[0;32m     48\u001b[0m \u001b[39mprint\u001b[39m(complexes_names)\n",
      "Cell \u001b[1;32mIn [10], line 4\u001b[0m, in \u001b[0;36mread_ply_folder\u001b[1;34m(folder_path)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_ply_folder\u001b[39m(folder_path):\n\u001b[0;32m      3\u001b[0m     files_ply \u001b[39m=\u001b[39m list_files(path)\n\u001b[1;32m----> 4\u001b[0m     features_prot \u001b[39m=\u001b[39m [read_ply_file(folder_path,ply_name,\u001b[39m'\u001b[39m\u001b[39mfeature\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39mfor\u001b[39;00m ply_name \u001b[39min\u001b[39;00m files_ply]\n\u001b[0;32m      5\u001b[0m     vertex_indices \u001b[39m=\u001b[39m [read_ply_file(folder_path,ply_name,\u001b[39m'\u001b[39m\u001b[39mvertex\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39mfor\u001b[39;00m ply_name \u001b[39min\u001b[39;00m files_ply]\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m features_prot,vertex_indices,files_ply\n",
      "Cell \u001b[1;32mIn [10], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_ply_folder\u001b[39m(folder_path):\n\u001b[0;32m      3\u001b[0m     files_ply \u001b[39m=\u001b[39m list_files(path)\n\u001b[1;32m----> 4\u001b[0m     features_prot \u001b[39m=\u001b[39m [read_ply_file(folder_path,ply_name,\u001b[39m'\u001b[39;49m\u001b[39mfeature\u001b[39;49m\u001b[39m'\u001b[39;49m)  \u001b[39mfor\u001b[39;00m ply_name \u001b[39min\u001b[39;00m files_ply]\n\u001b[0;32m      5\u001b[0m     vertex_indices \u001b[39m=\u001b[39m [read_ply_file(folder_path,ply_name,\u001b[39m'\u001b[39m\u001b[39mvertex\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39mfor\u001b[39;00m ply_name \u001b[39min\u001b[39;00m files_ply]\n\u001b[0;32m      7\u001b[0m     \u001b[39mreturn\u001b[39;00m features_prot,vertex_indices,files_ply\n",
      "Cell \u001b[1;32mIn [9], line 3\u001b[0m, in \u001b[0;36mread_ply_file\u001b[1;34m(folder_path, file_name, requirement)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mread_ply_file\u001b[39m(folder_path,file_name,requirement):\n\u001b[1;32m----> 3\u001b[0m     plydata \u001b[39m=\u001b[39m PlyData\u001b[39m.\u001b[39;49mread(folder_path \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m \u001b[39m+\u001b[39;49m file_name)\n\u001b[0;32m      5\u001b[0m     feature \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(plydata\u001b[39m.\u001b[39melements[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdata)\n\u001b[0;32m      6\u001b[0m     vertex_indices \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(plydata\u001b[39m.\u001b[39melements[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\ZHAW MLS\\David prod\\ADLProtProject\\.venv\\lib\\site-packages\\plyfile.py:395\u001b[0m, in \u001b[0;36mPlyData.read\u001b[1;34m(stream, mmap)\u001b[0m\n\u001b[0;32m    393\u001b[0m (must_close, stream) \u001b[39m=\u001b[39m _open_stream(stream, \u001b[39m'\u001b[39m\u001b[39mread\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    394\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 395\u001b[0m     data \u001b[39m=\u001b[39m PlyData\u001b[39m.\u001b[39;49m_parse_header(stream)\n\u001b[0;32m    396\u001b[0m     \u001b[39mfor\u001b[39;00m elt \u001b[39min\u001b[39;00m data:\n\u001b[0;32m    397\u001b[0m         elt\u001b[39m.\u001b[39m_read(stream, data\u001b[39m.\u001b[39mtext, data\u001b[39m.\u001b[39mbyte_order, mmap)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\ZHAW MLS\\David prod\\ADLProtProject\\.venv\\lib\\site-packages\\plyfile.py:373\u001b[0m, in \u001b[0;36mPlyData._parse_header\u001b[1;34m(stream)\u001b[0m\n\u001b[0;32m    368\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39mParse a PLY header from a readable file-like stream.\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \n\u001b[0;32m    371\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m    372\u001b[0m parser \u001b[39m=\u001b[39m _PlyHeaderParser()\n\u001b[1;32m--> 373\u001b[0m \u001b[39mwhile\u001b[39;00m parser\u001b[39m.\u001b[39;49mconsume(stream\u001b[39m.\u001b[39;49mreadline()):\n\u001b[0;32m    374\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \u001b[39mreturn\u001b[39;00m PlyData(\n\u001b[0;32m    377\u001b[0m     [PlyElement(\u001b[39m*\u001b[39me) \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m parser\u001b[39m.\u001b[39melements],\n\u001b[0;32m    378\u001b[0m     parser\u001b[39m.\u001b[39mformat \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mascii\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    381\u001b[0m     parser\u001b[39m.\u001b[39mobj_info\n\u001b[0;32m    382\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\ZHAW MLS\\David prod\\ADLProtProject\\.venv\\lib\\site-packages\\plyfile.py:127\u001b[0m, in \u001b[0;36m_PlyHeaderParser.consume\u001b[1;34m(self, raw_line)\u001b[0m\n\u001b[0;32m    124\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_error()\n\u001b[0;32m    126\u001b[0m \u001b[39mif\u001b[39;00m keyword \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allowed:\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_error(\u001b[39m\"\u001b[39;49m\u001b[39mexpected one of \u001b[39;49m\u001b[39m{\u001b[39;49m\u001b[39m%s\u001b[39;49;00m\u001b[39m}\u001b[39;49m\u001b[39m\"\u001b[39;49m \u001b[39m%\u001b[39;49m\n\u001b[0;32m    128\u001b[0m                 \u001b[39m\"\u001b[39;49m\u001b[39m, \u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_allowed))\n\u001b[0;32m    130\u001b[0m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mparse_\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m keyword)(line[\u001b[39mlen\u001b[39m(keyword)\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:])\n\u001b[0;32m    131\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_allowed\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\ZHAW MLS\\David prod\\ADLProtProject\\.venv\\lib\\site-packages\\plyfile.py:134\u001b[0m, in \u001b[0;36m_PlyHeaderParser._error\u001b[1;34m(self, message)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_error\u001b[39m(\u001b[39mself\u001b[39m, message\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mparse error\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 134\u001b[0m     \u001b[39mraise\u001b[39;00m PlyHeaderParseError(message, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlines)\n",
      "\u001b[1;31mPlyHeaderParseError\u001b[0m: line 1: expected one of {ply}"
     ]
    }
   ],
   "source": [
    "def read_ply_folder(folder_path):\n",
    "\n",
    "    files_ply = list_files(path)\n",
    "    features_prot = [read_ply_file(folder_path,ply_name,'feature')  for ply_name in files_ply]\n",
    "    vertex_indices = [read_ply_file(folder_path,ply_name,'vertex')  for ply_name in files_ply]\n",
    "    \n",
    "    return features_prot,vertex_indices,files_ply\n",
    "\n",
    "\n",
    "def complex_separation(files_ply,df):\n",
    "    \n",
    "    complexes_names = []\n",
    "    complexes = []\n",
    "    uniq_names= []\n",
    "    for f in files_ply :\n",
    "        sh = f.find('_')\n",
    "        val = f[0:sh]\n",
    "        if val not in uniq_names:\n",
    "            uniq_names.append(val)\n",
    "    for name in uniq_names:\n",
    "        prot = []\n",
    "        prot_names = []\n",
    "        max_len = 0\n",
    "\n",
    "        for i in range(len(files_ply)):\n",
    "            sh = files_ply[i].find('_')\n",
    "            val = files_ply[i][0:sh]\n",
    "            if name == val:\n",
    "                us = len(files_ply[i])\n",
    "                len_c = len(files_ply[i][sh:us])\n",
    "                if len_c > max_len:\n",
    "                    prot.append(df[i])\n",
    "                    prot_names.append(files_ply[i])\n",
    "                    max_len = len_c\n",
    "                else:\n",
    "                    prot.insert(0,df[i])\n",
    "                    prot_names.insert(0,files_ply[i])\n",
    "\n",
    "        complexes.append(prot)\n",
    "        complexes_names.append(prot_names)\n",
    "        \n",
    "    return  complexes,complexes_names\n",
    "        \n",
    "            \n",
    "\n",
    "feature,vetex,ply_files = read_ply_folder(path)\n",
    "complexes_feature,complexes_names = complex_separation(ply_files,feature)\n",
    "print(complexes_names)\n",
    "print(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from c_ProteinGraph import ProteinGraph\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "\n",
    "source_dir = path\n",
    "dest_dir = 'C:/Users/david/pyproj/pyg/adl/surfaces_graphs'\n",
    "\n",
    "for complex in complexes_names: \n",
    "\n",
    "    if len(complex) != 3: # Check if there are three files of the complex\n",
    "        continue\n",
    "    else:\n",
    "        for file in complex[0:2]: # only do it for the subunits, not for the complex\n",
    "            name = file[0:-4]\n",
    "\n",
    "            if f'{name}.pkl' not in os.listdir(dest_dir): #Only do it for proteins that are not yet in source dir\n",
    "\n",
    "                features, edges = read_ply_file(path, file,'all')\n",
    "                    \n",
    "                pos = features.loc[:,[\"x\",\"y\",\"z\"]].to_numpy()\n",
    "\n",
    "                features = features.loc[:,[\"charge\",\"hbond\",\"hphob\", \"iface\", 'nx', 'ny', 'nz']]\n",
    "\n",
    "                    \n",
    "                ## Translate edges into a adjacency matrix and then edge_index\n",
    "                triangles = np.asarray([list(triangle[0]) for _,triangle in edges.iterrows()])\n",
    "                n_nodes = pos.shape[0]\n",
    "                adj = np.identity(n_nodes)\n",
    "                for n1, n2, n3 in triangles:\n",
    "                        \n",
    "                    # edge1\n",
    "                    adj[n1][n2] = 1\n",
    "                    adj[n2][n1] = 1 # set location in adj = 1\n",
    "\n",
    "                    #edge2\n",
    "                    adj[n1][n3] = 1\n",
    "                    adj[n3][n1] = 1\n",
    "\n",
    "                    #edge3\n",
    "                    adj[n2][n3] = 1\n",
    "                    adj[n3][n2] = 1\n",
    "\n",
    "                adj = torch.from_numpy(adj)\n",
    "                edge_index, _ = dense_to_sparse(adj)\n",
    "\n",
    "\n",
    "                protein = ProteinGraph(features=features, pos=pos, edge_index=edge_index, name=name)\n",
    "                save_object(protein, dest_dir + f'/{name}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Number of Nodes: 4444\n",
      "            Features: (4444, 7)\n",
      "            Edge_Index: (2, 31096)\n",
      "            Coordinates of Nodes: (4444, 3)\n",
      "            Protein Name: 1DK4_B\n"
     ]
    }
   ],
   "source": [
    "from c_ProteinGraph import ProteinGraph\n",
    "\n",
    "path = 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces/surfaces_graphs'\n",
    "\n",
    "files_graph_name = list_files(path)\n",
    "files_graph = []\n",
    "\n",
    "for f in files_graph_name:\n",
    "    files_graph.append(load_object(path + '/' + f))\n",
    "\n",
    "print(files_graph[45])\n",
    "\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "graph_prot_1 = files_graph[0]\n",
    "graph_prot_2 = files_graph[45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Number of Nodes: 5436\n",
      "            Features: (5436, 7)\n",
      "            Edge_Index: (2, 38052)\n",
      "            Coordinates of Nodes: (5436, 3)\n",
      "            Protein Name: 1A99_C\n"
     ]
    }
   ],
   "source": [
    "print(graph_prot_1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "\n",
    "from torch_geometric.typing import PairTensor\n",
    "from torch_geometric.utils.mask import index_to_mask\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "\n",
    "\n",
    "\n",
    "def k_hop_subgraph_perso(\n",
    "    node_idx: Union[int, List[int], Tensor],\n",
    "    num_hops: int,\n",
    "    edge_index: Tensor,\n",
    "    relabel_nodes: bool = False,\n",
    "    num_nodes: Optional[int] = None,\n",
    "    flow: str = 'source_to_target',\n",
    "    max_nodes = int\n",
    ") -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
    "\n",
    "    num_nodes = maybe_num_nodes(edge_index, num_nodes)\n",
    "\n",
    "    assert flow in ['source_to_target', 'target_to_source']\n",
    "    if flow == 'target_to_source':\n",
    "        row, col = edge_index\n",
    "    else:\n",
    "        col, row = edge_index\n",
    "\n",
    "    node_mask = row.new_empty(num_nodes, dtype=torch.bool)\n",
    "    edge_mask = row.new_empty(row.size(0), dtype=torch.bool)\n",
    "\n",
    "    if isinstance(node_idx, (int, list, tuple)):\n",
    "        node_idx = torch.tensor([node_idx], device=row.device).flatten()\n",
    "    else:\n",
    "        node_idx = node_idx.to(row.device)\n",
    "\n",
    "    subsets = [node_idx]\n",
    "\n",
    "    for _ in range(num_hops):\n",
    "        if len(subsets) > max_nodes:\n",
    "            break\n",
    "        node_mask.fill_(False)\n",
    "        node_mask[subsets[-1]] = True\n",
    "        torch.index_select(node_mask, 0, row, out=edge_mask)\n",
    "        print(col[edge_mask[0]])\n",
    "        subsets.append(col[edge_mask])\n",
    "        \n",
    "\n",
    "    subset, inv = torch.cat(subsets).unique(return_inverse=True)\n",
    "    inv = inv[:node_idx.numel()]\n",
    "\n",
    "    node_mask.fill_(False)\n",
    "    node_mask[subset] = True\n",
    "    edge_mask = node_mask[row] & node_mask[col]\n",
    "\n",
    "    edge_index = edge_index[:, edge_mask]\n",
    "\n",
    "    if relabel_nodes:\n",
    "        node_idx = row.new_full((num_nodes, ), -1)\n",
    "        node_idx[subset] = torch.arange(subset.size(0), device=row.device)\n",
    "        edge_index = node_idx[edge_index]\n",
    "\n",
    "    return subset, edge_index, inv, edge_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils.subgraph import k_hop_subgraph\n",
    "\n",
    "subset,edge_index,mapping,edege_mask = k_hop_subgraph_perso(7,3,graph_prot_2.edge_index,max_nodes=12)\n",
    "print(len(subset))\n",
    "#print(subset)\n",
    "\n",
    "from f_visualize_pointcloud import visualize_pointcloud\n",
    "\n",
    "colors_protein = np.zeros_like(graph_prot_2.pos)\n",
    "colors_protein[:] = [1,0,0]\n",
    "colors_protein[subset] = [0,0,1]\n",
    "\n",
    "\n",
    "#visualize_pointcloud(graph_prot_2.pos, colors = colors_protein)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 524])\n"
     ]
    }
   ],
   "source": [
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test what is in a pickle file now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces/graphs/1A99_C.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mdir\u001b[39m \u001b[39m=\u001b[39m path \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/graphs\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 2\u001b[0m test \u001b[39m=\u001b[39m load_object(\u001b[39mdir\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39m/1A99_C.pkl\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39mprint\u001b[39m(test)\n",
      "File \u001b[1;32mc:\\Users\\thoma\\Desktop\\ZHAW MLS\\David prod\\ADLProtProject\\helper_functions.py:9\u001b[0m, in \u001b[0;36mload_object\u001b[1;34m(filename)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_object\u001b[39m(filename):\n\u001b[1;32m----> 9\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(filename, \u001b[39m'\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m     10\u001b[0m         \u001b[39mreturn\u001b[39;00m pickle\u001b[39m.\u001b[39mload(f)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces/graphs/1A99_C.pkl'"
     ]
    }
   ],
   "source": [
    "dir = path + '/graphs'\n",
    "test = load_object(dir + '/1A99_C.pkl')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m test\u001b[39m.\u001b[39mnum_features()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "test.num_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.num_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.patches = ((edge_index, label, center_index), (edge_index, label ), ())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3baeb0c3f97feb3023477fbaa09b9f4da769e45e64d8febc6957bb84d33ff77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
