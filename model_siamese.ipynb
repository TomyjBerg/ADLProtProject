{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize features? \n",
    "## Invert h-bond and charge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_siamese_071222'\n",
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader #To make use of this data loader, all graph attributes in the dataset need to have the same shape. In particular, this data loader should only be used when working with dense adjacency matrices.\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_1 = 'C:/Users/david/pyproj/pyg/adl/patch_label_1'\n",
    "data_dir_0 = 'C:/Users/david/pyproj/pyg/adl/patch_label_0'\n",
    "#data_dir_1 = 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces-DESKTOP-AQ00763/patch_label_1'\n",
    "#data_dir_0 = 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces-DESKTOP-AQ00763/patch_label_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from c_PatchDataset import PatchDataset\n",
    "dataset = PatchDataset(data_dir_label_0 = data_dir_0,  data_dir_label_1=data_dir_1,  neg_pos_ratio=1)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(572):\n",
      "====================\n",
      "Number of graphs pairs: 572\n",
      "\n",
      "PairData(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=0)\n",
      "=============================================================\n",
      "Number of nodes in each: None\n",
      "Number of node features: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:271: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'y', 'adj1', 'x2', 'x1', 'adj2'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs pairs: {len(dataset)}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes in each: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3503, -0.0233, -0.9363],\n",
       "        [-0.9692,  0.0000, -0.2463],\n",
       "        [-0.2931, -0.7217, -0.6271],\n",
       "        [-0.5819, -0.5850, -0.5650],\n",
       "        [ 0.5537,  0.5213, -0.6494],\n",
       "        [ 0.0188,  0.0000, -0.9998],\n",
       "        [ 0.1567,  0.1816, -0.9708],\n",
       "        [ 0.0884,  0.1744, -0.9807],\n",
       "        [-0.6031, -0.0094, -0.7976],\n",
       "        [ 0.0826,  0.0000, -0.9966],\n",
       "        [ 0.2023,  0.0000,  0.9793],\n",
       "        [ 0.0501,  0.1323, -0.9899],\n",
       "        [-0.3994, -0.1604, -0.9026],\n",
       "        [-0.1574,  0.0000, -0.9875],\n",
       "        [-0.0123,  0.0504, -0.9987],\n",
       "        [-0.8541,  0.0000, -0.5201],\n",
       "        [ 0.2201,  0.0000, -0.9755],\n",
       "        [-0.8022, -0.4657, -0.3737],\n",
       "        [ 0.3395, -0.6544, -0.6756],\n",
       "        [ 0.6591,  0.6025, -0.4501],\n",
       "        [ 0.1213,  0.0641, -0.9905],\n",
       "        [-0.5164,  0.0000, -0.8563],\n",
       "        [ 0.6997,  0.2051, -0.6843],\n",
       "        [ 0.1851, -0.4083, -0.8939],\n",
       "        [ 0.7274,  0.4809, -0.4896],\n",
       "        [-0.0153,  0.0000, -0.9999],\n",
       "        [-0.4952, -0.0448, -0.8676],\n",
       "        [-0.0110,  0.0000, -0.9999],\n",
       "        [ 0.0261,  0.0000, -0.9997],\n",
       "        [ 0.2569,  0.0000, -0.9664],\n",
       "        [ 0.3892,  0.0000, -0.9211],\n",
       "        [ 0.3089,  0.0000, -0.9511],\n",
       "        [-0.2480,  0.0000, -0.9688],\n",
       "        [ 0.2289,  0.0000, -0.9735],\n",
       "        [ 0.6134,  0.0000, -0.7898],\n",
       "        [-0.5679,  0.0000, -0.8231],\n",
       "        [-0.0088,  0.0000, -1.0000],\n",
       "        [ 0.6097,  0.0110, -0.7926],\n",
       "        [ 0.0991,  0.0000, -0.9951],\n",
       "        [ 0.1557,  0.0000, -0.9878],\n",
       "        [-0.5891, -0.5830, -0.5595],\n",
       "        [ 0.0324,  0.1282, -0.9912],\n",
       "        [-0.7920, -0.0582, -0.6077],\n",
       "        [ 0.3496,  0.0000, -0.9369],\n",
       "        [ 0.0271,  0.0000, -0.9996],\n",
       "        [ 0.3106,  0.0000, -0.9505],\n",
       "        [ 0.0227, -0.3480, -0.9372],\n",
       "        [-0.3419, -0.0763, -0.9366],\n",
       "        [ 0.8789,  0.1187, -0.4620],\n",
       "        [-0.8950, -0.0381, -0.4444],\n",
       "        [-0.1960, -0.0315, -0.9801],\n",
       "        [-0.0153, -0.2299,  0.9731],\n",
       "        [ 0.0145,  0.0000, -0.9999],\n",
       "        [ 0.1151,  0.0683, -0.9910],\n",
       "        [ 0.3581, -0.5940, -0.7203],\n",
       "        [ 0.3571,  0.0000, -0.9341],\n",
       "        [ 0.1188,  0.0000, -0.9929],\n",
       "        [ 0.3111,  0.1239, -0.9423],\n",
       "        [ 0.0778,  0.0000, -0.9970],\n",
       "        [-0.5463, -0.0813, -0.8336],\n",
       "        [ 0.1541,  0.0798, -0.9848],\n",
       "        [ 0.5916,  0.0000, -0.8062],\n",
       "        [ 0.1340,  0.0000, -0.9910],\n",
       "        [-0.2185, -0.1711, -0.9607],\n",
       "        [ 0.6523,  0.1554, -0.7419],\n",
       "        [-0.1093, -0.3193, -0.9413],\n",
       "        [-0.6963, -0.3846, -0.6060],\n",
       "        [-0.8567,  0.0000, -0.5159],\n",
       "        [-0.0011, -0.7588, -0.6514],\n",
       "        [ 0.1798,  0.0036, -0.9837],\n",
       "        [ 0.2954,  0.2035, -0.9334],\n",
       "        [ 0.0928,  0.0000, -0.9957],\n",
       "        [-0.2274,  0.0000, -0.9738],\n",
       "        [ 0.0215,  0.0000, -0.9998],\n",
       "        [ 0.0600,  0.0000, -0.9982],\n",
       "        [ 0.1284,  0.0000, -0.9917],\n",
       "        [-0.8234, -0.1356, -0.5510],\n",
       "        [ 0.0658,  0.0000,  0.9978],\n",
       "        [-0.0690,  0.0000, -0.9976],\n",
       "        [-0.7586, -0.3700, -0.5363],\n",
       "        [ 0.4496,  0.0000, -0.8932],\n",
       "        [ 0.0746,  0.0000, -0.9972],\n",
       "        [-0.4016, -0.0272, -0.9154],\n",
       "        [ 0.2847,  0.0240, -0.9583],\n",
       "        [ 0.0691,  0.0000, -0.9976],\n",
       "        [ 0.3830,  0.0000, -0.9238],\n",
       "        [-0.3936,  0.0000, -0.9193],\n",
       "        [ 0.4639,  0.4556, -0.7597],\n",
       "        [ 0.0580,  0.0000, -0.9983],\n",
       "        [ 0.1287, -0.4554, -0.8809],\n",
       "        [ 0.0220,  0.0000, -0.9998],\n",
       "        [-0.5819,  0.0000, -0.8133],\n",
       "        [-0.8700, -0.0283, -0.4923],\n",
       "        [ 0.3424, -0.2198, -0.9135],\n",
       "        [-0.4120, -0.0111, -0.9111],\n",
       "        [ 0.0799, -0.1249, -0.9889],\n",
       "        [ 0.1596,  0.0000, -0.9872],\n",
       "        [-0.3207,  0.0000, -0.9472],\n",
       "        [-0.2116, -0.1261, -0.9692],\n",
       "        [ 0.2174,  0.0000, -0.9761]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work we do not have pos\n",
    "#visualize_points(data.pos, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs pairs: 382\n",
      "Number of validation graphs: 95\n",
      "Number of test graphs: 95\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "n_train = math.ceil((4/6) * len(dataset))\n",
    "n_val = math.ceil((len(dataset) - n_train)/2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f'Number of training graphs pairs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size= batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairDataBatch(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=[1])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = next(iter(train_loader))\n",
    "databatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_nodes, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        # Each instance of this GNN will have 3 convolutional layers and three batch norm layers        \n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bns1 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bns2 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bns3 = torch.nn.BatchNorm1d(in_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        \n",
    "        #Step 1\n",
    "        x = self.conv1(x, adj, mask)\n",
    "        x = self.bns1(x)\n",
    "        \n",
    "        #Step 2\n",
    "        x = self.conv2(x, adj, mask)\n",
    "        x = self.bns2(x)\n",
    "\n",
    "        #Step 3\n",
    "        x = self.conv3(x, adj, mask)\n",
    "        if x.shape[2] != 1: \n",
    "            x = self.bns3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        in_nodes = num_nodes\n",
    "        out_nodes = 25 # Number of clusters / nodes in the next layer\n",
    "        self.gnn1_pool = GNN(in_nodes, dataset.num_features, 16, out_nodes) # PoolGNN --> Cluster Assignment Matrix to reduce to num_nodes\n",
    "        self.gnn1_embed = GNN(in_nodes, dataset.num_features, 8, 8) # EmbGNN --> Convolutions to create new node embedding\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 10\n",
    "        self.gnn2_pool = GNN(in_nodes, 8, 8, out_nodes)\n",
    "        self.gnn2_embed = GNN(in_nodes, 8, 12, 16, lin=False)\n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 1\n",
    "        self.gnn3_pool = GNN(in_nodes, 16, 16, out_nodes)\n",
    "        self.gnn3_embed = GNN(in_nodes, 16, 16, 32, lin=False)\n",
    "\n",
    "        # Final Classifier\n",
    "        self.lin1 = torch.nn.Linear(32, 64) \n",
    "        #self.lin2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, batch, mask=None):\n",
    "        \n",
    "        #if batch == 0: print('Shape of input data batch:')\n",
    "        #if batch == 0: print(f'Feature Matrix: {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'Adjacency Matrix: {tuple(adj.shape)}')\n",
    "       \n",
    "\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        #if batch == 0: print('Hierarchical Step #1')\n",
    "        x1 = self.gnn1_embed(x, adj, mask) # node feature embedding\n",
    "        s = self.gnn1_pool(x, adj, mask) # cluster assignment matrix\n",
    "\n",
    "        #if batch == 0: print(f'X1 = {tuple(x1.shape)}    S1: {tuple(s.shape)}')\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x1, adj, s, mask) # does the necessary matrix multiplications\n",
    "        adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "   \n",
    "\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        #if batch == 0: print('Hierarchical Step #2')\n",
    "        x2 = self.gnn2_embed(x, adj)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "\n",
    "        #if batch == 0: print(f'X2: {tuple(x2.shape)}    S2: {tuple(s.shape)}')\n",
    "        \n",
    "        x, adj, l2, e2 = dense_diff_pool(x2, adj, s)\n",
    "        adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "      \n",
    "        \n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        #if batch == 0: print('Hierarchical Step #3')\n",
    "        x3 = self.gnn3_embed(x, adj)\n",
    "        s = self.gnn3_pool(x, adj)\n",
    "        \n",
    "        #if batch == 0: print(f'X3: {tuple(x3.shape)}    S3: {tuple(s.shape)}')\n",
    "\n",
    "        x, adj, l3, e3 = dense_diff_pool(x3, adj, s)\n",
    "        adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "     \n",
    "        \n",
    "\n",
    "        # Final Classification\n",
    "        #if batch == 0: print('Final Output')\n",
    "        x = x.mean(dim=1) # Pool the features of all nodes (global mean pool)  dim = 1 refers to columns\n",
    "        #if batch == 0: print(f'---X Output after mean= {tuple(x.shape)}')\n",
    "\n",
    "        x = F.relu(self.lin1(x)) # Fully connected layer + relu\n",
    "        #if batch == 0: print(f'------ X Output 3 after lin= {tuple(x.shape)}')\n",
    "\n",
    "        \n",
    "        return x, l1 + l2 + l3, e1 + e2 + e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt of a contrastive loss function\n",
    "#   pairs with label 1 --> should get small euclid dist = small loss\n",
    "#   pairs with label 0 --> should get large euclid dist = small loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y,epochs):\n",
    "        # euclidian distance\n",
    "        #print(x0)\n",
    "        #print(x1)\n",
    "        #print(y)\n",
    "        diff = x0 - x1\n",
    "        #print(diff)\n",
    "        pow = torch.pow(diff, 2)\n",
    "\n",
    "        dist_sq = torch.sum(pow, 1)\n",
    "        #print(dist_sq) # sum of squared distance = 0.5 = 9\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "        print(f'Euclid dist: {dist}') # euclidean distance\n",
    "\n",
    "        mdist = self.margin- dist #negative euclidean distance - margin = 0.3 = -2\n",
    "        #print(mdist)\n",
    "        dist_marg = torch.clamp(mdist, min=0.0) # only distances <margin will be still positive here = 0.3 = 0\n",
    "        #print(dist)\n",
    "        loss =  y * torch.pow(dist, 2) + (1-y) * torch.pow(dist_marg,2)\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 0.5\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0.3^2\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 0.5\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 9\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 9\n",
    "\n",
    "        #print(loss)\n",
    "        #loss = torch.sum(loss) / 2.0 \n",
    "        print(f'Label: {y}')\n",
    "        print(f'Loss: {loss}')\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Euclid dist: tensor([5.3376], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([4.6124], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.6006], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([2.5618], grad_fn=<AddBackward0>)\n",
      "tensor([2.5618], grad_fn=<AddBackward0>)\n",
      "2.5617928504943848\n",
      "Euclid dist: tensor([44.2894], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1961.5546], grad_fn=<AddBackward0>)\n",
      "tensor([1961.5546], grad_fn=<AddBackward0>)\n",
      "1961.5545654296875\n",
      "Euclid dist: tensor([1.6151], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([2.6087], grad_fn=<AddBackward0>)\n",
      "tensor([2.6087], grad_fn=<AddBackward0>)\n",
      "2.608694076538086\n",
      "Euclid dist: tensor([8.8399], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([78.1433], grad_fn=<AddBackward0>)\n",
      "tensor([78.1433], grad_fn=<AddBackward0>)\n",
      "78.14334106445312\n",
      "Euclid dist: tensor([6.7360], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([45.3739], grad_fn=<AddBackward0>)\n",
      "tensor([45.3739], grad_fn=<AddBackward0>)\n",
      "45.37391662597656\n",
      "Euclid dist: tensor([44.5957], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1988.7760], grad_fn=<AddBackward0>)\n",
      "tensor([1988.7760], grad_fn=<AddBackward0>)\n",
      "1988.7760009765625\n",
      "Euclid dist: tensor([2.0667], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([12.1087], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([18.3993], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([338.5351], grad_fn=<AddBackward0>)\n",
      "tensor([338.5351], grad_fn=<AddBackward0>)\n",
      "338.5350646972656\n",
      "Euclid dist: tensor([43.4136], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1884.7407], grad_fn=<AddBackward0>)\n",
      "tensor([1884.7407], grad_fn=<AddBackward0>)\n",
      "1884.74072265625\n",
      "Euclid dist: tensor([8.9781], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([42.5631], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([7.9208], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([62.7388], grad_fn=<AddBackward0>)\n",
      "tensor([62.7388], grad_fn=<AddBackward0>)\n",
      "62.738765716552734\n",
      "Euclid dist: tensor([10.5429], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([24.8922], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([619.6232], grad_fn=<AddBackward0>)\n",
      "tensor([619.6232], grad_fn=<AddBackward0>)\n",
      "619.6232299804688\n",
      "Euclid dist: tensor([21.1787], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([448.5355], grad_fn=<AddBackward0>)\n",
      "tensor([448.5355], grad_fn=<AddBackward0>)\n",
      "448.53546142578125\n",
      "Euclid dist: tensor([1.9993], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([5.0530], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([3.6305], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([41.7381], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1742.0682], grad_fn=<AddBackward0>)\n",
      "tensor([1742.0682], grad_fn=<AddBackward0>)\n",
      "1742.0682373046875\n",
      "Euclid dist: tensor([6.5528], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([42.9386], grad_fn=<AddBackward0>)\n",
      "tensor([42.9386], grad_fn=<AddBackward0>)\n",
      "42.93855285644531\n",
      "Euclid dist: tensor([19.3392], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([374.0035], grad_fn=<AddBackward0>)\n",
      "tensor([374.0035], grad_fn=<AddBackward0>)\n",
      "374.0035095214844\n",
      "Euclid dist: tensor([21.1615], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([447.8094], grad_fn=<AddBackward0>)\n",
      "tensor([447.8094], grad_fn=<AddBackward0>)\n",
      "447.8094482421875\n",
      "Euclid dist: tensor([0.3482], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.4249], grad_fn=<AddBackward0>)\n",
      "tensor([0.4249], grad_fn=<AddBackward0>)\n",
      "0.42489510774612427\n",
      "Euclid dist: tensor([0.8789], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.7725], grad_fn=<AddBackward0>)\n",
      "tensor([0.7725], grad_fn=<AddBackward0>)\n",
      "0.7724534869194031\n",
      "Euclid dist: tensor([40.9535], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.1159], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([40.0757], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.3472], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([4.1726], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([17.4104], grad_fn=<AddBackward0>)\n",
      "tensor([17.4104], grad_fn=<AddBackward0>)\n",
      "17.410362243652344\n",
      "Euclid dist: tensor([1.1758], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1.3826], grad_fn=<AddBackward0>)\n",
      "tensor([1.3826], grad_fn=<AddBackward0>)\n",
      "1.382603406906128\n",
      "Euclid dist: tensor([36.6341], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([4.4657], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.9554], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([3.8236], grad_fn=<AddBackward0>)\n",
      "tensor([3.8236], grad_fn=<AddBackward0>)\n",
      "3.823641300201416\n",
      "Euclid dist: tensor([4.0870], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([37.5730], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1411.7288], grad_fn=<AddBackward0>)\n",
      "tensor([1411.7288], grad_fn=<AddBackward0>)\n",
      "1411.728759765625\n",
      "Euclid dist: tensor([35.5747], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([6.8887], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([47.4541], grad_fn=<AddBackward0>)\n",
      "tensor([47.4541], grad_fn=<AddBackward0>)\n",
      "47.4541015625\n",
      "Euclid dist: tensor([4.6949], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([22.0424], grad_fn=<AddBackward0>)\n",
      "tensor([22.0424], grad_fn=<AddBackward0>)\n",
      "22.042377471923828\n",
      "Euclid dist: tensor([6.2738], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([39.3608], grad_fn=<AddBackward0>)\n",
      "tensor([39.3608], grad_fn=<AddBackward0>)\n",
      "39.360809326171875\n",
      "Euclid dist: tensor([4.9493], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.5052], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([2.2657], grad_fn=<AddBackward0>)\n",
      "tensor([2.2657], grad_fn=<AddBackward0>)\n",
      "2.2656891345977783\n",
      "Euclid dist: tensor([40.3343], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1626.8571], grad_fn=<AddBackward0>)\n",
      "tensor([1626.8571], grad_fn=<AddBackward0>)\n",
      "1626.8570556640625\n",
      "Euclid dist: tensor([3.8395], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([9.6150], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([92.4476], grad_fn=<AddBackward0>)\n",
      "tensor([92.4476], grad_fn=<AddBackward0>)\n",
      "92.44759368896484\n",
      "Euclid dist: tensor([0.1479], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.7261], grad_fn=<AddBackward0>)\n",
      "tensor([0.7261], grad_fn=<AddBackward0>)\n",
      "0.7261267304420471\n",
      "Euclid dist: tensor([1.8953], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([6.3784], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([35.8368], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1284.2727], grad_fn=<AddBackward0>)\n",
      "tensor([1284.2727], grad_fn=<AddBackward0>)\n",
      "1284.272705078125\n",
      "Euclid dist: tensor([3.1646], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([10.0149], grad_fn=<AddBackward0>)\n",
      "tensor([10.0149], grad_fn=<AddBackward0>)\n",
      "10.014883041381836\n",
      "Euclid dist: tensor([9.6723], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([2.6866], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([0.6223], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.3873], grad_fn=<AddBackward0>)\n",
      "tensor([0.3873], grad_fn=<AddBackward0>)\n",
      "0.38726621866226196\n",
      "Euclid dist: tensor([1.9729], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([7.0274], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([49.3847], grad_fn=<AddBackward0>)\n",
      "tensor([49.3847], grad_fn=<AddBackward0>)\n",
      "49.38465881347656\n",
      "Euclid dist: tensor([1.2022], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1.4453], grad_fn=<AddBackward0>)\n",
      "tensor([1.4453], grad_fn=<AddBackward0>)\n",
      "1.4453237056732178\n",
      "Euclid dist: tensor([5.4820], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([30.0522], grad_fn=<AddBackward0>)\n",
      "tensor([30.0522], grad_fn=<AddBackward0>)\n",
      "30.052217483520508\n",
      "Euclid dist: tensor([19.2933], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([372.2298], grad_fn=<AddBackward0>)\n",
      "tensor([372.2298], grad_fn=<AddBackward0>)\n",
      "372.2297668457031\n",
      "Euclid dist: tensor([37.2733], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1389.2983], grad_fn=<AddBackward0>)\n",
      "tensor([1389.2983], grad_fn=<AddBackward0>)\n",
      "1389.29833984375\n",
      "Euclid dist: tensor([5.1196], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([26.2105], grad_fn=<AddBackward0>)\n",
      "tensor([26.2105], grad_fn=<AddBackward0>)\n",
      "26.21053695678711\n",
      "Euclid dist: tensor([39.9146], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1593.1744], grad_fn=<AddBackward0>)\n",
      "tensor([1593.1744], grad_fn=<AddBackward0>)\n",
      "1593.1744384765625\n",
      "Euclid dist: tensor([3.3017], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([25.7144], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([661.2329], grad_fn=<AddBackward0>)\n",
      "tensor([661.2329], grad_fn=<AddBackward0>)\n",
      "661.23291015625\n",
      "Euclid dist: tensor([38.3289], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1469.1030], grad_fn=<AddBackward0>)\n",
      "tensor([1469.1030], grad_fn=<AddBackward0>)\n",
      "1469.10302734375\n",
      "Euclid dist: tensor([3.1476], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([9.9076], grad_fn=<AddBackward0>)\n",
      "tensor([9.9076], grad_fn=<AddBackward0>)\n",
      "9.907593727111816\n",
      "Euclid dist: tensor([13.8609], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([192.1236], grad_fn=<AddBackward0>)\n",
      "tensor([192.1236], grad_fn=<AddBackward0>)\n",
      "192.12356567382812\n",
      "Euclid dist: tensor([37.4360], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([33.0038], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([24.3918], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([0.6375], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.1314], grad_fn=<AddBackward0>)\n",
      "tensor([0.1314], grad_fn=<AddBackward0>)\n",
      "0.13143105804920197\n",
      "Euclid dist: tensor([0.6261], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.3919], grad_fn=<AddBackward0>)\n",
      "tensor([0.3919], grad_fn=<AddBackward0>)\n",
      "0.39193958044052124\n",
      "Euclid dist: tensor([0.4632], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.2146], grad_fn=<AddBackward0>)\n",
      "tensor([0.2146], grad_fn=<AddBackward0>)\n",
      "0.21458089351654053\n",
      "Euclid dist: tensor([34.6071], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1197.6512], grad_fn=<AddBackward0>)\n",
      "tensor([1197.6512], grad_fn=<AddBackward0>)\n",
      "1197.6512451171875\n",
      "Euclid dist: tensor([25.5911], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([654.9049], grad_fn=<AddBackward0>)\n",
      "tensor([654.9049], grad_fn=<AddBackward0>)\n",
      "654.9049072265625\n",
      "Euclid dist: tensor([1.1566], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.1486], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1.3193], grad_fn=<AddBackward0>)\n",
      "tensor([1.3193], grad_fn=<AddBackward0>)\n",
      "1.3193345069885254\n",
      "Euclid dist: tensor([11.8678], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([1.2839], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1.6483], grad_fn=<AddBackward0>)\n",
      "tensor([1.6483], grad_fn=<AddBackward0>)\n",
      "1.6483484506607056\n",
      "Euclid dist: tensor([6.2806], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([32.1417], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1033.0872], grad_fn=<AddBackward0>)\n",
      "tensor([1033.0872], grad_fn=<AddBackward0>)\n",
      "1033.087158203125\n",
      "Euclid dist: tensor([7.3402], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([53.8791], grad_fn=<AddBackward0>)\n",
      "tensor([53.8791], grad_fn=<AddBackward0>)\n",
      "53.879058837890625\n",
      "Euclid dist: tensor([38.2505], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1463.0990], grad_fn=<AddBackward0>)\n",
      "tensor([1463.0990], grad_fn=<AddBackward0>)\n",
      "1463.0989990234375\n",
      "Euclid dist: tensor([0.8193], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.6712], grad_fn=<AddBackward0>)\n",
      "tensor([0.6712], grad_fn=<AddBackward0>)\n",
      "0.6712207198143005\n",
      "Euclid dist: tensor([32.9015], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([35.0898], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1231.2913], grad_fn=<AddBackward0>)\n",
      "tensor([1231.2913], grad_fn=<AddBackward0>)\n",
      "1231.291259765625\n",
      "Euclid dist: tensor([0.9381], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.8801], grad_fn=<AddBackward0>)\n",
      "tensor([0.8801], grad_fn=<AddBackward0>)\n",
      "0.8801231384277344\n",
      "Euclid dist: tensor([3.5251], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([12.4262], grad_fn=<AddBackward0>)\n",
      "tensor([12.4262], grad_fn=<AddBackward0>)\n",
      "12.426207542419434\n",
      "Euclid dist: tensor([2.6632], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([11.7669], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([138.4597], grad_fn=<AddBackward0>)\n",
      "tensor([138.4597], grad_fn=<AddBackward0>)\n",
      "138.45968627929688\n",
      "Euclid dist: tensor([1.3827], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([1.9118], grad_fn=<AddBackward0>)\n",
      "tensor([1.9118], grad_fn=<AddBackward0>)\n",
      "1.9117616415023804\n",
      "Euclid dist: tensor([25.1949], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n",
      "Euclid dist: tensor([0.7368], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([1])\n",
      "Loss: tensor([0.5429], grad_fn=<AddBackward0>)\n",
      "tensor([0.5429], grad_fn=<AddBackward0>)\n",
      "0.5428943634033203\n",
      "Euclid dist: tensor([33.2796], grad_fn=<SqrtBackward0>)\n",
      "Label: tensor([0])\n",
      "Loss: tensor([0.], grad_fn=<AddBackward0>)\n",
      "tensor([0.], grad_fn=<AddBackward0>)\n",
      "0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [13], line 86\u001b[0m\n\u001b[0;32m     82\u001b[0m test_labels \u001b[39m=\u001b[39m []\n\u001b[0;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> 86\u001b[0m     train_loss \u001b[39m=\u001b[39m train(epoch)\n\u001b[0;32m     87\u001b[0m     \u001b[39mprint\u001b[39m(train_loss)\n\u001b[0;32m     88\u001b[0m     \u001b[39mprint\u001b[39m()\n",
      "Cell \u001b[1;32mIn [13], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mfloat\u001b[39m(loss_contrastive))\n\u001b[0;32m     23\u001b[0m     loss_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(loss_contrastive)\n\u001b[1;32m---> 24\u001b[0m     optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[0;32m     26\u001b[0m \u001b[39mreturn\u001b[39;00m loss_all \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_dataset)\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\optim\\optimizer.py:109\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    107\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m--> 109\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\optim\\adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    153\u001b[0m                 max_exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    155\u001b[0m             state_steps\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mstep\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m--> 157\u001b[0m     adam(params_with_grad,\n\u001b[0;32m    158\u001b[0m          grads,\n\u001b[0;32m    159\u001b[0m          exp_avgs,\n\u001b[0;32m    160\u001b[0m          exp_avg_sqs,\n\u001b[0;32m    161\u001b[0m          max_exp_avg_sqs,\n\u001b[0;32m    162\u001b[0m          state_steps,\n\u001b[0;32m    163\u001b[0m          amsgrad\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mamsgrad\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    164\u001b[0m          beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    165\u001b[0m          beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    166\u001b[0m          lr\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mlr\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    167\u001b[0m          weight_decay\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mweight_decay\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    168\u001b[0m          eps\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39meps\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    169\u001b[0m          maximize\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mmaximize\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    170\u001b[0m          foreach\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mforeach\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    171\u001b[0m          capturable\u001b[39m=\u001b[39;49mgroup[\u001b[39m'\u001b[39;49m\u001b[39mcapturable\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m    173\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\optim\\adam.py:213\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     func \u001b[39m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 213\u001b[0m func(params,\n\u001b[0;32m    214\u001b[0m      grads,\n\u001b[0;32m    215\u001b[0m      exp_avgs,\n\u001b[0;32m    216\u001b[0m      exp_avg_sqs,\n\u001b[0;32m    217\u001b[0m      max_exp_avg_sqs,\n\u001b[0;32m    218\u001b[0m      state_steps,\n\u001b[0;32m    219\u001b[0m      amsgrad\u001b[39m=\u001b[39;49mamsgrad,\n\u001b[0;32m    220\u001b[0m      beta1\u001b[39m=\u001b[39;49mbeta1,\n\u001b[0;32m    221\u001b[0m      beta2\u001b[39m=\u001b[39;49mbeta2,\n\u001b[0;32m    222\u001b[0m      lr\u001b[39m=\u001b[39;49mlr,\n\u001b[0;32m    223\u001b[0m      weight_decay\u001b[39m=\u001b[39;49mweight_decay,\n\u001b[0;32m    224\u001b[0m      eps\u001b[39m=\u001b[39;49meps,\n\u001b[0;32m    225\u001b[0m      maximize\u001b[39m=\u001b[39;49mmaximize,\n\u001b[0;32m    226\u001b[0m      capturable\u001b[39m=\u001b[39;49mcapturable)\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\optim\\adam.py:265\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[39m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[0;32m    264\u001b[0m exp_avg\u001b[39m.\u001b[39mmul_(beta1)\u001b[39m.\u001b[39madd_(grad, alpha\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta1)\n\u001b[1;32m--> 265\u001b[0m exp_avg_sq\u001b[39m.\u001b[39;49mmul_(beta2)\u001b[39m.\u001b[39maddcmul_(grad, grad\u001b[39m.\u001b[39mconj(), value\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m beta2)\n\u001b[0;32m    267\u001b[0m \u001b[39mif\u001b[39;00m capturable:\n\u001b[0;32m    268\u001b[0m     step \u001b[39m=\u001b[39m step_t\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = DiffPool(num_nodes = 100).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "num_epochs = 30\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1, _, _ = model(data.x1, data.adj1, batch = None)\n",
    "        output2, _, _ = model(data.x2, data.adj2, batch = None)\n",
    "        \n",
    "        #Contrastive Loss\n",
    "        loss_contrastive = criterion(output1,output2,data.y,epoch)\n",
    "        loss_contrastive.backward()\n",
    "        print(loss_contrastive)\n",
    "        print(float(loss_contrastive))\n",
    "        loss_all += float(loss_contrastive)\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader,epochs):\n",
    "    model.eval()\n",
    "\n",
    "    distances_lab1 = []\n",
    "    distances_lab0 = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    \n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output1, _, _ = model(data.x1, data.adj1, batch=None)\n",
    "        output2, _, _ = model(data.x2, data.adj2, batch=None)\n",
    "\n",
    "        test_loss_contrastive = criterion(output1, output2, data.y,epochs)\n",
    "        #diff = output1 -output2\n",
    "        #print(diff)\n",
    "        #pow = torch.pow(diff, 2)\n",
    "        #print(pow)\n",
    "        #dist_sq = torch.sum(pow, 1)\n",
    "        #print(dist_sq) # sum of squared distance = 0.5 = 9\n",
    "        #euclidean_distance = torch.sqrt(dist_sq)\n",
    "        #print(dist) # euclidean distance = 0.7 = 3\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        label = data.y\n",
    "\n",
    "        if int(label) == 1: \n",
    "            distances_lab1.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "        else:\n",
    "            distances_lab0.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "\n",
    "    return  distances_lab0, distances_lab1, losses, labels\n",
    "\n",
    "\n",
    "\n",
    "train_distances_lab0 = []\n",
    "train_distances_lab1 = []\n",
    "train_losses = []\n",
    "train_labels = []\n",
    "\n",
    "validation_distances_lab0 = []\n",
    "validation_distances_lab1 = []\n",
    "validation_losses = []\n",
    "validation_labels = []\n",
    "\n",
    "test_distances_lab0 = []\n",
    "test_distances_lab1 = []\n",
    "test_losses = []\n",
    "test_labels = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "    print(train_loss)\n",
    "    print()\n",
    "\n",
    "    train_results = test(train_loader,epoch)\n",
    "    train_distances_lab0.append(train_results[0])\n",
    "    train_distances_lab1.append(train_results[1])\n",
    "    train_losses.append(train_results[2])\n",
    "    train_labels.append(train_results[3])\n",
    "    print(train_results[2])\n",
    "    print(len(train_results[2]))\n",
    "    print(sum(train_results[2]))\n",
    "    print(len(train_dataset))\n",
    "    print(sum(train_results[2])/len(train_dataset))\n",
    "\n",
    "\n",
    "    validation_results = test(val_loader,epoch)\n",
    "    validation_distances_lab0.append(validation_results[0])\n",
    "    validation_distances_lab1.append(validation_results[1])\n",
    "    validation_losses.append(validation_results[2])\n",
    "    validation_labels.append(validation_results[3])\n",
    "\n",
    "    test_results = test(test_loader,epoch)\n",
    "    test_distances_lab0.append(test_results[0])\n",
    "    test_distances_lab1.append(test_results[1])\n",
    "    test_losses.append(test_results[2])\n",
    "    test_labels.append(test_results[3])\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, Train Loss Weird: {sum(train_losses[epoch-1])/len(train_dataset)}')\n",
    "    #Train Acc: {train_acc:.3f}, f'Val Acc: {val_acc:.3f}, Test Acc: {test_acc:.3f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_euclid_distances(distances_lab0, distances_lab1):\n",
    "\n",
    "    w = 0.8    # bar width\n",
    "    x = [1, 2] # x-coordinates of your bars\n",
    "    colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "\n",
    "    # Epoch 0\n",
    "    y = [distances_lab0, distances_lab1]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x,\n",
    "        height=[np.mean(yi) for yi in y],\n",
    "        yerr=[np.std(yi) for yi in y],    # error bars\n",
    "        capsize=12, # error bar cap width in points\n",
    "        width=w,    # bar width\n",
    "        tick_label=[\"Label 0\", \"Label 1\"],\n",
    "        color=(0,0,0,0),  # face color transparent\n",
    "        edgecolor=colors,\n",
    "        )\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # distribute scatter randomly across whole width of bar\n",
    "        ax.scatter(x[i] + np.random.random(len(y[i])) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "    plt.ylabel = 'Euclidean Distance'\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_euclid_distances(train_distances_lab0[2], train_distances_lab1[2])\n",
    "compare_euclid_distances(train_distances_lab0[6], train_distances_lab1[6])\n",
    "\n",
    "compare_euclid_distances(train_distances_lab0[8], train_distances_lab1[8])\n",
    "compare_euclid_distances(train_distances_lab0[24], train_distances_lab1[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{model_name}_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "tr_results = np.asarray(train_results)\n",
    "tr_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "w = 0.8    # bar width\n",
    "x = [1, 2] # x-coordinates of your bars\n",
    "colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "y = [np.random.random(30) * 2 + 5,       # data series\n",
    "    np.random.random(10) * 3 + 8]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x,\n",
    "       height=[np.mean(yi) for yi in y],\n",
    "       yerr=[np.std(yi) for yi in y],    # error bars\n",
    "       capsize=12, # error bar cap width in points\n",
    "       width=w,    # bar width\n",
    "       tick_label=[\"control\", \"test\"],\n",
    "       color=(0,0,0,0),  # face color transparent\n",
    "       edgecolor=colors,\n",
    "       #ecolor=colors,    # error bar colors; setting this raises an error for whatever reason.\n",
    "       )\n",
    "\n",
    "for i in range(len(x)):\n",
    "    # distribute scatter randomly across whole width of bar\n",
    "    ax.scatter(x[i] + np.random.random(y[i].size) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(f'{model_name}_training_loss.npy', tr_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_training_accuracy.npy', tr_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_validation_loss.npy', v_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_validation_accuracy.npy', v_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_test_loss.npy', tst_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_test_accuracy.npy', tst_acc, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_loss = np.load(f'{model_name}_training_loss.npy', allow_pickle=True)\n",
    "training_accuracy = np.load(f'{model_name}_training_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "validation_loss = np.load(f'{model_name}_validation_loss.npy', allow_pickle=True)\n",
    "validation_accuracy = np.load(f'{model_name}_validation_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "test_loss = np.load(f'{model_name}_test_loss.npy', allow_pickle=True)\n",
    "test_accuracy = np.load(f'{model_name}_test_accuracy.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_loss, label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Training and Validation Loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, label='Validation Accuracy')\n",
    "plt.plot(epochs, test_accuracy, label='Test Accuracy')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    "plt.ylim(0,1)\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Accuracy vs. Epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fraction = [0,0]\n",
    "\n",
    "train_fraction = [0,0]\n",
    "val_fraction = [0,0]\n",
    "test_fraction = [0,0]\n",
    "\n",
    "for grph in train_dataset: \n",
    "    if grph.y == 1: \n",
    "        train_fraction[1] +=1\n",
    "        dataset_fraction[1] +=1 \n",
    "    else: \n",
    "        train_fraction[0] +=1\n",
    "        dataset_fraction[0] +=1 \n",
    "\n",
    "for grph in val_dataset: \n",
    "    if grph.y == 1:\n",
    "         val_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1  \n",
    "    else:\n",
    "         val_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "for grph in test_dataset: \n",
    "    if grph.y == 1:\n",
    "         test_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1 \n",
    "    else:\n",
    "         test_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "print(f'Overall dataset percentage of label 1 = {dataset_fraction[1]/len(dataset)})')\n",
    "print(f'Training dataset percentage of label 1 = {train_fraction} = {train_fraction[1]/len(train_dataset)}')\n",
    "print(f'Validation dataset percentage of label 1 = {val_fraction} = {val_fraction[1]/len(val_dataset)}')\n",
    "print(f'Test dataset percentage of label 1 = {test_fraction} = {test_fraction[1]/len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, pos0, adj0 = torch.load(f'{model_name}_img0_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN\n",
    "print(x0[0].shape)\n",
    "x0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos0[0].shape)\n",
    "pos0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj0[0].shape)\n",
    "adj0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj0[0])\n",
    "visualize_points(pos0[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph After 1st Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_emb, x1_pool, pos1, adj1, s1= torch.load(f'{model_name}_img1_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj0 @ x_0 @ w_gnn_emb)\n",
    "print(x1_emb[0].shape)\n",
    "x1_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj_0 @ x_0 @ w_gnn_pool\n",
    "print(s1[0].shape)\n",
    "s1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s).t() @ pos_in)\n",
    "print(pos1[0].shape)\n",
    "pos1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s).t() @ x_in)\n",
    "print(x1_pool[0].shape)\n",
    "x1_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix = softmax(adj_out = softmax(s.t()) @ adj_in @ softmax(s))\n",
    "print(adj1[0].shape)\n",
    "adj1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj1[0])\n",
    "visualize_points(pos1[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 2nd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_emb, x2_pool, pos2, adj2, s2 = torch.load(f'{model_name}_img2_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj1 @ x1_pool @ w_gnn_emb)\n",
    "print(x2_emb[0].shape)\n",
    "x2_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj1 @ x1_pool @ w_gnn_pool), dim=1\n",
    "print(s2[0].shape)\n",
    "s2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos2[0].shape)\n",
    "pos2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s2).t() @ x2_emb)\n",
    "print(x2_pool[0].shape)\n",
    "x2_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s).T @ adj @ softmax(s)\n",
    "print(adj2[0].shape)\n",
    "adj2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj2[0])\n",
    "visualize_points(pos2[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 3rd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_emb, x3_pool, pos3, adj3, s3 = torch.load(f'{model_name}_img3_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj_0 @ x_0 @ w_gnn_emb)\n",
    "print(x3_emb[0].shape)\n",
    "x3_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: torch.softmax(adj_0 @ x_0 @ w_gnn_pool), dim=1)\n",
    "print(s3[0].shape)\n",
    "s3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos3[0].shape)\n",
    "pos3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s.t()) @ x_0)\n",
    "print(x3_pool[0].shape)\n",
    "x3_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s.t()) @ adj @ softmax(s)\n",
    "print(adj3[0].shape)\n",
    "adj3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj3[0])\n",
    "visualize_points(pos3[0].cpu(), edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f3baeb0c3f97feb3023477fbaa09b9f4da769e45e64d8febc6957bb84d33ff77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
