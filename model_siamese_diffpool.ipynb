{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'model_siamese_diffpool1'\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader #To make use of this data loader, all graph attributes in the dataset need to have the same shape. In particular, this data loader should only be used when working with dense adjacency matrices.\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_1 = 'C:/Users/david/pyproj/pyg/adl/patch_label_1'\n",
    "data_dir_0 = 'C:/Users/david/pyproj/pyg/adl/patch_label_0'\n",
    "#data_dir_1 = 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces-DESKTOP-AQ00763/patch_label_1'\n",
    "#data_dir_0 = 'C:/Users/thoma/OneDrive - ZHAW/ProteinSurfaces-DESKTOP-AQ00763/patch_label_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from c_PatchDataset import PatchDataset\n",
    "dataset = PatchDataset(data_dir_label_0 = data_dir_0,  data_dir_label_1=data_dir_1,  neg_pos_ratio=1)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(572):\n",
      "====================\n",
      "Number of graphs pairs: 572\n",
      "\n",
      "PairData(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=0)\n",
      "=============================================================\n",
      "Number of nodes in each: None\n",
      "Number of node features: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch_geometric\\data\\storage.py:271: UserWarning: Unable to accurately infer 'num_nodes' from the attribute set '{'x1', 'y', 'adj1', 'x2', 'adj2'}'. Please explicitly set 'num_nodes' as an attribute of 'data' to suppress this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs pairs: {len(dataset)}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes in each: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2600,  0.0000, -0.9656],\n",
       "        [ 0.9159,  0.0000, -0.4015],\n",
       "        [ 0.3376,  0.0000, -0.9413],\n",
       "        [ 0.1168,  0.0000,  0.9932],\n",
       "        [-0.3257, -0.0268,  0.9451],\n",
       "        [ 0.1749,  0.0000, -0.9846],\n",
       "        [ 0.2071,  0.0000, -0.9783],\n",
       "        [ 0.4701,  0.3119,  0.8257],\n",
       "        [-0.6824, -0.1672, -0.7116],\n",
       "        [-0.2098, -0.0515,  0.9764],\n",
       "        [ 0.4307,  0.0000,  0.9025],\n",
       "        [-0.5590, -0.0224, -0.8289],\n",
       "        [ 0.1548,  0.0000,  0.9879],\n",
       "        [ 0.0597,  0.0000, -0.9982],\n",
       "        [-0.2739,  0.0000, -0.9618],\n",
       "        [ 0.0963,  0.0000, -0.9954],\n",
       "        [ 0.2348,  0.0000, -0.9720],\n",
       "        [-0.0618,  0.0000, -0.9981],\n",
       "        [ 0.5539,  0.0000, -0.8326],\n",
       "        [ 0.1218,  0.0000, -0.9925],\n",
       "        [-0.3823, -0.0201, -0.9238],\n",
       "        [ 0.1407,  0.0000, -0.9901],\n",
       "        [ 0.1254,  0.0000, -0.9921],\n",
       "        [ 0.1351,  0.0000, -0.9908],\n",
       "        [ 0.0394,  0.0000, -0.9992],\n",
       "        [ 0.3158,  0.0041, -0.9488],\n",
       "        [ 0.1991,  0.0000, -0.9800],\n",
       "        [-0.0481, -0.0777,  0.9958],\n",
       "        [-0.3326, -0.0825, -0.9395],\n",
       "        [ 0.1081,  0.0000, -0.9941],\n",
       "        [-0.0059,  0.0000, -1.0000],\n",
       "        [ 0.2527,  0.0000, -0.9676],\n",
       "        [ 0.0668,  0.0000, -0.9978],\n",
       "        [-0.1452,  0.0000, -0.9894],\n",
       "        [ 0.1030,  0.0244, -0.9944],\n",
       "        [ 0.1722,  0.0000,  0.9851],\n",
       "        [-0.1882, -0.0032,  0.9821],\n",
       "        [-0.1503, -0.1877,  0.9707],\n",
       "        [-0.0342,  0.0000,  0.9994],\n",
       "        [ 0.0740,  0.0000, -0.9973],\n",
       "        [-0.4274, -0.0051, -0.9040],\n",
       "        [ 0.3118,  0.0000,  0.9502],\n",
       "        [ 0.0312,  0.0000, -0.9995],\n",
       "        [ 0.3033,  0.0000,  0.9529],\n",
       "        [-0.9355, -0.1490, -0.3202],\n",
       "        [-0.1872,  0.0000, -0.9823],\n",
       "        [-0.0248, -0.0075,  0.9997],\n",
       "        [ 0.5621,  0.1044, -0.8204],\n",
       "        [-0.9312,  0.0000, -0.3644],\n",
       "        [ 0.3376,  0.0000, -0.9413],\n",
       "        [ 0.2268,  0.0000, -0.9739],\n",
       "        [ 0.2263,  0.1189, -0.9668],\n",
       "        [ 0.2580,  0.0000,  0.9662],\n",
       "        [ 0.1119,  0.0000,  0.9937],\n",
       "        [ 0.0079,  0.0000, -1.0000],\n",
       "        [ 0.1737,  0.0000, -0.9848],\n",
       "        [ 0.1483,  0.0000, -0.9889],\n",
       "        [ 0.2514,  0.1394, -0.9578],\n",
       "        [-0.5900, -0.5373, -0.6027],\n",
       "        [-0.1292,  0.0000, -0.9916],\n",
       "        [ 0.0812,  0.0000, -0.9967],\n",
       "        [ 0.0623,  0.0000,  0.9981],\n",
       "        [ 0.1599,  0.0000, -0.9871],\n",
       "        [-0.4625,  0.0000, -0.8866],\n",
       "        [ 0.0146,  0.0000,  0.9999],\n",
       "        [ 0.0481,  0.0000, -0.9988],\n",
       "        [ 0.4272,  0.0000, -0.9042],\n",
       "        [-0.4915, -0.5449, -0.6793],\n",
       "        [ 0.1745,  0.0000, -0.9847],\n",
       "        [-0.4021,  0.0000, -0.9156],\n",
       "        [ 0.6278,  0.0000,  0.7784],\n",
       "        [-0.2028, -0.0447,  0.9782],\n",
       "        [-0.1340,  0.0000, -0.9910],\n",
       "        [ 0.3392,  0.0000, -0.9407],\n",
       "        [-0.2140,  0.0000, -0.9768],\n",
       "        [ 0.3111,  0.0000, -0.9504],\n",
       "        [ 0.7741,  0.3127, -0.5504],\n",
       "        [-0.2075,  0.0000,  0.9782],\n",
       "        [ 0.3823,  0.0000, -0.9240],\n",
       "        [ 0.0085,  0.0000, -1.0000],\n",
       "        [ 0.1350,  0.0000, -0.9908],\n",
       "        [ 0.5230,  0.0000, -0.8523],\n",
       "        [ 0.1392,  0.0000, -0.9903],\n",
       "        [ 0.0961,  0.0000, -0.9954],\n",
       "        [ 0.2184,  0.0000, -0.9759],\n",
       "        [ 0.0432,  0.0000, -0.9991],\n",
       "        [-0.1854, -0.0822,  0.9792],\n",
       "        [ 0.0131,  0.0000, -0.9999],\n",
       "        [-0.0826, -0.0131,  0.9965],\n",
       "        [ 0.0920,  0.0000, -0.9958],\n",
       "        [ 0.3306,  0.0000, -0.9438],\n",
       "        [ 0.2377,  0.0000, -0.9713],\n",
       "        [ 0.1860,  0.0000,  0.9825],\n",
       "        [ 0.8635,  0.4992,  0.0721],\n",
       "        [ 0.9543,  0.0000, -0.2988],\n",
       "        [ 0.1151,  0.0000,  0.9934],\n",
       "        [ 0.0585,  0.0000, -0.9983],\n",
       "        [-0.0397,  0.0000,  0.9992],\n",
       "        [ 0.1693,  0.0000, -0.9856],\n",
       "        [ 0.8752,  0.0000,  0.4837]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work we do not have pos\n",
    "#visualize_points(data.pos, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs pairs: 382\n",
      "Number of validation graphs: 95\n",
      "Number of test graphs: 95\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "n_train = math.ceil((4/6) * len(dataset))\n",
    "n_val = math.ceil((len(dataset) - n_train)/2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f'Number of training graphs pairs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size= batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairDataBatch(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=[1])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = next(iter(train_loader))\n",
    "databatch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_nodes, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        # Each instance of this GNN will have 3 convolutional layers and three batch norm layers        \n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        self.bns1 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        self.bns2 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        self.bns3 = torch.nn.BatchNorm1d(in_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        \n",
    "        #Step 1\n",
    "        x = self.conv1(x, adj, mask)\n",
    "        x = self.bns1(x)\n",
    "        \n",
    "        #Step 2\n",
    "        x = self.conv2(x, adj, mask)\n",
    "        x = self.bns2(x)\n",
    "\n",
    "        #Step 3\n",
    "        x = self.conv3(x, adj, mask)\n",
    "        if x.shape[2] != 1: \n",
    "            x = self.bns3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        in_nodes = num_nodes\n",
    "        out_nodes = 25 # Number of clusters / nodes in the next layer\n",
    "        self.gnn1_pool = GNN(in_nodes, dataset.num_features, 16, out_nodes) # PoolGNN --> Cluster Assignment Matrix to reduce to num_nodes\n",
    "        self.gnn1_embed = GNN(in_nodes, dataset.num_features, 8, 8) # EmbGNN --> Convolutions to create new node embedding\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 10\n",
    "        self.gnn2_pool = GNN(in_nodes, 8, 8, out_nodes)\n",
    "        self.gnn2_embed = GNN(in_nodes, 8, 12, 16, lin=False)\n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 1\n",
    "        self.gnn3_pool = GNN(in_nodes, 16, 16, out_nodes)\n",
    "        self.gnn3_embed = GNN(in_nodes, 16, 16, 32, lin=False)\n",
    "\n",
    "        # Final Classifier\n",
    "        self.lin1 = torch.nn.Linear(32, 64) \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask = None, batch = None):\n",
    "        \n",
    "        #Hierarchical Step #1\n",
    "        x1 = self.gnn1_embed(x, adj, mask) # node feature embedding\n",
    "        s = self.gnn1_pool(x, adj, mask) # cluster assignment matrix\n",
    "\n",
    "        x, adj, l1, e1 = dense_diff_pool(x1, adj, s, mask) # does the necessary matrix multiplications\n",
    "        adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        x2 = self.gnn2_embed(x, adj)\n",
    "        s = self.gnn2_pool(x, adj)\n",
    "\n",
    "        x, adj, l2, e2 = dense_diff_pool(x2, adj, s)\n",
    "        adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "      \n",
    "        # Hierarchical Step #3\n",
    "        x3 = self.gnn3_embed(x, adj)\n",
    "        s = self.gnn3_pool(x, adj)\n",
    "        \n",
    "        x, adj, l3, e3 = dense_diff_pool(x3, adj, s)\n",
    "        adj = torch.softmax(adj, dim=-1)     \n",
    "        \n",
    "\n",
    "        x = x.mean(dim=1) \n",
    "\n",
    "        x = F.relu(self.lin1(x))\n",
    "\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contrastive Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt of a contrastive loss function\n",
    "#   pairs with label 1 --> should get small euclid dist = small loss\n",
    "#   pairs with label 0 --> should get large euclid dist = small loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "\n",
    "        diff = x0 - x1\n",
    "        pow = torch.pow(diff, 2)\n",
    "        dist_sq = torch.sum(pow, 1)\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "\n",
    "        mdist = self.margin- dist\n",
    "\n",
    "        dist_marg = torch.clamp(mdist, min=0.0) # only distances <margin will be still positive here\n",
    "        loss =  y * torch.pow(dist, 2) + (1-y) * torch.pow(dist_marg,2)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Before Tr:  Train Loss: 233.268, Val Loss: 170.849, Test Loss: 223.307, ValDistanceRatio (L0/L1): 1.375, TrainDistanceRatio (L0/L1): 1.395\n",
      "Epoch: 001, Train Loss: 409.001, Val Loss: 328.655, Test Loss: 396.247, ValDistanceRatio (L0/L1): 1.182, TrainDistanceRatio (L0/L1): 1.210\n",
      "Epoch: 002, Train Loss: 268.490, Val Loss: 72.423, Test Loss: 150.844, ValDistanceRatio (L0/L1): 1.884, TrainDistanceRatio (L0/L1): 1.653\n",
      "Epoch: 003, Train Loss: 359.715, Val Loss: 265.609, Test Loss: 312.821, ValDistanceRatio (L0/L1): 1.017, TrainDistanceRatio (L0/L1): 0.981\n",
      "Epoch: 004, Train Loss: 143.339, Val Loss: 76.576, Test Loss: 111.503, ValDistanceRatio (L0/L1): 1.332, TrainDistanceRatio (L0/L1): 1.313\n",
      "Epoch: 005, Train Loss: 170.992, Val Loss: 155.723, Test Loss: 177.429, ValDistanceRatio (L0/L1): 1.035, TrainDistanceRatio (L0/L1): 1.037\n",
      "Epoch: 006, Train Loss: 143.491, Val Loss: 41.588, Test Loss: 81.323, ValDistanceRatio (L0/L1): 1.741, TrainDistanceRatio (L0/L1): 1.569\n",
      "Epoch: 007, Train Loss: 286.620, Val Loss: 183.758, Test Loss: 221.656, ValDistanceRatio (L0/L1): 1.072, TrainDistanceRatio (L0/L1): 1.044\n",
      "Epoch: 008, Train Loss: 1034.945, Val Loss: 937.330, Test Loss: 915.894, ValDistanceRatio (L0/L1): 0.887, TrainDistanceRatio (L0/L1): 0.790\n",
      "Epoch: 009, Train Loss: 269.587, Val Loss: 221.481, Test Loss: 261.281, ValDistanceRatio (L0/L1): 0.990, TrainDistanceRatio (L0/L1): 0.972\n",
      "Epoch: 010, Train Loss: 212.321, Val Loss: 106.160, Test Loss: 148.433, ValDistanceRatio (L0/L1): 1.233, TrainDistanceRatio (L0/L1): 1.207\n",
      "Epoch: 011, Train Loss: 654.790, Val Loss: 696.929, Test Loss: 680.652, ValDistanceRatio (L0/L1): 0.796, TrainDistanceRatio (L0/L1): 0.687\n",
      "Epoch: 012, Train Loss: 713.431, Val Loss: 743.781, Test Loss: 735.260, ValDistanceRatio (L0/L1): 0.775, TrainDistanceRatio (L0/L1): 0.680\n",
      "Epoch: 013, Train Loss: 235.572, Val Loss: 188.412, Test Loss: 207.000, ValDistanceRatio (L0/L1): 0.987, TrainDistanceRatio (L0/L1): 0.961\n",
      "Epoch: 014, Train Loss: 188.626, Val Loss: 42.425, Test Loss: 99.279, ValDistanceRatio (L0/L1): 1.575, TrainDistanceRatio (L0/L1): 1.473\n",
      "Epoch: 015, Train Loss: 249.991, Val Loss: 221.603, Test Loss: 234.189, ValDistanceRatio (L0/L1): 0.995, TrainDistanceRatio (L0/L1): 0.968\n",
      "Epoch: 016, Train Loss: 52.977, Val Loss: 43.185, Test Loss: 51.077, ValDistanceRatio (L0/L1): 1.085, TrainDistanceRatio (L0/L1): 1.103\n",
      "Epoch: 017, Train Loss: 141.687, Val Loss: 38.210, Test Loss: 79.134, ValDistanceRatio (L0/L1): 1.772, TrainDistanceRatio (L0/L1): 1.635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 138\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[39m#===============================================================================================================================================\u001b[39;00m\n\u001b[0;32m    133\u001b[0m \u001b[39m# Training\u001b[39;00m\n\u001b[0;32m    134\u001b[0m \u001b[39m#===============================================================================================================================================\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, num_epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m--> 138\u001b[0m     train_loss \u001b[39m=\u001b[39m train(epoch)\n\u001b[0;32m    141\u001b[0m     \u001b[39m# Test and Validation Data During Training\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[39m#===============================================================================================================================================\u001b[39;00m\n\u001b[0;32m    143\u001b[0m     train_results \u001b[39m=\u001b[39m test(train_loader)\n",
      "Cell \u001b[1;32mIn [12], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39m#Contrastive Loss\u001b[39;00m\n\u001b[0;32m     18\u001b[0m loss_contrastive \u001b[39m=\u001b[39m criterion(output1, output2, data\u001b[39m.\u001b[39my)\n\u001b[1;32m---> 19\u001b[0m loss_contrastive\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     20\u001b[0m loss_all \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m data\u001b[39m.\u001b[39my\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m) \u001b[39m*\u001b[39m loss_contrastive\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     21\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\david\\pyproj\\pyg\\pyg_env\\lib\\site-packages\\torch\\autograd\\__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = DiffPool(num_nodes = 100).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1 = model(data.x1, data.adj1,None)\n",
    "        output2 = model(data.x2, data.adj2, batch=None)\n",
    "        \n",
    "        #Contrastive Loss\n",
    "        loss_contrastive = criterion(output1, output2, data.y)\n",
    "        loss_contrastive.backward()\n",
    "        loss_all += data.y.size(0) * loss_contrastive.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    distances_lab1 = []\n",
    "    distances_lab0 = []\n",
    "    loss_all = 0\n",
    "    \n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output1 = model(data.x1, data.adj1, batch=None)\n",
    "        output2 = model(data.x2, data.adj2, batch=None)\n",
    "\n",
    "        loss = criterion(output1, output2, data.y)\n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        label = data.y\n",
    "\n",
    "        if int(label) == 1: \n",
    "            distances_lab1.append(float(euclidean_distance))\n",
    "            loss_all += loss.item()\n",
    "        else:\n",
    "            distances_lab0.append(float(euclidean_distance))\n",
    "            loss_all += loss.item()\n",
    "\n",
    "    return  distances_lab0, distances_lab1, loss_all/(len(loader))\n",
    "\n",
    "\n",
    "\n",
    "# Initialize data collections\n",
    "#==============================================================================================================================\n",
    "\n",
    "train_distances_lab0 = []\n",
    "train_distances_lab1 = []\n",
    "train_losses = []\n",
    "\n",
    "validation_distances_lab0 = []\n",
    "validation_distances_lab1 = []\n",
    "validation_losses = []\n",
    "\n",
    "test_distances_lab0 = []\n",
    "test_distances_lab1 = []\n",
    "test_losses = []\n",
    "\n",
    "val_dists_0 = []\n",
    "val_dists_1 = []\n",
    "train_dists_0 = []\n",
    "train_dists_1 = []\n",
    "\n",
    "with open(f'{model_name}_printout.txt', 'w') as f:\n",
    "    f.write(f'Training Data ({model_name}):\\n')\n",
    "    f.close()\n",
    "#==============================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "# Test and Validation Data before Training\n",
    "#==============================================================================================================================\n",
    "train_results = test(train_loader)\n",
    "train_distances_lab0.append(train_results[0])\n",
    "train_distances_lab1.append(train_results[1])\n",
    "train_losses.append(train_results[2])\n",
    "\n",
    "validation_results = test(val_loader)\n",
    "validation_distances_lab0.append(validation_results[0])\n",
    "validation_distances_lab1.append(validation_results[1])\n",
    "validation_losses.append(validation_results[2])\n",
    "\n",
    "test_results = test(test_loader)\n",
    "test_distances_lab0.append(test_results[0])\n",
    "test_distances_lab1.append(test_results[1])\n",
    "test_losses.append(test_results[2])\n",
    "\n",
    "min_val_loss = (validation_results[2]+test_results[2])/2\n",
    "#==============================================================================================================================\n",
    "\n",
    "\n",
    "# Compute Euclidean Distances for printout\n",
    "#==============================================================================================================================\n",
    "val_dist_0 = test_results[0] + validation_results[0] # List of all euclidean distances of label 0 members of val and test dataset\n",
    "val_dist_1 = test_results[1] + validation_results[1] # List of all euclidean distances of label 1 members of val and test dataset\n",
    "# Ratio (mean label 0 euclid dist in validation set / mean label 1 euclid dist in validation set)\n",
    "val_dist_ratio = (sum(val_dist_0) / len(val_dist_0)) / (sum(val_dist_1) / len(val_dist_1))\n",
    "best_dist_ratio = val_dist_ratio\n",
    "val_dists_0.append(sum(val_dist_0) / len(val_dist_0))\n",
    "val_dists_1.append(sum(val_dist_1) / len(val_dist_1))\n",
    "\n",
    "train_dist_0 = train_results[0] # List of all euclidean distances of label 0 members of train dataset\n",
    "train_dist_1 = train_results[1] # List of all euclidean distances of label 0 members of train dataset\n",
    "# Ratio of (mean label 0 euclid dist in training set / mean label 1 euclid dist in training set)\n",
    "train_dist_ratio = (sum(train_dist_0) / len(train_dist_0)) / (sum(train_dist_1) / len(train_dist_1))\n",
    "train_dists_0.append(sum(train_dist_0) / len(train_dist_0))\n",
    "train_dists_1.append(sum(train_dist_1) / len(train_dist_1))\n",
    "#===============================================================================================================================\n",
    "\n",
    "printout = (f'Before Tr:  Train Loss: {train_results[2]:.3f}, Val Loss: {validation_results[2]:.3f}, Test Loss: {test_results[2]:.3f}, '\n",
    "            f'ValDistanceRatio (L0/L1): {(val_dist_ratio):.3f}, TrainDistanceRatio (L0/L1): {(train_dist_ratio):.3f}')\n",
    "\n",
    "print(printout)\n",
    "\n",
    "# append to outputs txt\n",
    "with open(f'{model_name}_printout.txt', 'a') as f:\n",
    "        f.write(printout + '\\n')\n",
    "        f.close()\n",
    "\n",
    "\n",
    "\n",
    "#===============================================================================================================================================\n",
    "# Training\n",
    "#===============================================================================================================================================\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "\n",
    "\n",
    "    # Test and Validation Data During Training\n",
    "    #===============================================================================================================================================\n",
    "    train_results = test(train_loader)\n",
    "    train_distances_lab0.append(train_results[0])\n",
    "    train_distances_lab1.append(train_results[1])\n",
    "    train_losses.append(train_results[2])\n",
    "\n",
    "    validation_results = test(val_loader)\n",
    "    validation_distances_lab0.append(validation_results[0])\n",
    "    validation_distances_lab1.append(validation_results[1])\n",
    "    validation_losses.append(validation_results[2])\n",
    "\n",
    "    test_results = test(test_loader)\n",
    "    test_distances_lab0.append(test_results[0])\n",
    "    test_distances_lab1.append(test_results[1])\n",
    "    test_losses.append(test_results[2])\n",
    "\n",
    "    #===============================================================================================================================================\n",
    "\n",
    "\n",
    "    # Compute Euclidean Distances for printout\n",
    "    #==============================================================================================================================\n",
    "    val_dist_0 = test_results[0]+validation_results[0] # List of all euclidean distances of label 0 members of val and test dataset\n",
    "    val_dist_1 = test_results[1]+validation_results[1] # List of all euclidean distances of label 1 members of val and test dataset\n",
    "    # Ratio (mean label 0 euclid dist in validation set / mean label 1 euclid dist in validation set)\n",
    "    val_dist_ratio = (sum(val_dist_0) / len(val_dist_0)) / (sum(val_dist_1) / len(val_dist_1))\n",
    "    val_dists_0.append(sum(val_dist_0) / len(val_dist_0))\n",
    "    val_dists_1.append(sum(val_dist_1) / len(val_dist_1))\n",
    "\n",
    "    train_dist_0 = train_results[0] # List of all euclidean distances of label 0 members of train dataset\n",
    "    train_dist_1 = train_results[1] # List of all euclidean distances of label 0 members of train dataset\n",
    "    # Ratio of (mean label 0 euclid dist in training set / mean label 1 euclid dist in training set)\n",
    "    train_dist_ratio = (sum(train_dist_0) / len(train_dist_0)) / (sum(train_dist_1) / len(train_dist_1))\n",
    "    train_dists_0.append(sum(train_dist_0) / len(train_dist_0))\n",
    "    train_dists_1.append(sum(train_dist_1) / len(train_dist_1))\n",
    "    #===============================================================================================================================\n",
    "    \n",
    "\n",
    "\n",
    "    # Create Printout\n",
    "    #===============================================================================================================================================\n",
    "    #printout = (f'Epoch: {epoch:03d}, Train Loss: {train_results[2]:.3f}, Val Loss: {validation_results[2]:.3f}, Test Loss: {test_results[2]:.3f}, '\n",
    "    #        f'ValDist 0: {val_dist_0:.3f}, ValDist 1: {val_dist_1:.3f}, TrainDist 0: {train_dist_0:.3f}, TrainDist 1: {train_dist_1:.3f}')\n",
    "    printout = (f'Epoch: {epoch:03d}, Train Loss: {train_results[2]:.3f}, Val Loss: {validation_results[2]:.3f}, Test Loss: {test_results[2]:.3f}, '\n",
    "            f'ValDistanceRatio (L0/L1): {(val_dist_ratio):.3f}, TrainDistanceRatio (L0/L1): {(train_dist_ratio):.3f}')\n",
    "    print(printout)\n",
    "\n",
    "    # append to outputs txt\n",
    "    with open(f'{model_name}_printout.txt', 'a') as f:\n",
    "        f.write(printout + '\\n')\n",
    "        f.close()\n",
    "    #===============================================================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "    # Save the model outputs\n",
    "    #===============================================================================================================================================\n",
    "    np.save(f'{model_name}_training_loss.npy', train_losses, allow_pickle=True)\n",
    "    np.save(f'{model_name}_training_distances_lab0.npy', train_distances_lab0, allow_pickle=True)\n",
    "    np.save(f'{model_name}_training_distances_lab1.npy', train_distances_lab1, allow_pickle=True)\n",
    "\n",
    "    np.save(f'{model_name}_validation_loss.npy', validation_losses, allow_pickle=True)\n",
    "    np.save(f'{model_name}_validation_distances_lab0.npy', validation_distances_lab0, allow_pickle=True)\n",
    "    np.save(f'{model_name}_validation_distances_lab1.npy', validation_distances_lab1, allow_pickle=True)\n",
    "\n",
    "    np.save(f'{model_name}_test_loss.npy', test_losses, allow_pickle=True)\n",
    "    np.save(f'{model_name}_test_distances_lab0.npy', test_distances_lab0, allow_pickle=True)\n",
    "    np.save(f'{model_name}_test_distances_lab1.npy', test_distances_lab1, allow_pickle=True)\n",
    "    \n",
    "    last_saved_epoch = epoch\n",
    "    #===============================================================================================================================================\n",
    "\n",
    "\n",
    "    # If the epoch has improved the model, save it\n",
    "    if val_dist_ratio > best_dist_ratio:\n",
    "        new_best = True\n",
    "        best_dist_ratio = val_dist_ratio\n",
    "        best_epoch = epoch\n",
    "    else:\n",
    "        new_best = False\n",
    "        \n",
    "    if new_best:\n",
    "        torch.save(model.state_dict(), f'{model_name}_state_dict_best.pt')\n",
    "        torch.save(model, f'{model_name}_save.pt')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch, last_saved_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_euclid_distances(distances_lab0, distances_lab1, save=None, title=None):\n",
    "\n",
    "    '''Function for plotting the euclidean distances of two datasets generated during a model epoch'''\n",
    "\n",
    "    w = 0.8    # bar width\n",
    "    x = [1, 2] # x-coordinates of your bars\n",
    "    colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "\n",
    "    # Epoch 0\n",
    "    y = [distances_lab0, distances_lab1]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x,\n",
    "        height=[np.mean(yi) for yi in y],\n",
    "        yerr=[np.std(yi) for yi in y],    # error bars\n",
    "        capsize=12, # error bar cap width in points\n",
    "        width=w,    # bar width\n",
    "        tick_label=[\"Label 0\", \"Label 1\"],\n",
    "        color=(0,0,0,0),  # face color transparent\n",
    "        edgecolor=colors,\n",
    "        )\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # distribute scatter randomly across whole width of bar\n",
    "        ax.scatter(x[i] + np.random.random(len(y[i])) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.ylabel('Euclidean Distance')\n",
    "    #plt.ylim(top=1.5, bottom=0)\n",
    "    \n",
    "    if save != None:\n",
    "        plt.savefig(save)\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Euclidean Distances in the Training Set (Before and after Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_euclid_distances(train_distances_lab0[0], train_distances_lab1[0], save = model_name + '_plot_train_dist0.png', title = 'Training Distances Epoch 0')\n",
    "compare_euclid_distances(train_distances_lab0[best_epoch], train_distances_lab1[best_epoch], save = model_name + f'_plot_train_dist_{best_epoch}', title = f'Training Distances Epoch {best_epoch}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Euclidean Distances in the Validation Set (Before and after Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_euclid_distances(validation_distances_lab0[0], validation_distances_lab1[0], save = model_name + '_plot_val_dist0.png', title = 'Validation Distances Epoch 0')\n",
    "compare_euclid_distances(validation_distances_lab0[best_epoch], validation_distances_lab1[best_epoch], save = model_name + f'_plot_val_dist_{best_epoch}', title = f'Validation Distances Epoch {best_epoch}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Training and Validation Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, last_saved_epoch+2)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, validation_losses, label='Validation Loss')\n",
    "plt.plot(epochs, test_losses, label='Test Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epochs')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, last_saved_epoch+1, last_saved_epoch/10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_plot_loss.png')\n",
    "#plt.ylim(top=60, bottom = 0)\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Euclidean Distances (Label0 vs. Label1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, last_saved_epoch+2)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, train_dists_0, label='Training Distances Label 0')\n",
    "plt.plot(epochs, train_dists_1, label='Training Distances Label 1')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training Euclidean Distances')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Euclidean Distance')\n",
    "\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, last_saved_epoch+1, last_saved_epoch/10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_plot_train_distances.png')\n",
    "#plt.ylim(top=60, bottom = 0)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5295f743bc4e47f7cb4c7d5e484c5cf6bb52e824ff35c3fecf2642e2b62ae0ab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
