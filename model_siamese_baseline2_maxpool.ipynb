{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Normalize features? \n",
    "## Invert h-bond and charge? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'adl_model_siamese_baseline'\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DenseDataLoader #To make use of this data loader, all graph attributes in the dataset need to have the same shape. In particular, this data loader should only be used when working with dense adjacency matrices.\n",
    "from torch_geometric.nn import DenseGCNConv, dense_diff_pool, global_max_pool\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_dir_1 = 'C:/Users/david/pyproj/pyg/adl/patch_label_1'\n",
    "#data_dir_0 = 'C:/Users/david/pyproj/pyg/adl/patch_label_0'\n",
    "data_dir_1 = 'adl_data_1'\n",
    "data_dir_0 = 'adl_data_0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "572"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from c_PatchDataset import PatchDataset\n",
    "dataset = PatchDataset(data_dir_label_0 = data_dir_0,  data_dir_label_1=data_dir_1,  neg_pos_ratio=1)\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PatchDataset(572):\n",
      "====================\n",
      "Number of graphs pairs: 572\n",
      "\n",
      "PairData(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=0)\n",
      "=============================================================\n",
      "Number of nodes in each: None\n",
      "Number of node features: 0\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('====================')\n",
    "print(f'Number of graphs pairs: {len(dataset)}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "print()\n",
    "print(data)\n",
    "print('=============================================================')\n",
    "\n",
    "# Gather some statistics about the first graph.\n",
    "print(f'Number of nodes in each: {data.num_nodes}')\n",
    "print(f'Number of node features: {data.num_node_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3572, -0.0267, -0.9336],\n",
       "        [ 0.0396,  0.0000, -0.9992],\n",
       "        [-0.0485,  0.0000,  0.9988],\n",
       "        [-0.4344, -0.0069, -0.9007],\n",
       "        [-0.1350, -0.1865, -0.9731],\n",
       "        [-0.1509,  0.0000,  0.9886],\n",
       "        [-0.1171,  0.0000,  0.9931],\n",
       "        [-0.4347, -0.1262, -0.8917],\n",
       "        [-0.0705,  0.0000,  0.9975],\n",
       "        [-0.0645,  0.0000,  0.9979],\n",
       "        [-0.4160, -0.1807, -0.8912],\n",
       "        [-0.3700, -0.2242, -0.9016],\n",
       "        [-0.0617,  0.0000,  0.9981],\n",
       "        [-0.0311,  0.0000, -0.9995],\n",
       "        [-0.1928,  0.0000, -0.9812],\n",
       "        [-0.3160,  0.0000, -0.9487],\n",
       "        [-0.3164,  0.0000, -0.9486],\n",
       "        [ 0.0632,  0.2151, -0.9745],\n",
       "        [-0.0716, -0.0550, -0.9959],\n",
       "        [-0.1753, -0.0257,  0.9842],\n",
       "        [-0.0445,  0.0000,  0.9990],\n",
       "        [ 0.0430,  0.0000,  0.9991],\n",
       "        [-0.0855,  0.0000,  0.9963],\n",
       "        [-0.0633,  0.0000,  0.9980],\n",
       "        [ 0.0041,  0.0000, -1.0000],\n",
       "        [-0.1053,  0.0000,  0.9944],\n",
       "        [-0.1116, -0.0582, -0.9920],\n",
       "        [-0.1183, -0.0318, -0.9925],\n",
       "        [-0.3643, -0.1583, -0.9177],\n",
       "        [-0.1803, -0.1657, -0.9696],\n",
       "        [-0.0795,  0.0000,  0.9968],\n",
       "        [-0.4887,  0.0000, -0.8725],\n",
       "        [-0.0100,  0.0000,  1.0000],\n",
       "        [-0.0653,  0.0000,  0.9979],\n",
       "        [-0.0071,  0.0000,  1.0000],\n",
       "        [-0.0526,  0.0000,  0.9986],\n",
       "        [-0.0814,  0.0000,  0.9967],\n",
       "        [-0.0656,  0.0000,  0.9978],\n",
       "        [ 0.0380,  0.0000, -0.9993],\n",
       "        [-0.0227,  0.0000,  0.9997],\n",
       "        [-0.1706,  0.0000,  0.9853],\n",
       "        [-0.0269,  0.0000,  0.9996],\n",
       "        [-0.0776,  0.0000,  0.9970],\n",
       "        [-0.1378, -0.2694, -0.9531],\n",
       "        [-0.1545,  0.0000,  0.9880],\n",
       "        [ 0.0193,  0.0000,  0.9998],\n",
       "        [-0.0545,  0.0000, -0.9985],\n",
       "        [ 0.0342,  0.0000, -0.9994],\n",
       "        [-0.5150, -0.0014, -0.8572],\n",
       "        [-0.1118,  0.0000,  0.9937],\n",
       "        [-0.2518, -0.0100, -0.9677],\n",
       "        [-0.2519,  0.0000, -0.9678],\n",
       "        [ 0.0307,  0.0000,  0.9995],\n",
       "        [-0.0739, -0.0055, -0.9973],\n",
       "        [-0.0587,  0.0000,  0.9983],\n",
       "        [ 0.0520,  0.0000, -0.9986],\n",
       "        [ 0.0686,  0.1453, -0.9870],\n",
       "        [-0.0226,  0.0137, -0.9997],\n",
       "        [-0.4854, -0.0164, -0.8741],\n",
       "        [-0.1249,  0.0000,  0.9922],\n",
       "        [-0.0246,  0.0000, -0.9997],\n",
       "        [-0.1222,  0.0000, -0.9925],\n",
       "        [-0.0508,  0.0000,  0.9987],\n",
       "        [-0.0349,  0.1583, -0.9868],\n",
       "        [-0.0624,  0.0000,  0.9981],\n",
       "        [-0.0357,  0.0000, -0.9994],\n",
       "        [ 0.0103,  0.0000,  0.9999],\n",
       "        [-0.1849,  0.0000,  0.9828],\n",
       "        [-0.0445,  0.0000, -0.9990],\n",
       "        [-0.0820,  0.0000,  0.9966],\n",
       "        [ 0.0682,  0.0000,  0.9977],\n",
       "        [-0.1788,  0.0000, -0.9839],\n",
       "        [-0.1028,  0.0000, -0.9947],\n",
       "        [ 0.0209,  0.0049, -0.9998],\n",
       "        [ 0.0305,  0.0000,  0.9995],\n",
       "        [-0.2395, -0.3629, -0.9005],\n",
       "        [-0.1462,  0.0000,  0.9893],\n",
       "        [-0.0169,  0.0000, -0.9999],\n",
       "        [ 0.1536,  0.0000, -0.9881],\n",
       "        [-0.3422,  0.0000,  0.9396],\n",
       "        [-0.0794,  0.0000,  0.9968],\n",
       "        [-0.3226, -0.0324, -0.9460],\n",
       "        [ 0.0577,  0.0000,  0.9983],\n",
       "        [-0.1278,  0.0000, -0.9918],\n",
       "        [-0.0064,  0.0000,  1.0000],\n",
       "        [ 0.0866,  0.0570, -0.9946],\n",
       "        [-0.0926,  0.0000,  0.9957],\n",
       "        [-0.0038,  0.0000, -1.0000],\n",
       "        [-0.2570,  0.0000, -0.9664],\n",
       "        [-0.0982,  0.0000,  0.9952],\n",
       "        [ 0.0323,  0.0000, -0.9995],\n",
       "        [-0.0605,  0.0000,  0.9982],\n",
       "        [-0.0666,  0.0000,  0.9978],\n",
       "        [ 0.0047,  0.0000,  1.0000],\n",
       "        [-0.2895, -0.1053, -0.9514],\n",
       "        [-0.0404,  0.0000,  0.9992],\n",
       "        [-0.1173,  0.0000,  0.9931],\n",
       "        [-0.0577,  0.0000,  0.9983],\n",
       "        [-0.2455,  0.0000,  0.9694],\n",
       "        [-0.0061,  0.0897, -0.9960]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does not work we do not have pos\n",
    "#visualize_points(data.pos, data.edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training graphs pairs: 382\n",
      "Number of validation graphs: 95\n",
      "Number of test graphs: 95\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import DataLoader \n",
    "\n",
    "batch_size = 1\n",
    "\n",
    "n_train = math.ceil((4/6) * len(dataset))\n",
    "n_val = math.ceil((len(dataset) - n_train)/2)\n",
    "n_test = len(dataset) - n_train - n_val\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [n_train, n_val, n_test])\n",
    "print(f'Number of training graphs pairs: {len(train_dataset)}')\n",
    "print(f'Number of validation graphs: {len(val_dataset)}')\n",
    "print(f'Number of test graphs: {len(test_dataset)}')\n",
    "\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size= batch_size, shuffle=True)\n",
    "val_loader = DataLoader(dataset = val_dataset, batch_size= batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size= batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PairDataBatch(adj1=[100, 100], x1=[100, 3], adj2=[100, 100], x2=[100, 3], y=[1])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "databatch = next(iter(train_loader))\n",
    "databatch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, in_nodes, in_channels, hidden_channels, out_channels,\n",
    "                 normalize=False, lin=True):\n",
    "        super(GNN, self).__init__()\n",
    "\n",
    "        # Each instance of this GNN will have 3 convolutional layers and three batch norm layers        \n",
    "        self.conv1 = DenseGCNConv(in_channels, hidden_channels, normalize)\n",
    "        #self.bns1 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv2 = DenseGCNConv(hidden_channels, hidden_channels, normalize)\n",
    "        #self.bns2 = torch.nn.BatchNorm1d(in_nodes)\n",
    "        \n",
    "        self.conv3 = DenseGCNConv(hidden_channels, out_channels, normalize)\n",
    "        #self.bns3 = torch.nn.BatchNorm1d(in_nodes)\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, mask=None):\n",
    "        \n",
    "        #Step 1\n",
    "        x = self.conv1(x, adj, mask)\n",
    "        #x = self.bns1(x)\n",
    "        \n",
    "        #Step 2\n",
    "        x = self.conv2(x, adj, mask)\n",
    "        #x = self.bns2(x)\n",
    "\n",
    "        #Step 3\n",
    "        x = self.conv3(x, adj, mask)\n",
    "        #if x.shape[2] != 1: \n",
    "        #    x = self.bns3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class DiffPool(torch.nn.Module):\n",
    "    def __init__(self, num_nodes):\n",
    "        super(DiffPool, self).__init__()\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        in_nodes = num_nodes\n",
    "        out_nodes = 25 # Number of clusters / nodes in the next layer\n",
    "        #self.gnn1_pool = GNN(in_nodes, dataset.num_features, 16, out_nodes) # PoolGNN --> Cluster Assignment Matrix to reduce to num_nodes\n",
    "        self.gnn1_embed = GNN(in_nodes, dataset.num_features, 8, 8) # EmbGNN --> Convolutions to create new node embedding\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 10\n",
    "        #self.gnn2_pool = GNN(in_nodes, 8, 8, out_nodes)\n",
    "        self.gnn2_embed = GNN(in_nodes, 8, 12, 16, lin=False)\n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        in_nodes = out_nodes\n",
    "        out_nodes = 1\n",
    "        #self.gnn3_pool = GNN(in_nodes, 16, 16, out_nodes)\n",
    "        self.gnn3_embed = GNN(in_nodes, 16, 16, 32, lin=False)\n",
    "\n",
    "        # Final Classifier\n",
    "        self.lin1 = torch.nn.Linear(32, 64) \n",
    "        #self.lin2 = torch.nn.Linear(64, 2)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, adj, batch, mask=None):\n",
    "        \n",
    "        #if batch == 0: print('Shape of input data batch:')\n",
    "        #if batch == 0: print(f'Feature Matrix: {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'Adjacency Matrix: {tuple(adj.shape)}')\n",
    "       \n",
    "\n",
    "\n",
    "        #Hierarchical Step #1\n",
    "        #if batch == 0: print('Hierarchical Step #1')\n",
    "        x = self.gnn1_embed(x, adj, mask) # node feature embedding\n",
    "        #s = self.gnn1_pool(x, adj, mask) # cluster assignment matrix\n",
    "\n",
    "        #if batch == 0: print(f'X1 = {tuple(x1.shape)}    S1: {tuple(s.shape)}')\n",
    "\n",
    "        #x, adj, l1, e1 = dense_diff_pool(x1, adj, s, mask) # does the necessary matrix multiplications\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "   \n",
    "\n",
    "\n",
    "        # Hierarchical Step #2\n",
    "        #if batch == 0: print('Hierarchical Step #2')\n",
    "        x = self.gnn2_embed(x, adj)\n",
    "        #s = self.gnn2_pool(x, adj)\n",
    "\n",
    "        #if batch == 0: print(f'X2: {tuple(x2.shape)}    S2: {tuple(s.shape)}')\n",
    "        \n",
    "        #x, adj, l2, e2 = dense_diff_pool(x2, adj, s)\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "      \n",
    "        \n",
    "\n",
    "        # Hierarchical Step #3\n",
    "        #if batch == 0: print('Hierarchical Step #3')\n",
    "        x = self.gnn3_embed(x, adj)\n",
    "        #s = self.gnn3_pool(x, adj)\n",
    "        \n",
    "        #if batch == 0: print(f'X3: {tuple(x3.shape)}    S3: {tuple(s.shape)}')\n",
    "\n",
    "        #x, adj, l3, e3 = dense_diff_pool(x3, adj, s)\n",
    "        #adj = torch.softmax(adj, dim=-1)\n",
    "\n",
    "        #if batch == 0: print(f'---matmul---> New feature matrix (softmax(s_0.t()) @ z_0) = {tuple(x.shape)}')\n",
    "        #if batch == 0: print(f'---matmul---> New adjacency matrix (s_0.t() @ adj_0 @ s_0) = {tuple(adj.shape)}')\n",
    "     \n",
    "        \n",
    "\n",
    "        # Final Classification\n",
    "        #if batch == 0: print('Final Output')\n",
    "        x = global_max_pool(x, batch=None) # Pool the features of all nodes (global mean pool)  dim = 1 refers to columns\n",
    "        #if batch == 0: print(f'---X Output after mean= {tuple(x.shape)}')\n",
    "\n",
    "        x = F.relu(self.lin1(x)) # Fully connected layer + relu\n",
    "        #if batch == 0: print(f'------ X Output 3 after lin= {tuple(x.shape)}')\n",
    "\n",
    "        \n",
    "        return x #, l1 + l2 + l3, e1 + e2 + e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An attempt of a contrastive loss function\n",
    "#   pairs with label 1 --> should get small euclid dist = small loss\n",
    "#   pairs with label 0 --> should get large euclid dist = large loss\n",
    "\n",
    "class ContrastiveLoss(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, margin=1.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, x0, x1, y):\n",
    "        # euclidian distance\n",
    "        #print(x0)\n",
    "        #print(x1)\n",
    "        #print(y)\n",
    "        diff = x0 - x1\n",
    "        #print(diff)\n",
    "        pow = torch.pow(diff, 2)\n",
    "        #print(pow)\n",
    "        dist_sq = torch.sum(pow, 1)\n",
    "        #print(dist_sq) # sum of squared distance = 0.5 = 9\n",
    "        dist = torch.sqrt(dist_sq)\n",
    "        #print(dist) # euclidean distance = 0.7 = 3\n",
    "\n",
    "        mdist = self.margin - dist #negative euclidean distance - margin = 0.3 = -2\n",
    "        #print(mdist)\n",
    "        dist_marg = torch.clamp(mdist, min=0.0) # only distances <margin will be still positive here = 0.3 = 0\n",
    "        #print(dist)\n",
    "        loss =  y * torch.pow(dist, 2) + (1-y) * torch.pow(dist_marg,2)\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 0.5\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0.3^2\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 0.5\n",
    "\n",
    "        # What happens to a pair with squared euclid dist (dist_sq) of 9\n",
    "        # if label = 0 --> 0 + squared clampled euclid distance --> loss = 0\n",
    "        # if label = 1 --> squared euclidean distance + 0 --> loss = 9\n",
    "\n",
    "        #print(loss)\n",
    "        #loss = torch.sum(loss) / 2.0 \n",
    "        loss = torch.sum(loss) / 2.0 / x0.size()[0]\n",
    "        #print(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Epoch: 001, Train Loss: 0.130\n",
      "Epoch: 002, Train Loss: 0.121\n",
      "Epoch: 003, Train Loss: 0.117\n",
      "Epoch: 004, Train Loss: 0.114\n",
      "Epoch: 005, Train Loss: 0.110\n",
      "Epoch: 006, Train Loss: 0.110\n",
      "Epoch: 007, Train Loss: 0.112\n",
      "Epoch: 008, Train Loss: 0.109\n",
      "Epoch: 009, Train Loss: 0.110\n",
      "Epoch: 010, Train Loss: 0.103\n",
      "Epoch: 011, Train Loss: 0.106\n",
      "Epoch: 012, Train Loss: 0.103\n",
      "Epoch: 013, Train Loss: 0.111\n",
      "Epoch: 014, Train Loss: 0.104\n",
      "Epoch: 015, Train Loss: 0.102\n",
      "Epoch: 016, Train Loss: 0.103\n",
      "Epoch: 017, Train Loss: 0.102\n",
      "Epoch: 018, Train Loss: 0.099\n",
      "Epoch: 019, Train Loss: 0.099\n",
      "Epoch: 020, Train Loss: 0.099\n",
      "Epoch: 021, Train Loss: 0.100\n",
      "Epoch: 022, Train Loss: 0.103\n",
      "Epoch: 023, Train Loss: 0.098\n",
      "Epoch: 024, Train Loss: 0.102\n",
      "Epoch: 025, Train Loss: 0.097\n",
      "Epoch: 026, Train Loss: 0.099\n",
      "Epoch: 027, Train Loss: 0.099\n",
      "Epoch: 028, Train Loss: 0.098\n",
      "Epoch: 029, Train Loss: 0.100\n",
      "Epoch: 030, Train Loss: 0.096\n",
      "Epoch: 031, Train Loss: 0.100\n",
      "Epoch: 032, Train Loss: 0.096\n",
      "Epoch: 033, Train Loss: 0.101\n",
      "Epoch: 034, Train Loss: 0.097\n",
      "Epoch: 035, Train Loss: 0.098\n",
      "Epoch: 036, Train Loss: 0.096\n",
      "Epoch: 037, Train Loss: 0.094\n",
      "Epoch: 038, Train Loss: 0.089\n",
      "Epoch: 039, Train Loss: 0.089\n",
      "Epoch: 040, Train Loss: 0.095\n",
      "Epoch: 041, Train Loss: 0.092\n",
      "Epoch: 042, Train Loss: 0.094\n",
      "Epoch: 043, Train Loss: 0.090\n",
      "Epoch: 044, Train Loss: 0.088\n",
      "Epoch: 045, Train Loss: 0.094\n",
      "Epoch: 046, Train Loss: 0.088\n",
      "Epoch: 047, Train Loss: 0.090\n",
      "Epoch: 048, Train Loss: 0.088\n",
      "Epoch: 049, Train Loss: 0.091\n",
      "Epoch: 050, Train Loss: 0.089\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "model = DiffPool(num_nodes = 100).to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train(epoch):\n",
    "    batch = 0\n",
    "    model.train()\n",
    "    loss_all = 0\n",
    "\n",
    "    for data in train_loader:\n",
    "        data = data.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output1 = model(data.x1, data.adj1, batch)\n",
    "        output2 = model(data.x2, data.adj2, batch = None)\n",
    "        \n",
    "        #Contrastive Loss\n",
    "        loss_contrastive = criterion(output1,output2,data.y)\n",
    "        loss_contrastive.backward()\n",
    "        loss_all += data.y.size(0) * loss_contrastive.item()\n",
    "        optimizer.step()\n",
    "        batch +=1\n",
    "\n",
    "    return loss_all / len(train_dataset)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "\n",
    "    distances_lab1 = []\n",
    "    distances_lab0 = []\n",
    "    labels = []\n",
    "    losses = []\n",
    "    \n",
    "\n",
    "    for data in loader:\n",
    "        data = data.to(device)\n",
    "        output1 = model(data.x1, data.adj2, batch=None)\n",
    "        output2 = model(data.x2, data.adj2, batch=None)\n",
    "\n",
    "        test_loss_contrastive = criterion(output1, output2, data.y)\n",
    "        \n",
    "        euclidean_distance = F.pairwise_distance(output1, output2)\n",
    "        label = data.y\n",
    "\n",
    "        if int(label) == 1: \n",
    "            distances_lab1.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "        else:\n",
    "            distances_lab0.append(float(euclidean_distance))\n",
    "            labels.append(int(label))\n",
    "            losses.append(float(test_loss_contrastive))\n",
    "\n",
    "    return  distances_lab0, distances_lab1, losses, labels\n",
    "\n",
    "\n",
    "\n",
    "train_distances_lab0 = []\n",
    "train_distances_lab1 = []\n",
    "train_losses = []\n",
    "train_labels = []\n",
    "\n",
    "validation_distances_lab0 = []\n",
    "validation_distances_lab1 = []\n",
    "validation_losses = []\n",
    "validation_labels = []\n",
    "\n",
    "test_distances_lab0 = []\n",
    "test_distances_lab1 = []\n",
    "test_losses = []\n",
    "test_labels = []\n",
    "\n",
    "for epoch in range(1, num_epochs+1):\n",
    "    \n",
    "    train_loss = train(epoch)\n",
    "\n",
    "    train_results = test(train_loader)\n",
    "    train_distances_lab0.append(train_results[0])\n",
    "    train_distances_lab1.append(train_results[1])\n",
    "    train_losses.append(train_results[2])\n",
    "    train_labels.append(train_results[3])\n",
    "\n",
    "\n",
    "    validation_results = test(val_loader)\n",
    "    validation_distances_lab0.append(validation_results[0])\n",
    "    validation_distances_lab1.append(validation_results[1])\n",
    "    validation_losses.append(validation_results[2])\n",
    "    validation_labels.append(validation_results[3])\n",
    "\n",
    "    test_results = test(test_loader)\n",
    "    test_distances_lab0.append(test_results[0])\n",
    "    test_distances_lab1.append(test_results[1])\n",
    "    test_losses.append(test_results[2])\n",
    "    test_labels.append(test_results[3])\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}')\n",
    "    #Train Acc: {train_acc:.3f}, f'Val Acc: {val_acc:.3f}, Test Acc: {test_acc:.3f}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compare_euclid_distances(distances_lab0, distances_lab1):\n",
    "\n",
    "    w = 0.8    # bar width\n",
    "    x = [1, 2] # x-coordinates of your bars\n",
    "    colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "\n",
    "    # Epoch 0\n",
    "    y = [distances_lab0, distances_lab1]\n",
    "\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.bar(x,\n",
    "        height=[np.mean(yi) for yi in y],\n",
    "        yerr=[np.std(yi) for yi in y],    # error bars\n",
    "        capsize=12, # error bar cap width in points\n",
    "        width=w,    # bar width\n",
    "        tick_label=[\"Label 0\", \"Label 1\"],\n",
    "        color=(0,0,0,0),  # face color transparent\n",
    "        edgecolor=colors,\n",
    "        )\n",
    "\n",
    "    for i in range(len(x)):\n",
    "        # distribute scatter randomly across whole width of bar\n",
    "        ax.scatter(x[i] + np.random.random(len(y[i])) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "    plt.ylabel = 'Euclidean Distance'\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "compare_euclid_distances(train_distances_lab0[49], train_distances_lab1[49])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's state_dict:\n",
      "gnn1_embed.conv1.bias \t torch.Size([8])\n",
      "gnn1_embed.conv1.lin.weight \t torch.Size([8, 3])\n",
      "gnn1_embed.conv2.bias \t torch.Size([8])\n",
      "gnn1_embed.conv2.lin.weight \t torch.Size([8, 8])\n",
      "gnn1_embed.conv3.bias \t torch.Size([8])\n",
      "gnn1_embed.conv3.lin.weight \t torch.Size([8, 8])\n",
      "gnn2_embed.conv1.bias \t torch.Size([12])\n",
      "gnn2_embed.conv1.lin.weight \t torch.Size([12, 8])\n",
      "gnn2_embed.conv2.bias \t torch.Size([12])\n",
      "gnn2_embed.conv2.lin.weight \t torch.Size([12, 12])\n",
      "gnn2_embed.conv3.bias \t torch.Size([16])\n",
      "gnn2_embed.conv3.lin.weight \t torch.Size([16, 12])\n",
      "gnn3_embed.conv1.bias \t torch.Size([16])\n",
      "gnn3_embed.conv1.lin.weight \t torch.Size([16, 16])\n",
      "gnn3_embed.conv2.bias \t torch.Size([16])\n",
      "gnn3_embed.conv2.lin.weight \t torch.Size([16, 16])\n",
      "gnn3_embed.conv3.bias \t torch.Size([32])\n",
      "gnn3_embed.conv3.lin.weight \t torch.Size([32, 16])\n",
      "lin1.weight \t torch.Size([64, 32])\n",
      "lin1.bias \t torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "# Print model's state_dict\n",
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in model.state_dict():\n",
    "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer's state_dict:\n",
      "state \t {0: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0135,  0.0123, -0.0111,  0.0040, -0.0045,  0.0243,  0.0174, -0.0046],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0393, 0.0050, 0.0251, 0.0362, 0.0419, 0.0314, 0.0370, 0.0054],\n",
      "       device='cuda:0')}, 1: {'step': tensor(19100.), 'exp_avg': tensor([[-0.0242, -0.0045,  0.0062],\n",
      "        [-0.0010, -0.0010, -0.0206],\n",
      "        [ 0.0093,  0.0023,  0.0057],\n",
      "        [ 0.0039,  0.0001, -0.0058],\n",
      "        [ 0.0282,  0.0043, -0.0240],\n",
      "        [ 0.0103, -0.0002, -0.0541],\n",
      "        [ 0.0033, -0.0006, -0.0382],\n",
      "        [-0.0044, -0.0003,  0.0148]], device='cuda:0'), 'exp_avg_sq': tensor([[5.9735e-03, 3.3805e-04, 5.1046e-02],\n",
      "        [1.7554e-03, 6.9309e-05, 7.2733e-03],\n",
      "        [2.2729e-03, 6.9902e-05, 2.1599e-02],\n",
      "        [7.3488e-03, 2.6627e-04, 2.8862e-02],\n",
      "        [1.4034e-02, 8.1643e-04, 6.8332e-02],\n",
      "        [7.7614e-03, 4.1002e-04, 4.5817e-02],\n",
      "        [1.0501e-02, 3.9348e-04, 3.4368e-02],\n",
      "        [1.1158e-03, 5.0787e-05, 5.7439e-03]], device='cuda:0')}, 2: {'step': tensor(19100.), 'exp_avg': tensor([-0.0026,  0.0103,  0.0264, -0.0151,  0.0114,  0.0008,  0.0014, -0.0016],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([2.6011e-02, 5.8587e-03, 2.8342e-02, 9.9301e-03, 4.2287e-02, 9.9486e-05,\n",
      "        1.7050e-02, 1.6663e-02], device='cuda:0')}, 3: {'step': tensor(19100.), 'exp_avg': tensor([[ 8.8578e-03,  2.5691e-03, -8.6739e-03,  6.3362e-03,  3.1069e-03,\n",
      "          3.8912e-03, -7.5932e-03,  6.9757e-03],\n",
      "        [-8.3703e-03, -5.4070e-03,  5.6695e-03, -4.1442e-03, -8.1300e-04,\n",
      "         -5.3536e-03,  4.4190e-03, -5.3138e-03],\n",
      "        [-1.8558e-02, -1.6630e-02,  9.7581e-03, -7.8242e-03,  7.3914e-04,\n",
      "         -1.4529e-02,  6.2496e-03, -1.0124e-02],\n",
      "        [ 5.9033e-03,  1.0833e-02,  3.6421e-04,  6.6359e-04, -3.2673e-03,\n",
      "          7.8885e-03,  2.2743e-03,  1.2182e-03],\n",
      "        [-2.5972e-02, -1.8565e-03,  2.7378e-02, -1.8356e-02, -1.1030e-02,\n",
      "         -7.8822e-03,  2.5511e-02, -2.2020e-02],\n",
      "        [-4.6263e-05, -6.9120e-04, -3.6082e-04,  1.6946e-04,  3.3750e-04,\n",
      "         -4.2646e-04, -4.6741e-04,  2.0319e-04],\n",
      "        [-1.0788e-03, -2.7711e-03, -6.0283e-05, -4.8672e-04,  6.1289e-04,\n",
      "         -2.0220e-03, -6.4498e-04, -9.5103e-05],\n",
      "        [ 1.4342e-03,  2.9498e-03, -1.8953e-04,  6.6507e-04, -5.8017e-04,\n",
      "          2.1983e-03,  4.3617e-04,  3.3705e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[4.0772e-03, 3.4265e-03, 3.0229e-03, 1.7118e-03, 7.0973e-04, 2.4792e-03,\n",
      "         2.8735e-03, 2.3786e-03],\n",
      "        [1.6019e-03, 1.5095e-03, 9.4273e-04, 5.9831e-04, 1.2447e-04, 1.1499e-03,\n",
      "         7.1734e-04, 7.5768e-04],\n",
      "        [8.2927e-03, 8.9485e-03, 4.6809e-03, 3.0736e-03, 6.2241e-04, 6.4954e-03,\n",
      "         3.4639e-03, 3.7136e-03],\n",
      "        [3.2416e-03, 3.3894e-03, 2.0142e-03, 1.3175e-03, 3.2129e-04, 2.3895e-03,\n",
      "         1.5213e-03, 1.5044e-03],\n",
      "        [1.7706e-02, 1.3442e-02, 1.1601e-02, 7.7921e-03, 1.6080e-03, 1.0056e-02,\n",
      "         8.3863e-03, 8.6615e-03],\n",
      "        [3.2457e-05, 2.9760e-05, 2.1181e-05, 1.3936e-05, 3.6508e-06, 2.1039e-05,\n",
      "         1.6275e-05, 1.5952e-05],\n",
      "        [2.9333e-03, 2.3627e-03, 2.5099e-03, 1.2476e-03, 6.8088e-04, 1.5317e-03,\n",
      "         2.5707e-03, 1.9197e-03],\n",
      "        [3.0292e-03, 2.4424e-03, 2.6119e-03, 1.3124e-03, 7.0809e-04, 1.5518e-03,\n",
      "         2.6610e-03, 1.9836e-03]], device='cuda:0')}, 4: {'step': tensor(19100.), 'exp_avg': tensor([-0.0064,  0.0001, -0.0165,  0.0073, -0.0127,  0.0127, -0.0053,  0.0075],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0124, 0.0069, 0.0296, 0.0062, 0.0052, 0.0056, 0.0179, 0.0736],\n",
      "       device='cuda:0')}, 5: {'step': tensor(19100.), 'exp_avg': tensor([[-1.4264e-02,  8.9740e-06,  2.5416e-03, -1.8099e-03,  3.6802e-03,\n",
      "         -4.0233e-04,  1.2837e-02, -9.0783e-03],\n",
      "        [ 1.4107e-02,  1.4119e-03,  3.2301e-03, -1.1956e-03, -4.7112e-03,\n",
      "          3.6246e-04, -1.2723e-02,  8.4370e-03],\n",
      "        [ 2.8955e-03,  2.7346e-03,  1.1867e-02, -6.4656e-03, -4.0970e-03,\n",
      "          1.8160e-04, -2.0891e-03,  1.7383e-04],\n",
      "        [-1.3134e-02, -2.9568e-03, -9.7311e-03,  4.6912e-03,  5.8078e-03,\n",
      "         -3.1878e-04,  1.1772e-02, -7.1129e-03],\n",
      "        [ 3.6584e-03,  3.1710e-03,  1.2315e-02, -6.5044e-03, -3.8217e-03,\n",
      "          8.5881e-05, -3.0619e-03,  8.1053e-04],\n",
      "        [-8.3362e-03, -3.5818e-03, -1.3217e-02,  6.7777e-03,  5.2900e-03,\n",
      "         -1.9943e-04,  7.3279e-03, -3.6754e-03],\n",
      "        [-5.8112e-03, -3.7529e-04,  4.7359e-04, -6.3103e-04,  1.0343e-03,\n",
      "         -5.4067e-05,  5.5273e-03, -3.8789e-03],\n",
      "        [ 2.1543e-02,  2.0407e-03,  2.9620e-03, -3.6454e-04, -5.6752e-03,\n",
      "          3.6412e-04, -2.0051e-02,  1.3606e-02]], device='cuda:0'), 'exp_avg_sq': tensor([[7.9473e-03, 9.3953e-04, 7.6249e-03, 2.1151e-03, 1.9682e-03, 1.8686e-05,\n",
      "         6.2103e-03, 2.6749e-03],\n",
      "        [3.2902e-03, 4.7815e-04, 4.0744e-03, 1.0573e-03, 9.3256e-04, 9.0644e-06,\n",
      "         2.5301e-03, 9.7408e-04],\n",
      "        [5.0054e-03, 5.8910e-04, 7.7931e-03, 1.6371e-03, 1.4673e-03, 1.7478e-05,\n",
      "         3.7536e-03, 1.6370e-03],\n",
      "        [2.5757e-03, 4.3321e-04, 3.7741e-03, 9.6736e-04, 8.1648e-04, 8.2934e-06,\n",
      "         1.8933e-03, 6.9128e-04],\n",
      "        [2.2690e-03, 3.3293e-04, 3.4195e-03, 8.6996e-04, 6.1788e-04, 5.9341e-06,\n",
      "         1.6161e-03, 6.5921e-04],\n",
      "        [2.3584e-03, 3.7328e-04, 3.6261e-03, 9.3045e-04, 7.0590e-04, 7.1993e-06,\n",
      "         1.6465e-03, 6.5168e-04],\n",
      "        [2.0713e-03, 2.6733e-04, 3.2923e-03, 6.6240e-04, 5.2907e-04, 8.3430e-06,\n",
      "         1.7638e-03, 8.7223e-04],\n",
      "        [8.8054e-03, 1.5109e-03, 1.6389e-02, 3.4771e-03, 2.3548e-03, 3.6203e-05,\n",
      "         7.5355e-03, 3.3775e-03]], device='cuda:0')}, 6: {'step': tensor(19100.), 'exp_avg': tensor([-2.6359e-05,  2.4346e-03,  2.0398e-03, -1.2724e-02,  1.4660e-02,\n",
      "         2.7467e-04, -3.4451e-03,  3.0359e-03,  1.3926e-02,  4.0799e-03,\n",
      "        -2.7671e-03,  2.0671e-04], device='cuda:0'), 'exp_avg_sq': tensor([0.0014, 0.0306, 0.0262, 0.0055, 0.0084, 0.0007, 0.0031, 0.0014, 0.0256,\n",
      "        0.0020, 0.0007, 0.0033], device='cuda:0')}, 7: {'step': tensor(19100.), 'exp_avg': tensor([[-1.1984e-03, -1.8842e-04,  3.6506e-03,  1.1179e-03, -8.4810e-04,\n",
      "          1.6761e-03,  5.9153e-03, -3.1524e-03],\n",
      "        [ 1.9222e-03, -4.2274e-04, -4.1653e-03, -4.5528e-04,  5.5916e-04,\n",
      "         -1.4232e-03, -6.6261e-03,  3.0257e-03],\n",
      "        [ 7.0157e-03, -4.1025e-04, -5.9657e-03, -2.7165e-03,  3.9782e-03,\n",
      "         -6.8774e-03, -1.6901e-02,  7.1002e-03],\n",
      "        [-5.7538e-03,  9.5780e-04, -1.2183e-02, -4.7940e-04, -3.9843e-03,\n",
      "          4.1440e-03, -3.5890e-03,  5.3797e-03],\n",
      "        [ 5.4598e-03, -1.1215e-03,  1.1316e-02,  8.1722e-04,  3.5578e-03,\n",
      "         -3.5610e-03,  3.4625e-03, -5.2723e-03],\n",
      "        [ 6.5071e-05, -2.2429e-04,  1.3301e-03,  4.5192e-04, -1.6467e-04,\n",
      "          3.3646e-04,  1.5767e-03, -1.0717e-03],\n",
      "        [ 2.5150e-03,  1.0485e-04, -4.6225e-03, -1.6617e-03,  1.4468e-03,\n",
      "         -2.9600e-03, -8.7954e-03,  4.4183e-03],\n",
      "        [ 3.6580e-03,  1.1696e-05, -1.1172e-03, -1.5547e-03,  2.5923e-03,\n",
      "         -3.8960e-03, -7.3154e-03,  2.7968e-03],\n",
      "        [ 1.2513e-02, -9.8777e-04,  2.5817e-03, -3.0660e-03,  8.0221e-03,\n",
      "         -1.1458e-02, -1.7073e-02,  4.2840e-03],\n",
      "        [ 5.5232e-03, -2.9858e-04,  1.2306e-03, -1.5989e-03,  3.6612e-03,\n",
      "         -5.3002e-03, -7.6277e-03,  2.0400e-03],\n",
      "        [-2.7307e-03,  7.8714e-05, -4.4379e-04,  8.9773e-04, -1.9178e-03,\n",
      "          2.7282e-03,  4.0721e-03, -1.1951e-03],\n",
      "        [ 5.6358e-03, -1.0794e-05, -2.5171e-03, -2.5077e-03,  3.7490e-03,\n",
      "         -6.0145e-03, -1.1864e-02,  4.7895e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[1.9524e-04, 2.9448e-05, 6.6582e-04, 5.8901e-05, 1.2256e-04, 1.9036e-04,\n",
      "         5.7465e-04, 2.0976e-04],\n",
      "        [3.0395e-03, 2.5603e-04, 4.4193e-03, 7.3437e-04, 1.7428e-03, 3.3299e-03,\n",
      "         6.0487e-03, 1.4933e-03],\n",
      "        [1.8715e-03, 1.8990e-04, 5.3932e-03, 4.7872e-04, 1.2802e-03, 2.0858e-03,\n",
      "         3.8375e-03, 1.3103e-03],\n",
      "        [1.8513e-03, 1.3290e-04, 3.1734e-03, 3.8569e-04, 1.1471e-03, 1.9233e-03,\n",
      "         3.4974e-03, 9.5635e-04],\n",
      "        [1.3592e-03, 1.4602e-04, 3.4325e-03, 4.0075e-04, 1.0899e-03, 1.5928e-03,\n",
      "         3.1218e-03, 1.0670e-03],\n",
      "        [1.0953e-04, 1.2280e-05, 2.0310e-04, 3.0038e-05, 6.6611e-05, 1.1514e-04,\n",
      "         2.6800e-04, 7.8819e-05],\n",
      "        [3.7619e-04, 4.7086e-05, 1.2940e-03, 1.0030e-04, 2.4423e-04, 3.7936e-04,\n",
      "         9.0749e-04, 3.4896e-04],\n",
      "        [3.8724e-04, 3.5666e-05, 9.0493e-04, 7.8761e-05, 2.0364e-04, 3.5324e-04,\n",
      "         1.0553e-03, 3.1086e-04],\n",
      "        [1.9375e-03, 1.6658e-04, 5.8363e-03, 4.9278e-04, 1.5583e-03, 2.2585e-03,\n",
      "         4.6045e-03, 1.5496e-03],\n",
      "        [6.8049e-04, 5.4472e-05, 1.4680e-03, 1.3323e-04, 3.6795e-04, 6.4389e-04,\n",
      "         1.8587e-03, 5.1082e-04],\n",
      "        [2.3210e-04, 1.7873e-05, 5.0281e-04, 4.3872e-05, 1.2518e-04, 2.1872e-04,\n",
      "         6.2932e-04, 1.7209e-04],\n",
      "        [8.2476e-04, 9.4166e-05, 1.9795e-03, 1.9940e-04, 4.5331e-04, 7.6789e-04,\n",
      "         2.2137e-03, 6.9976e-04]], device='cuda:0')}, 8: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0133, -0.0127,  0.0005,  0.0026,  0.0022,  0.0027, -0.0032,  0.0065,\n",
      "         0.0066,  0.0096,  0.0069, -0.0049], device='cuda:0'), 'exp_avg_sq': tensor([4.0630e-03, 4.6753e-02, 6.1533e-05, 3.6051e-03, 8.4156e-04, 7.9411e-04,\n",
      "        1.5703e-02, 1.0760e-03, 2.0158e-03, 3.8210e-03, 2.4902e-03, 2.2310e-03],\n",
      "       device='cuda:0')}, 9: {'step': tensor(19100.), 'exp_avg': tensor([[ 7.3460e-04,  3.6206e-04, -3.4073e-03,  7.3709e-03, -7.7205e-03,\n",
      "          2.7713e-04, -4.3370e-04, -1.6963e-03, -1.0407e-02, -2.3087e-03,\n",
      "          1.7775e-03,  5.4739e-04],\n",
      "        [ 1.6836e-03, -9.3819e-03, -3.3695e-03, -8.3303e-03,  1.0369e-03,\n",
      "         -9.4048e-04,  4.6401e-04,  3.0224e-03,  1.3944e-04,  4.2502e-03,\n",
      "         -2.4064e-03,  1.9605e-03],\n",
      "        [ 3.0335e-06, -1.6058e-04,  7.5008e-05, -3.6598e-04,  2.7374e-04,\n",
      "         -3.5794e-06,  9.3935e-06,  6.4316e-05,  3.1915e-04,  9.4179e-05,\n",
      "         -9.2944e-05, -1.9197e-05],\n",
      "        [ 2.9952e-03, -9.9924e-03, -5.7176e-03, -5.2339e-03, -3.0429e-03,\n",
      "         -3.9140e-04, -4.8092e-04,  1.1403e-03, -6.2349e-03,  2.1637e-03,\n",
      "         -1.0856e-03,  1.3008e-03],\n",
      "        [-8.1289e-04,  1.7072e-03,  7.9600e-04,  1.7730e-03, -2.3978e-04,\n",
      "         -3.1639e-05,  3.7403e-04,  7.6336e-05,  2.2096e-04, -1.8087e-04,\n",
      "          6.3957e-05,  2.8296e-04],\n",
      "        [-9.3298e-04,  3.9577e-03,  1.2885e-03,  3.9865e-03, -8.9537e-04,\n",
      "          2.4137e-04,  4.0620e-05, -9.4179e-04, -4.0245e-04, -1.5054e-03,\n",
      "          9.2479e-04, -4.0033e-04],\n",
      "        [-2.5096e-03,  8.3989e-03,  6.3276e-03,  1.1053e-03,  6.0594e-03,\n",
      "          4.5528e-04,  2.3191e-04, -8.4234e-04,  9.6104e-03, -1.4408e-03,\n",
      "          4.2352e-04, -1.9266e-03],\n",
      "        [-1.1279e-04,  1.0840e-03, -4.0913e-04,  2.7432e-03, -1.9795e-03,\n",
      "          1.5956e-04, -7.5153e-05, -6.5424e-04, -2.5280e-03, -9.4997e-04,\n",
      "          6.0021e-04,  1.4370e-05],\n",
      "        [ 1.0440e-03, -2.0717e-03, -3.0939e-03,  2.6913e-03, -4.7509e-03,\n",
      "          3.5523e-05, -3.1315e-04, -5.7723e-04, -6.8530e-03, -6.6362e-04,\n",
      "          6.6891e-04,  6.2812e-04],\n",
      "        [-1.1600e-03,  6.0542e-03,  1.0076e-03,  8.0336e-03, -3.4653e-03,\n",
      "          4.8636e-04, -1.0415e-04, -1.9840e-03, -3.5417e-03, -3.0008e-03,\n",
      "          1.9041e-03, -5.3343e-04],\n",
      "        [-7.8553e-04,  4.8283e-03,  2.7388e-04,  7.3428e-03, -3.8323e-03,\n",
      "          3.8804e-04, -1.4210e-04, -1.7523e-03, -4.2152e-03, -2.6256e-03,\n",
      "          1.7684e-03, -3.1143e-04],\n",
      "        [-2.1317e-03,  6.3802e-03,  4.1952e-03,  2.3408e-03,  3.0649e-03,\n",
      "          1.4945e-04,  4.7739e-04, -3.1920e-04,  5.6011e-03, -8.8359e-04,\n",
      "          3.6107e-04, -7.5303e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[3.9311e-04, 2.1087e-03, 8.1004e-04, 2.7588e-03, 1.4747e-03, 2.4370e-05,\n",
      "         1.4606e-04, 1.9926e-04, 2.5099e-03, 2.7544e-04, 1.2644e-04, 2.9297e-04],\n",
      "        [9.1730e-04, 5.1137e-03, 1.5859e-03, 7.5448e-03, 3.9513e-03, 7.7086e-05,\n",
      "         4.6840e-04, 6.4418e-04, 6.0567e-03, 7.5855e-04, 3.9622e-04, 1.1655e-03],\n",
      "        [3.2617e-06, 1.9179e-05, 8.6617e-06, 2.1041e-05, 1.6346e-05, 3.0356e-07,\n",
      "         1.4298e-06, 2.3280e-06, 2.8181e-05, 2.8426e-06, 1.2193e-06, 3.8689e-06],\n",
      "        [3.1696e-04, 1.6449e-03, 9.1533e-04, 1.3077e-03, 1.3985e-03, 2.8809e-05,\n",
      "         1.3938e-04, 2.2662e-04, 2.6494e-03, 2.5423e-04, 1.0436e-04, 3.5467e-04],\n",
      "        [6.4288e-05, 4.9627e-04, 1.6317e-04, 4.5843e-04, 2.4053e-04, 7.6652e-06,\n",
      "         2.6406e-05, 6.3790e-05, 4.2153e-04, 8.7717e-05, 4.1162e-05, 6.8503e-05],\n",
      "        [3.7098e-05, 2.3257e-04, 9.8771e-05, 2.4761e-04, 1.7228e-04, 3.6689e-06,\n",
      "         1.5385e-05, 2.8521e-05, 3.0542e-04, 3.6882e-05, 1.6085e-05, 4.0357e-05],\n",
      "        [4.4677e-04, 2.8015e-03, 1.0294e-03, 3.7431e-03, 2.3080e-03, 4.7435e-05,\n",
      "         2.2255e-04, 3.8760e-04, 3.6664e-03, 4.8348e-04, 2.4867e-04, 5.9434e-04],\n",
      "        [9.8623e-05, 6.1059e-04, 2.4130e-04, 6.7918e-04, 4.0265e-04, 7.0772e-06,\n",
      "         3.4501e-05, 5.3524e-05, 7.0378e-04, 7.6457e-05, 3.2615e-05, 8.0443e-05],\n",
      "        [1.8644e-04, 1.0069e-03, 4.1427e-04, 1.2661e-03, 7.5951e-04, 1.3553e-05,\n",
      "         7.3420e-05, 1.0911e-04, 1.2998e-03, 1.4277e-04, 6.6872e-05, 1.5989e-04],\n",
      "        [1.5838e-04, 9.6065e-04, 3.6411e-04, 1.2374e-03, 6.9428e-04, 1.1357e-05,\n",
      "         5.8244e-05, 8.7357e-05, 1.1944e-03, 1.2637e-04, 5.2571e-05, 1.3399e-04],\n",
      "        [1.4930e-04, 9.0987e-04, 3.2261e-04, 1.1187e-03, 5.5639e-04, 1.0159e-05,\n",
      "         5.1685e-05, 8.1256e-05, 9.6208e-04, 1.2086e-04, 5.2265e-05, 1.0759e-04],\n",
      "        [2.1010e-04, 1.0232e-03, 5.3760e-04, 8.5541e-04, 8.0269e-04, 1.7592e-05,\n",
      "         9.3124e-05, 1.4048e-04, 1.5108e-03, 1.5455e-04, 6.4352e-05, 2.2172e-04]],\n",
      "       device='cuda:0')}, 10: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0015,  0.0021,  0.0188, -0.0006, -0.0016, -0.0167, -0.0046,  0.0074,\n",
      "         0.0129,  0.0037,  0.0025, -0.0008, -0.0023, -0.0010, -0.0002, -0.0042],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([1.1445e-04, 1.2671e-03, 1.4416e-02, 6.0948e-04, 6.5163e-05, 1.8818e-02,\n",
      "        3.5178e-03, 2.0898e-03, 7.9542e-03, 5.2617e-03, 3.3190e-02, 3.6126e-04,\n",
      "        8.8644e-05, 4.5596e-05, 1.1186e-04, 1.9650e-03], device='cuda:0')}, 11: {'step': tensor(19100.), 'exp_avg': tensor([[-9.9393e-04, -7.1665e-04,  2.4532e-05, -5.2744e-04, -3.8773e-04,\n",
      "          1.3290e-05,  1.1274e-03, -5.2284e-04, -9.5967e-04, -3.8739e-04,\n",
      "         -4.5318e-04,  4.1326e-04],\n",
      "        [-2.2862e-03, -5.3411e-03,  6.0860e-05, -8.6144e-04, -2.7600e-03,\n",
      "         -1.5345e-04,  5.1787e-03, -1.0316e-03, -2.5002e-03, -4.8202e-04,\n",
      "         -1.2027e-03,  5.8134e-04],\n",
      "        [-1.0326e-02, -3.4770e-03,  4.4543e-06,  2.3627e-03, -7.5503e-03,\n",
      "         -1.7709e-03,  6.8426e-03, -3.9754e-03, -7.4765e-03, -8.5580e-03,\n",
      "         -6.3930e-03, -9.9456e-04],\n",
      "        [-5.0343e-04,  4.2865e-03, -4.7814e-05,  8.8358e-04,  1.1800e-03,\n",
      "         -1.5621e-04, -3.0317e-03, -1.7105e-04,  2.7724e-04, -1.4884e-03,\n",
      "         -4.2138e-04, -3.4507e-04],\n",
      "        [ 6.1505e-04,  9.8110e-04, -1.6617e-05,  1.7115e-04,  6.0715e-04,\n",
      "          4.9925e-05, -1.0301e-03,  2.8016e-04,  5.9942e-04,  2.3652e-04,\n",
      "          3.2101e-04, -1.0601e-04],\n",
      "        [ 8.3488e-03, -6.1248e-05,  7.6229e-05, -2.8117e-03,  5.4338e-03,\n",
      "          1.5943e-03, -3.2286e-03,  3.0847e-03,  5.5125e-03,  7.8156e-03,\n",
      "          5.2936e-03,  1.3157e-03],\n",
      "        [ 1.1809e-03, -5.2748e-03,  5.4082e-05, -1.5755e-03, -7.7007e-04,\n",
      "          3.9533e-04,  3.6536e-03,  3.5502e-04, -7.9158e-05,  2.5297e-03,\n",
      "          9.0750e-04,  8.2339e-04],\n",
      "        [-3.1726e-03,  9.1026e-03, -7.2437e-05,  2.5902e-03,  1.0116e-03,\n",
      "         -7.4566e-04, -5.7009e-03, -1.1228e-03, -7.6519e-04, -5.1585e-03,\n",
      "         -2.2134e-03, -1.1810e-03],\n",
      "        [-9.7713e-03,  3.9505e-03, -6.5397e-05,  2.3839e-03, -3.8160e-03,\n",
      "         -1.4962e-03,  1.3336e-03, -3.9130e-03, -6.3490e-03, -9.2764e-03,\n",
      "         -5.9911e-03, -5.8905e-04],\n",
      "        [-5.3327e-03,  3.6636e-04,  2.5212e-05,  2.6331e-04, -2.0999e-03,\n",
      "         -5.8360e-04,  2.3188e-03, -2.3005e-03, -3.9430e-03, -4.2561e-03,\n",
      "         -3.1215e-03,  3.8999e-04],\n",
      "        [-2.7956e-03,  1.5579e-02, -5.8761e-05,  1.2008e-03,  5.1957e-03,\n",
      "         -1.4481e-04, -1.0127e-02, -1.3517e-03, -2.3358e-04, -5.1718e-03,\n",
      "         -1.6505e-03,  1.5456e-04],\n",
      "        [-3.0936e-03, -2.7552e-04, -1.1280e-05,  8.5453e-04, -1.8421e-03,\n",
      "         -5.2384e-04,  1.5873e-03, -1.2125e-03, -2.1752e-03, -2.7255e-03,\n",
      "         -1.9852e-03, -2.0927e-04],\n",
      "        [ 8.9404e-04,  1.1152e-03, -1.6209e-05,  3.0619e-05,  8.8376e-04,\n",
      "          1.1921e-04, -1.2243e-03,  3.7226e-04,  7.8654e-04,  5.0039e-04,\n",
      "          5.1054e-04, -2.1633e-05],\n",
      "        [ 3.9191e-04, -3.2581e-04,  1.0744e-05, -5.8223e-04,  4.1815e-04,\n",
      "          1.8220e-04,  1.9903e-04,  7.1859e-05,  1.0509e-04,  6.4351e-04,\n",
      "          3.4652e-04,  3.6625e-04],\n",
      "        [-7.9975e-05,  7.8377e-04, -7.5503e-06,  6.1372e-04, -8.4420e-05,\n",
      "         -1.3938e-04, -6.6650e-04,  5.2849e-05,  1.7792e-04, -4.8860e-04,\n",
      "         -1.7065e-04, -3.7462e-04],\n",
      "        [-2.1441e-05, -4.4333e-03,  3.9611e-05, -1.0519e-03, -1.1992e-03,\n",
      "          1.6235e-04,  3.5238e-03, -1.0581e-04, -7.7395e-04,  1.2193e-03,\n",
      "          1.0341e-04,  6.3135e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[2.8644e-05, 6.5910e-05, 6.0345e-08, 1.4634e-05, 1.3798e-05, 1.4969e-06,\n",
      "         3.1377e-05, 7.1487e-06, 1.5561e-05, 2.9485e-05, 1.0358e-05, 6.8292e-06],\n",
      "        [5.3653e-04, 1.4309e-03, 9.1941e-07, 3.3910e-04, 4.1782e-04, 3.7401e-05,\n",
      "         7.9590e-04, 1.1894e-04, 2.9508e-04, 5.8061e-04, 2.1452e-04, 1.4338e-04],\n",
      "        [2.5973e-03, 3.8080e-03, 3.7267e-06, 9.1857e-04, 8.8715e-04, 8.3020e-05,\n",
      "         2.3636e-03, 6.9059e-04, 1.6439e-03, 1.9571e-03, 7.9801e-04, 4.5577e-04],\n",
      "        [2.0298e-04, 5.9229e-04, 3.6099e-07, 1.2182e-04, 1.1873e-04, 1.1998e-05,\n",
      "         2.7138e-04, 4.7862e-05, 1.0456e-04, 2.3280e-04, 7.8392e-05, 5.2937e-05],\n",
      "        [1.2642e-05, 2.1480e-05, 2.8746e-08, 6.9517e-06, 7.1011e-06, 6.8214e-07,\n",
      "         1.2754e-05, 3.3199e-06, 7.9280e-06, 1.0677e-05, 4.3080e-06, 3.3786e-06],\n",
      "        [3.4166e-03, 5.8287e-03, 5.1207e-06, 1.4271e-03, 1.1272e-03, 1.1391e-04,\n",
      "         3.4239e-03, 9.3708e-04, 2.1805e-03, 2.7066e-03, 1.0486e-03, 6.9629e-04],\n",
      "        [7.4037e-04, 1.8375e-03, 1.3575e-06, 5.4461e-04, 5.1689e-04, 4.7574e-05,\n",
      "         1.0731e-03, 1.9904e-04, 4.7379e-04, 6.9121e-04, 2.6447e-04, 2.5546e-04],\n",
      "        [7.3300e-04, 1.9268e-03, 1.2196e-06, 5.2740e-04, 3.3863e-04, 4.2701e-05,\n",
      "         8.9627e-04, 1.8815e-04, 4.0429e-04, 8.2490e-04, 2.7337e-04, 2.3214e-04],\n",
      "        [2.1008e-03, 3.5118e-03, 3.1099e-06, 8.6646e-04, 7.2639e-04, 7.3943e-05,\n",
      "         2.1225e-03, 5.5494e-04, 1.3209e-03, 1.7078e-03, 6.6324e-04, 4.0262e-04],\n",
      "        [1.5356e-03, 2.5848e-03, 2.5474e-06, 6.5599e-04, 9.9991e-04, 7.8957e-05,\n",
      "         1.6797e-03, 3.7035e-04, 9.2428e-04, 1.2341e-03, 5.5269e-04, 3.1852e-04],\n",
      "        [5.7676e-03, 7.3849e-03, 9.6205e-06, 2.2978e-03, 2.6325e-03, 2.4920e-04,\n",
      "         4.1954e-03, 1.4734e-03, 3.3721e-03, 4.5042e-03, 1.9878e-03, 1.2081e-03],\n",
      "        [1.3202e-04, 2.6699e-04, 2.2687e-07, 5.9083e-05, 8.4751e-05, 6.9586e-06,\n",
      "         1.5419e-04, 3.1190e-05, 7.6718e-05, 1.1713e-04, 4.7834e-05, 2.7602e-05],\n",
      "        [2.4197e-05, 5.0343e-05, 5.1107e-08, 2.0919e-05, 1.8784e-05, 1.8886e-06,\n",
      "         2.2926e-05, 6.2460e-06, 1.3490e-05, 2.5530e-05, 9.2258e-06, 1.0113e-05],\n",
      "        [1.1622e-05, 2.8924e-05, 2.4972e-08, 5.8242e-06, 5.7356e-06, 5.2022e-07,\n",
      "         1.4792e-05, 2.9145e-06, 6.6715e-06, 1.1447e-05, 4.1500e-06, 2.6428e-06],\n",
      "        [3.1192e-05, 1.1814e-04, 5.6080e-08, 2.3011e-05, 2.5814e-05, 1.8324e-06,\n",
      "         5.3847e-05, 8.6174e-06, 1.8236e-05, 3.3413e-05, 1.0715e-05, 1.1280e-05],\n",
      "        [5.9677e-04, 1.5504e-03, 9.8202e-07, 4.4604e-04, 3.6007e-04, 3.7881e-05,\n",
      "         8.3010e-04, 1.5647e-04, 3.5866e-04, 6.1953e-04, 2.1840e-04, 2.0032e-04]],\n",
      "       device='cuda:0')}, 12: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0009,  0.0055, -0.0047,  0.0004, -0.0009, -0.0016,  0.0115, -0.0133,\n",
      "         0.0021, -0.0035,  0.0162, -0.0024,  0.0021,  0.0147, -0.0018,  0.0009],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([4.7508e-03, 2.4286e-03, 2.5415e-03, 1.4863e-02, 3.0251e-03, 7.1177e-05,\n",
      "        2.2782e-02, 7.5049e-03, 2.7347e-04, 1.7638e-02, 3.7006e-03, 2.9258e-04,\n",
      "        3.1992e-03, 2.5038e-03, 1.1946e-04, 7.1473e-04], device='cuda:0')}, 13: {'step': tensor(19100.), 'exp_avg': tensor([[-2.6734e-05, -5.7325e-04, -5.1024e-05,  2.3626e-04,  9.3936e-06,\n",
      "         -2.0512e-04, -8.1977e-04, -2.0825e-05,  2.8541e-05, -9.3487e-04,\n",
      "         -1.2366e-03, -2.5213e-04,  8.2857e-05,  2.7990e-05, -7.8712e-06,\n",
      "         -7.3834e-04],\n",
      "        [ 1.4805e-04, -2.0176e-03, -8.8237e-03,  7.8472e-04, -2.5542e-04,\n",
      "          3.8785e-03, -7.6652e-04,  1.3855e-03, -5.0170e-03, -7.1409e-04,\n",
      "          2.1090e-03, -8.4622e-04,  5.8732e-05,  3.3624e-04, -3.0682e-04,\n",
      "         -7.3396e-04],\n",
      "        [ 5.2142e-05,  1.3723e-03,  2.8224e-03,  1.9417e-04,  8.1085e-05,\n",
      "         -1.7972e-03,  4.6286e-04,  2.0291e-03,  3.0632e-03,  2.9438e-03,\n",
      "          4.9859e-03,  3.1881e-04, -2.4683e-04, -3.7340e-04,  2.0491e-04,\n",
      "          5.6164e-04],\n",
      "        [ 3.0194e-04,  5.0969e-04, -8.7778e-03,  7.5001e-04, -2.8631e-04,\n",
      "          3.7792e-03,  1.8413e-03,  4.6033e-03, -3.2660e-03,  5.4244e-03,\n",
      "          1.2945e-02, -1.2371e-04, -3.7461e-04, -8.8762e-05, -1.9098e-04,\n",
      "          1.7804e-03],\n",
      "        [ 1.0209e-04,  1.2600e-03, -5.8441e-04,  3.7389e-04, -5.8266e-05,\n",
      "         -2.2386e-04,  1.0489e-03,  2.8368e-03,  1.3032e-03,  3.9783e-03,\n",
      "          7.8480e-03,  1.9279e-04, -2.7303e-04, -2.9365e-04,  4.0030e-05,\n",
      "          1.0687e-03],\n",
      "        [ 2.0536e-05, -2.8433e-04, -6.8190e-05,  1.1048e-04,  7.7483e-06,\n",
      "         -9.9284e-05, -4.5195e-04,  1.3440e-04,  1.7541e-05, -3.8842e-04,\n",
      "         -4.4796e-04, -1.2356e-04,  1.7324e-06,  1.5380e-05,  1.4289e-05,\n",
      "         -3.6104e-04],\n",
      "        [-5.1118e-04, -1.5673e-03,  6.6805e-03, -3.1191e-04,  2.3844e-04,\n",
      "         -2.9931e-03, -2.6288e-03, -4.9764e-03,  1.9939e-03, -6.8027e-03,\n",
      "         -1.4315e-02, -4.0513e-04,  7.5594e-04,  2.2566e-04,  1.2842e-05,\n",
      "         -2.6998e-03],\n",
      "        [ 6.3924e-05,  3.2241e-03,  9.2832e-03, -7.7163e-04,  2.3489e-04,\n",
      "         -4.3081e-03,  1.7036e-03,  6.1119e-04,  6.2972e-03,  3.7431e-03,\n",
      "          3.3130e-03,  1.2147e-03, -4.6132e-04, -5.6291e-04,  4.3164e-04,\n",
      "          1.7835e-03],\n",
      "        [-6.5238e-05, -1.6050e-04,  3.5990e-04,  3.1900e-04,  2.4283e-05,\n",
      "         -5.3681e-04, -5.6115e-04,  5.8968e-04,  7.2265e-04,  1.0419e-04,\n",
      "          6.4706e-04, -2.1764e-04,  1.0063e-04, -4.1584e-05, -1.0193e-05,\n",
      "         -4.9406e-04],\n",
      "        [ 9.1197e-05,  9.5368e-04, -6.7958e-04,  2.5861e-04, -1.7029e-05,\n",
      "          1.1466e-05,  8.0185e-04,  2.3352e-03,  8.5599e-04,  3.1727e-03,\n",
      "          6.2848e-03,  1.4987e-04, -1.9065e-04, -2.0440e-04,  5.9904e-05,\n",
      "          8.5504e-04],\n",
      "        [-4.8835e-05, -2.6328e-03, -1.0675e-02,  3.2573e-04, -3.1695e-04,\n",
      "          5.5804e-03, -1.7558e-04, -1.2201e-03, -7.6556e-03, -2.8716e-03,\n",
      "         -2.2946e-03, -8.4305e-04,  4.0516e-04,  6.2723e-04, -5.0603e-04,\n",
      "         -5.4526e-04],\n",
      "        [ 4.9105e-05,  3.4532e-04,  7.9066e-04,  1.6481e-04,  1.4504e-05,\n",
      "         -6.6566e-04, -7.1304e-05,  9.3640e-04,  1.0671e-03,  9.7630e-04,\n",
      "          1.8786e-03,  2.4230e-05, -1.0557e-04, -1.1066e-04,  5.0887e-05,\n",
      "          3.3650e-05],\n",
      "        [ 2.2488e-04,  3.4297e-04, -6.3510e-03,  9.7148e-04, -2.1613e-04,\n",
      "          2.2527e-03,  8.8120e-04,  4.4850e-03, -1.5582e-03,  4.6120e-03,\n",
      "          1.1327e-02, -2.6424e-04, -3.1307e-04, -1.7753e-04, -1.2575e-04,\n",
      "          9.0464e-04],\n",
      "        [-8.3067e-05, -1.7003e-03, -8.2906e-03,  8.0225e-04, -2.4554e-04,\n",
      "          3.6976e-03, -3.0128e-04,  9.7175e-04, -4.6451e-03, -3.7713e-04,\n",
      "          2.4130e-03, -8.2201e-04,  3.1383e-04,  2.7678e-04, -3.7697e-04,\n",
      "         -5.0248e-04],\n",
      "        [ 7.5971e-05, -1.3588e-04, -8.9534e-04,  2.6133e-04, -2.0731e-05,\n",
      "          1.3274e-04, -2.9141e-04,  1.0048e-03, -2.6505e-05,  5.5518e-04,\n",
      "          1.7159e-03, -1.4932e-04, -8.1084e-05, -1.9166e-05,  1.0030e-06,\n",
      "         -1.7885e-04],\n",
      "        [-1.5655e-04,  2.9562e-04,  3.6403e-03, -4.0559e-04,  1.1137e-04,\n",
      "         -1.4201e-03, -4.6765e-05, -1.7722e-03,  1.4393e-03, -1.3013e-03,\n",
      "         -3.9332e-03,  2.4314e-04,  1.4880e-04, -2.9001e-05,  7.8394e-05,\n",
      "         -1.2872e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[2.9710e-06, 1.2799e-04, 9.6790e-04, 4.5471e-05, 2.3259e-06, 2.8531e-04,\n",
      "         2.3068e-04, 3.0854e-04, 4.8739e-04, 3.6509e-04, 1.4019e-03, 3.4552e-05,\n",
      "         3.0043e-06, 1.5176e-06, 3.1081e-06, 1.6827e-04],\n",
      "        [3.0549e-06, 1.0670e-04, 1.6030e-03, 5.4071e-05, 2.4347e-06, 3.8542e-04,\n",
      "         1.8847e-04, 5.6473e-04, 6.0990e-04, 3.9823e-04, 2.0850e-03, 3.6955e-05,\n",
      "         2.8275e-06, 2.1736e-06, 3.1523e-06, 1.3908e-04],\n",
      "        [6.0222e-06, 2.5721e-04, 2.7494e-03, 9.3664e-05, 4.8567e-06, 6.4731e-04,\n",
      "         4.2129e-04, 8.1780e-04, 1.0170e-03, 8.9796e-04, 3.9340e-03, 7.3256e-05,\n",
      "         5.8026e-06, 3.2942e-06, 6.4500e-06, 3.4366e-04],\n",
      "        [7.3259e-06, 3.0939e-04, 3.8664e-03, 1.3706e-04, 6.2276e-06, 1.1358e-03,\n",
      "         6.5208e-04, 8.4202e-04, 1.8190e-03, 7.3230e-04, 3.2805e-03, 9.9463e-05,\n",
      "         1.0133e-05, 5.3493e-06, 8.4063e-06, 4.6844e-04],\n",
      "        [1.9386e-06, 8.6618e-05, 9.7339e-04, 4.1812e-05, 1.5231e-06, 2.5792e-04,\n",
      "         1.7583e-04, 2.7983e-04, 4.0986e-04, 1.9336e-04, 9.4480e-04, 2.9658e-05,\n",
      "         2.6381e-06, 1.3540e-06, 2.1146e-06, 1.2729e-04],\n",
      "        [1.0484e-07, 4.5228e-06, 4.9294e-05, 2.0124e-06, 9.0316e-08, 1.2505e-05,\n",
      "         1.0740e-05, 1.2234e-05, 1.7967e-05, 1.1831e-05, 5.1393e-05, 1.5328e-06,\n",
      "         1.3169e-07, 5.9754e-08, 1.1770e-07, 8.3348e-06],\n",
      "        [8.7464e-06, 2.7450e-04, 3.5557e-03, 1.4296e-04, 6.8636e-06, 1.0983e-03,\n",
      "         6.0602e-04, 9.7148e-04, 1.7987e-03, 7.5499e-04, 3.5680e-03, 9.5047e-05,\n",
      "         1.0687e-05, 5.6485e-06, 9.1317e-06, 4.2909e-04],\n",
      "        [4.2123e-06, 1.2796e-04, 1.5918e-03, 7.2888e-05, 3.4578e-06, 4.3211e-04,\n",
      "         2.6949e-04, 6.9667e-04, 7.1349e-04, 4.6858e-04, 2.3757e-03, 4.6629e-05,\n",
      "         3.9741e-06, 2.7750e-06, 4.2958e-06, 1.8375e-04],\n",
      "        [2.3886e-07, 1.2550e-05, 1.4048e-04, 5.4581e-06, 2.0763e-07, 3.7952e-05,\n",
      "         2.8377e-05, 3.2901e-05, 5.7111e-05, 3.0219e-05, 1.3039e-04, 4.1865e-06,\n",
      "         3.7890e-07, 1.8545e-07, 3.2176e-07, 2.0492e-05],\n",
      "        [7.1833e-06, 2.4085e-04, 2.3216e-03, 1.1970e-04, 5.7241e-06, 7.6852e-04,\n",
      "         5.4832e-04, 8.1367e-04, 1.2907e-03, 7.0915e-04, 3.1047e-03, 7.8346e-05,\n",
      "         8.0738e-06, 4.3523e-06, 7.2043e-06, 3.7339e-04],\n",
      "        [6.6154e-06, 3.0047e-04, 3.0808e-03, 1.0583e-04, 5.7936e-06, 7.8292e-04,\n",
      "         4.2664e-04, 9.9292e-04, 1.3357e-03, 9.2005e-04, 3.9696e-03, 8.3439e-05,\n",
      "         6.4154e-06, 4.0497e-06, 6.6430e-06, 3.3401e-04],\n",
      "        [3.7495e-07, 1.3850e-05, 1.7688e-04, 5.9011e-06, 3.0558e-07, 4.1918e-05,\n",
      "         2.3994e-05, 4.9858e-05, 6.6229e-05, 4.2567e-05, 2.0110e-04, 4.4642e-06,\n",
      "         3.5767e-07, 2.3244e-07, 4.9182e-07, 1.8785e-05],\n",
      "        [4.5115e-06, 2.1197e-04, 2.9304e-03, 1.0300e-04, 3.6097e-06, 6.4594e-04,\n",
      "         4.0872e-04, 8.1424e-04, 9.5172e-04, 5.7756e-04, 3.1339e-03, 7.8047e-05,\n",
      "         5.9763e-06, 3.5519e-06, 6.0510e-06, 3.0547e-04],\n",
      "        [4.1055e-06, 1.8489e-04, 2.2273e-03, 7.7324e-05, 3.4486e-06, 5.3159e-04,\n",
      "         2.8508e-04, 7.0709e-04, 8.6642e-04, 5.5387e-04, 2.6795e-03, 5.7248e-05,\n",
      "         4.4022e-06, 2.7345e-06, 4.4892e-06, 2.1942e-04],\n",
      "        [9.6204e-08, 3.1491e-06, 3.9290e-05, 1.5373e-06, 7.8417e-08, 9.8207e-06,\n",
      "         7.1366e-06, 1.3464e-05, 1.4905e-05, 1.0767e-05, 5.1788e-05, 1.1467e-06,\n",
      "         9.1342e-08, 5.2292e-08, 1.2032e-07, 5.2945e-06],\n",
      "        [1.0209e-06, 4.6957e-05, 4.1725e-04, 1.7710e-05, 8.1598e-07, 1.0014e-04,\n",
      "         8.0056e-05, 1.4427e-04, 1.6226e-04, 1.5599e-04, 6.6116e-04, 1.3904e-05,\n",
      "         1.1106e-06, 5.5838e-07, 1.1043e-06, 6.2433e-05]], device='cuda:0')}, 14: {'step': tensor(19100.), 'exp_avg': tensor([ 8.3154e-05,  7.6576e-03, -9.1385e-03,  9.7536e-04, -7.7551e-03,\n",
      "         2.9044e-03,  1.0906e-02, -2.9812e-03, -1.7301e-02,  4.8104e-03,\n",
      "        -1.9664e-02,  4.2242e-03, -6.9807e-03, -1.2384e-02, -2.6636e-03,\n",
      "         3.1322e-03], device='cuda:0'), 'exp_avg_sq': tensor([0.0009, 0.0018, 0.0013, 0.0012, 0.0208, 0.0003, 0.0008, 0.0031, 0.0167,\n",
      "        0.0009, 0.0034, 0.0018, 0.0018, 0.0063, 0.0005, 0.0116],\n",
      "       device='cuda:0')}, 15: {'step': tensor(19100.), 'exp_avg': tensor([[-5.2851e-04, -1.7594e-03,  1.6400e-03,  1.0611e-03, -2.1099e-05,\n",
      "          1.3457e-05, -1.8819e-03,  1.7638e-03, -1.8507e-04,  1.1372e-03,\n",
      "         -1.9010e-03,  1.6510e-05,  2.8711e-04, -8.7347e-04,  9.1580e-05,\n",
      "          2.2914e-04],\n",
      "        [-3.6035e-03, -1.7493e-03,  1.1098e-03,  5.9922e-03,  1.8839e-03,\n",
      "         -1.0051e-04, -9.0698e-03,  5.0516e-03, -1.0027e-03,  4.7680e-03,\n",
      "         -2.4656e-03,  2.9512e-04, -3.7710e-04,  3.9159e-04, -4.8530e-05,\n",
      "         -2.8589e-04],\n",
      "        [ 1.2152e-03,  2.9946e-03, -2.1476e-03, -9.7339e-04,  4.6560e-04,\n",
      "         -8.9033e-05,  3.7660e-03, -3.7607e-03,  5.0777e-05, -1.6191e-03,\n",
      "          3.3477e-03, -7.0784e-05,  3.2421e-04,  1.5026e-03, -1.6940e-05,\n",
      "         -1.7782e-04],\n",
      "        [ 2.5258e-04, -1.6964e-04, -4.1969e-04, -2.0029e-03, -1.0603e-03,\n",
      "          4.0470e-05,  1.5010e-03, -2.2247e-04,  3.6783e-04, -1.0561e-03,\n",
      "          1.2311e-04, -9.6181e-05, -1.0997e-03, -3.9338e-04,  3.1160e-05,\n",
      "          3.7897e-05],\n",
      "        [-4.9259e-03, -8.9093e-04, -2.2783e-03,  8.7010e-04, -1.2463e-03,\n",
      "         -1.7712e-04, -5.8623e-03,  4.3056e-03, -4.2844e-04,  2.7858e-03,\n",
      "         -3.4125e-05, -1.5783e-04, -7.0031e-03,  4.0405e-04,  1.5175e-04,\n",
      "         -4.4607e-04],\n",
      "        [ 2.9165e-04, -1.5606e-03,  1.1837e-03, -1.2194e-03, -9.4936e-04,\n",
      "          9.5521e-05,  3.6799e-04,  9.5064e-04,  2.6270e-04, -3.6536e-04,\n",
      "         -1.5235e-03, -6.8217e-05, -1.6967e-04, -1.2072e-03,  4.4935e-05,\n",
      "          2.4791e-04],\n",
      "        [-1.4772e-04,  1.4390e-03, -1.8831e-03, -6.5285e-04,  9.4455e-05,\n",
      "          2.3069e-05,  2.4707e-05, -3.8403e-04,  2.9162e-04, -6.9516e-04,\n",
      "          1.3773e-03,  9.2679e-05, -6.0126e-04,  8.9215e-04, -2.1771e-04,\n",
      "         -4.2082e-04],\n",
      "        [ 4.5682e-03,  3.1845e-03, -1.7238e-03, -5.7802e-03, -1.0621e-03,\n",
      "          1.0754e-04,  1.0219e-02, -6.6232e-03,  1.0295e-03, -5.4437e-03,\n",
      "          3.6412e-03, -1.9424e-04,  1.9679e-03,  3.9646e-04, -8.2333e-05,\n",
      "          1.0464e-04],\n",
      "        [-3.7197e-03,  5.8573e-03, -6.4991e-03,  3.5308e-03,  2.1363e-03,\n",
      "         -4.2890e-04, -3.2698e-03, -1.7860e-03, -1.0820e-03,  2.1247e-03,\n",
      "          6.4016e-03,  6.9185e-05, -4.4380e-03,  4.7503e-03, -1.6176e-04,\n",
      "         -1.2296e-03],\n",
      "        [ 1.7072e-03, -9.0914e-04,  1.6807e-03, -8.5485e-04, -1.5299e-04,\n",
      "          1.3514e-04,  1.7485e-03, -4.7753e-04,  3.2893e-04, -9.5001e-04,\n",
      "         -1.2207e-03, -3.6990e-06,  2.2396e-03, -9.8078e-04, -5.7333e-05,\n",
      "          3.2327e-04],\n",
      "        [-3.2907e-03,  4.4176e-03, -5.5950e-03,  1.4175e-03,  7.1092e-04,\n",
      "         -3.6787e-04, -1.7756e-03, -1.3844e-03, -7.7531e-04,  1.2489e-03,\n",
      "          5.2760e-03, -3.3045e-05, -5.2123e-03,  3.4643e-03,  1.6209e-05,\n",
      "         -9.1793e-04],\n",
      "        [ 3.1788e-03,  9.3002e-04,  4.0848e-04, -2.4710e-03, -1.2823e-04,\n",
      "          1.4977e-04,  5.1716e-03, -3.1710e-03,  6.8089e-04, -2.8091e-03,\n",
      "          7.7792e-04, -6.6066e-05,  2.9197e-03, -3.2050e-04, -1.3923e-04,\n",
      "          2.2016e-04],\n",
      "        [ 2.3387e-03, -1.1685e-03,  1.8490e-03, -2.9002e-03, -1.2949e-03,\n",
      "          1.1141e-04,  4.6433e-03, -1.7083e-03,  4.2896e-04, -2.0478e-03,\n",
      "         -8.7080e-04, -2.2759e-04,  1.2685e-03, -1.5991e-03,  1.7470e-04,\n",
      "          6.0681e-04],\n",
      "        [ 5.8592e-05,  7.9073e-03, -7.6428e-03, -1.4848e-03,  1.0684e-03,\n",
      "         -2.9803e-04,  4.6739e-03, -6.5054e-03, -8.4051e-05, -2.4606e-03,\n",
      "          8.7697e-03,  1.7239e-05, -2.6626e-03,  4.7137e-03, -1.9092e-04,\n",
      "         -1.1203e-03],\n",
      "        [ 2.4137e-04,  4.0343e-03, -4.0004e-03, -1.2587e-03,  3.6425e-04,\n",
      "         -1.2620e-04,  2.8197e-03, -3.4344e-03,  9.8250e-05, -1.6425e-03,\n",
      "          4.4826e-03,  1.0202e-05, -1.3115e-03,  2.3287e-03, -1.0484e-04,\n",
      "         -5.8697e-04],\n",
      "        [ 3.3201e-03, -1.3466e-03,  1.2573e-03, -6.3770e-03, -2.8315e-03,\n",
      "          2.5113e-04,  7.0957e-03, -2.0020e-03,  1.2469e-03, -4.2016e-03,\n",
      "         -8.9827e-04, -2.4565e-04,  4.6619e-04, -2.3170e-03,  8.2745e-05,\n",
      "          5.8996e-04]], device='cuda:0'), 'exp_avg_sq': tensor([[8.6412e-05, 1.0917e-04, 1.2062e-04, 1.3419e-04, 4.9323e-05, 6.4586e-07,\n",
      "         2.6336e-04, 1.4073e-04, 4.9581e-06, 7.6258e-05, 1.3574e-04, 1.1869e-06,\n",
      "         1.4751e-04, 5.0083e-05, 9.9709e-07, 5.9324e-06],\n",
      "        [3.5780e-04, 3.9931e-04, 3.9454e-04, 7.1595e-04, 2.0448e-04, 2.7592e-06,\n",
      "         1.2830e-03, 6.5928e-04, 2.2237e-05, 4.2141e-04, 4.9742e-04, 3.3676e-06,\n",
      "         6.0210e-04, 1.4844e-04, 3.8833e-06, 1.3771e-05],\n",
      "        [2.0886e-04, 2.3146e-04, 2.2842e-04, 4.3481e-04, 1.1959e-04, 1.6108e-06,\n",
      "         7.7653e-04, 3.7075e-04, 1.4754e-05, 2.5332e-04, 2.8984e-04, 2.2549e-06,\n",
      "         3.0252e-04, 9.5326e-05, 2.4737e-06, 9.0805e-06],\n",
      "        [1.9914e-04, 2.5937e-04, 2.9218e-04, 2.7230e-04, 9.0730e-05, 1.4967e-06,\n",
      "         6.0259e-04, 3.5251e-04, 1.0634e-05, 1.8509e-04, 3.3205e-04, 2.1067e-06,\n",
      "         3.4593e-04, 1.0183e-04, 2.3740e-06, 1.1194e-05],\n",
      "        [1.0118e-03, 1.5658e-03, 1.7489e-03, 1.3379e-03, 5.8303e-04, 7.4295e-06,\n",
      "         3.0804e-03, 1.7968e-03, 5.9010e-05, 8.3751e-04, 1.9512e-03, 1.8138e-05,\n",
      "         1.6817e-03, 7.1551e-04, 1.5850e-05, 9.7860e-05],\n",
      "        [4.3695e-05, 5.6224e-05, 7.2820e-05, 6.9205e-05, 2.3735e-05, 3.4898e-07,\n",
      "         1.2154e-04, 6.8053e-05, 2.7916e-06, 3.8315e-05, 7.3846e-05, 5.4116e-07,\n",
      "         9.6574e-05, 2.3781e-05, 6.0399e-07, 2.4993e-06],\n",
      "        [1.2666e-04, 1.1789e-04, 1.1555e-04, 1.8089e-04, 6.0327e-05, 8.1503e-07,\n",
      "         3.8829e-04, 2.1775e-04, 6.4799e-06, 1.1365e-04, 1.4442e-04, 1.2573e-06,\n",
      "         2.2801e-04, 4.4307e-05, 1.2915e-06, 4.9364e-06],\n",
      "        [5.3567e-04, 6.7234e-04, 6.6118e-04, 9.6566e-04, 2.8142e-04, 4.3005e-06,\n",
      "         1.9293e-03, 1.0465e-03, 3.3449e-05, 6.0424e-04, 8.5009e-04, 5.8884e-06,\n",
      "         8.6104e-04, 2.4767e-04, 6.0412e-06, 2.1450e-05],\n",
      "        [8.8569e-04, 1.5948e-03, 1.4055e-03, 1.6415e-03, 6.7990e-04, 6.5324e-06,\n",
      "         3.0226e-03, 1.8875e-03, 6.1103e-05, 8.3837e-04, 1.8862e-03, 1.5412e-05,\n",
      "         1.3622e-03, 7.4016e-04, 1.7956e-05, 6.9883e-05],\n",
      "        [1.3639e-04, 1.0893e-04, 1.1290e-04, 1.8968e-04, 5.0252e-05, 9.0578e-07,\n",
      "         4.5251e-04, 2.2085e-04, 7.5136e-06, 1.3405e-04, 1.3909e-04, 1.4253e-06,\n",
      "         1.9067e-04, 4.1122e-05, 1.3406e-06, 5.5724e-06],\n",
      "        [5.5406e-04, 1.0920e-03, 1.1412e-03, 1.1795e-03, 4.4131e-04, 3.9589e-06,\n",
      "         1.7561e-03, 9.4637e-04, 3.6263e-05, 5.6354e-04, 1.3173e-03, 6.6201e-06,\n",
      "         8.0194e-04, 5.7383e-04, 8.4069e-06, 4.6442e-05],\n",
      "        [3.1336e-04, 4.3261e-04, 4.3480e-04, 5.5538e-04, 1.6778e-04, 2.5225e-06,\n",
      "         1.1236e-03, 6.2673e-04, 2.0096e-05, 3.5549e-04, 5.5904e-04, 3.2643e-06,\n",
      "         5.0784e-04, 1.6336e-04, 3.5850e-06, 1.3302e-05],\n",
      "        [3.4879e-04, 2.8643e-04, 3.0464e-04, 5.8786e-04, 1.4994e-04, 2.4975e-06,\n",
      "         1.2145e-03, 5.7847e-04, 1.9322e-05, 3.7408e-04, 3.7243e-04, 3.4393e-06,\n",
      "         5.2634e-04, 1.0267e-04, 3.5711e-06, 1.1701e-05],\n",
      "        [5.8051e-04, 7.1485e-04, 8.6577e-04, 1.0539e-03, 3.2709e-04, 3.9451e-06,\n",
      "         1.8153e-03, 8.3703e-04, 3.4608e-05, 5.6088e-04, 9.1595e-04, 8.0740e-06,\n",
      "         9.3029e-04, 3.6180e-04, 7.4613e-06, 3.3634e-05],\n",
      "        [8.8017e-05, 8.6098e-05, 8.4113e-05, 1.6802e-04, 5.2527e-05, 7.1408e-07,\n",
      "         2.9514e-04, 1.3862e-04, 6.3260e-06, 9.3642e-05, 1.0376e-04, 1.0216e-06,\n",
      "         1.3630e-04, 4.0137e-05, 9.9295e-07, 4.1439e-06],\n",
      "        [6.8380e-04, 1.2681e-03, 1.5640e-03, 8.5640e-04, 4.6695e-04, 5.1250e-06,\n",
      "         1.5582e-03, 1.0733e-03, 3.1864e-05, 4.0615e-04, 1.5612e-03, 1.0685e-05,\n",
      "         1.3923e-03, 6.4550e-04, 1.3762e-05, 8.0631e-05]], device='cuda:0')}, 16: {'step': tensor(19100.), 'exp_avg': tensor([ 0.0111, -0.0023, -0.0090, -0.0023, -0.0063, -0.0030, -0.0014,  0.0067,\n",
      "        -0.0100, -0.0023, -0.0052, -0.0082,  0.0013, -0.0043, -0.0051,  0.0028,\n",
      "        -0.0061,  0.0010, -0.0056, -0.0012, -0.0130, -0.0115, -0.0061, -0.0019,\n",
      "        -0.0035,  0.0004,  0.0056,  0.0028, -0.0095, -0.0082, -0.0082,  0.0007],\n",
      "       device='cuda:0'), 'exp_avg_sq': tensor([0.0127, 0.0013, 0.0022, 0.0019, 0.0100, 0.0003, 0.0004, 0.0040, 0.0010,\n",
      "        0.0006, 0.0021, 0.0023, 0.0007, 0.0021, 0.0061, 0.0027, 0.0008, 0.0019,\n",
      "        0.0006, 0.0004, 0.0048, 0.0032, 0.0065, 0.0033, 0.0003, 0.0002, 0.0062,\n",
      "        0.0021, 0.0012, 0.0072, 0.0018, 0.0004], device='cuda:0')}, 17: {'step': tensor(19100.), 'exp_avg': tensor([[-9.9000e-04, -5.4578e-04,  9.4437e-04, -7.0028e-03, -2.4650e-02,\n",
      "         -2.0137e-03, -1.6943e-03, -3.1525e-03, -2.8745e-02, -4.7508e-03,\n",
      "          4.2496e-03,  9.0983e-04,  1.1335e-03, -9.8323e-03,  1.1881e-03,\n",
      "          2.2632e-02],\n",
      "        [ 5.4147e-04,  2.4694e-03, -9.1820e-04,  7.1721e-04,  1.0369e-03,\n",
      "          2.1692e-04,  1.0328e-03, -1.7229e-03, -5.8676e-04,  9.0423e-04,\n",
      "         -2.5364e-03, -1.0857e-03, -9.0394e-04,  4.3655e-04, -2.3546e-04,\n",
      "         -4.1145e-03],\n",
      "        [ 2.2866e-05,  2.0948e-03, -6.5855e-04,  1.5309e-03,  2.2641e-04,\n",
      "          3.4926e-04,  3.8045e-04, -6.3616e-04, -5.1962e-03,  7.8260e-04,\n",
      "         -6.8257e-03,  2.6477e-04,  2.2569e-04, -3.2457e-03, -4.3943e-04,\n",
      "         -4.6037e-03],\n",
      "        [ 3.2588e-04,  1.7447e-03, -8.4200e-04,  5.6408e-04,  1.3338e-03,\n",
      "          4.8268e-05,  6.7055e-04, -1.8227e-03, -2.0196e-04,  5.9083e-04,\n",
      "         -1.4686e-03, -1.3834e-03, -8.7744e-04,  6.2595e-04, -1.8185e-04,\n",
      "         -3.2933e-03],\n",
      "        [-6.7029e-05,  2.0941e-03, -1.3936e-03,  2.2469e-03,  2.5430e-03,\n",
      "          1.8380e-04, -6.4080e-05, -2.6508e-03, -6.5867e-03,  5.3911e-04,\n",
      "         -7.5648e-03, -1.6056e-03, -3.5336e-04, -3.6507e-03, -4.6941e-04,\n",
      "         -6.2090e-03],\n",
      "        [ 5.7233e-04,  2.3267e-03, -7.6070e-04, -3.8852e-04, -8.1569e-04,\n",
      "         -2.5632e-05,  1.1739e-03, -1.9894e-03,  1.5416e-04,  6.2686e-04,\n",
      "          3.4106e-04, -1.3185e-03, -1.1814e-03,  1.6656e-03, -1.0499e-04,\n",
      "         -1.7277e-03],\n",
      "        [ 8.0947e-05,  5.2232e-05,  1.7158e-04, -7.4697e-04, -1.4320e-03,\n",
      "         -8.9916e-05,  2.3618e-04,  1.4652e-04,  5.9951e-04, -6.0086e-05,\n",
      "          1.6358e-03,  1.7778e-04, -8.7341e-05,  7.9643e-04,  8.3862e-05,\n",
      "          1.5032e-03],\n",
      "        [ 1.1721e-03,  4.6040e-03, -2.2427e-03, -3.1155e-04,  7.2700e-05,\n",
      "         -1.0382e-04,  1.8723e-03, -5.8255e-03, -1.1793e-03,  9.1725e-04,\n",
      "          4.3702e-04, -4.4498e-03, -2.9237e-03,  2.9800e-03, -2.9099e-04,\n",
      "         -4.2163e-03],\n",
      "        [-1.0385e-04,  1.0897e-03, -4.1195e-04,  2.0857e-03,  2.5131e-03,\n",
      "          4.5133e-04,  4.2424e-05,  3.2340e-04, -2.4088e-03,  8.2915e-04,\n",
      "         -6.5343e-03,  5.6384e-04,  4.9410e-04, -2.6947e-03, -4.5357e-04,\n",
      "         -5.3378e-03],\n",
      "        [ 3.6427e-04,  1.2460e-03, -2.0894e-04, -9.0838e-04, -2.3226e-03,\n",
      "         -9.3024e-05,  7.3582e-04, -9.0021e-04, -6.4084e-04,  1.3924e-04,\n",
      "          1.2090e-03, -3.6979e-04, -5.7230e-04,  9.1760e-04,  6.4168e-05,\n",
      "          9.0115e-04],\n",
      "        [ 6.3651e-04,  3.1154e-03, -1.7664e-03,  7.1229e-04,  1.8087e-03,\n",
      "         -7.3358e-05,  1.0278e-03, -4.4068e-03, -1.9703e-03,  6.6794e-04,\n",
      "         -1.8600e-03, -3.3461e-03, -1.9244e-03,  9.3062e-04, -2.1575e-04,\n",
      "         -4.7940e-03],\n",
      "        [ 8.8357e-04,  4.1259e-03, -1.7700e-03,  1.2091e-03,  2.1490e-03,\n",
      "          3.3389e-04,  1.6062e-03, -3.5874e-03, -1.1552e-03,  1.4383e-03,\n",
      "         -3.9252e-03, -2.5117e-03, -1.7524e-03,  8.9798e-04, -4.4151e-04,\n",
      "         -7.0086e-03],\n",
      "        [ 4.1495e-04,  2.0642e-03, -1.2150e-03,  8.1977e-04,  2.1260e-03,\n",
      "          5.8641e-05,  6.5082e-04, -2.7564e-03, -5.9156e-04,  6.0952e-04,\n",
      "         -1.7531e-03, -2.2165e-03, -1.2471e-03,  6.8204e-04, -2.4769e-04,\n",
      "         -4.1385e-03],\n",
      "        [-4.2204e-04, -2.1295e-03,  1.9475e-03, -3.4554e-03, -9.8340e-03,\n",
      "         -4.7986e-04, -5.7127e-04,  3.1597e-03, -3.9260e-03, -1.5747e-03,\n",
      "          4.9837e-03,  3.4275e-03,  1.5982e-03, -1.5757e-03,  6.6367e-04,\n",
      "          1.1324e-02],\n",
      "        [-1.6062e-04,  1.2989e-03, -1.5019e-03,  2.4439e-03,  4.1056e-03,\n",
      "          4.7315e-05, -4.8839e-04, -3.1325e-03, -5.6777e-03,  1.8513e-04,\n",
      "         -6.5077e-03, -2.3842e-03, -5.4554e-04, -3.2293e-03, -3.6238e-04,\n",
      "         -6.0213e-03],\n",
      "        [ 2.3122e-04,  2.0414e-03, -1.2511e-03,  9.5866e-04,  2.7219e-04,\n",
      "         -8.0167e-05,  1.8605e-04, -2.9170e-03, -5.7327e-03,  6.1175e-05,\n",
      "         -3.9382e-03, -1.8739e-03, -7.8656e-04, -2.1113e-03, -1.8220e-04,\n",
      "         -3.0762e-03],\n",
      "        [ 6.7024e-04,  2.5874e-03, -3.0428e-04, -1.9106e-03, -5.0870e-03,\n",
      "         -2.0585e-04,  1.5337e-03, -1.5740e-03, -1.3238e-03,  3.2129e-04,\n",
      "          2.3149e-03, -4.5351e-04, -1.0615e-03,  1.7821e-03,  1.2823e-04,\n",
      "          1.8898e-03],\n",
      "        [ 6.7491e-04,  3.9641e-04,  1.1727e-04, -1.2911e-03,  9.7481e-04,\n",
      "          2.5300e-05,  1.2158e-03,  8.2860e-05,  9.3935e-03,  6.7877e-04,\n",
      "          6.4524e-03, -6.7632e-04, -1.1877e-03,  6.0755e-03,  1.4569e-04,\n",
      "          4.9085e-04],\n",
      "        [ 1.7885e-04,  1.4337e-03, -3.6482e-04,  2.9621e-04, -6.6861e-04,\n",
      "          1.1295e-04,  5.4391e-04, -5.8481e-04, -1.9213e-03,  4.4321e-04,\n",
      "         -2.2316e-03, -5.0078e-05, -1.7829e-04, -7.0201e-04, -1.7470e-04,\n",
      "         -1.7556e-03],\n",
      "        [ 4.9375e-05, -1.7093e-04,  4.3044e-04, -8.5411e-04, -2.0007e-03,\n",
      "         -7.2664e-05,  2.1547e-04,  7.9501e-04,  6.4741e-04, -1.1603e-04,\n",
      "          1.6863e-03,  7.7257e-04,  1.7224e-04,  5.9880e-04,  1.3284e-04,\n",
      "          2.1121e-03],\n",
      "        [-2.6155e-04,  2.6690e-03, -1.8455e-03,  5.9164e-03,  9.4790e-03,\n",
      "          8.6250e-04, -2.1456e-04, -1.6819e-03, -5.8955e-03,  1.7748e-03,\n",
      "         -1.5960e-02, -1.0266e-03,  2.9335e-04, -6.0370e-03, -1.0655e-03,\n",
      "         -1.5274e-02],\n",
      "        [-7.7624e-05,  1.5123e-03, -6.2537e-04,  1.4944e-03,  8.3344e-04,\n",
      "          2.6214e-04,  7.2614e-05, -7.4765e-04, -4.7138e-03,  5.8357e-04,\n",
      "         -6.0703e-03, -1.3468e-05,  2.0763e-04, -3.0949e-03, -3.6967e-04,\n",
      "         -4.1590e-03],\n",
      "        [-3.4402e-04,  3.9558e-04,  1.1076e-04,  5.6947e-04, -1.6473e-03,\n",
      "          1.3504e-04, -2.6123e-04,  6.0871e-04, -5.3222e-03,  3.5238e-05,\n",
      "         -4.4593e-03,  1.2291e-03,  9.0936e-04, -3.5024e-03, -1.5436e-04,\n",
      "         -6.7261e-04],\n",
      "        [ 6.1350e-04,  1.7226e-03, -6.3327e-04,  3.6688e-04,  2.5909e-03,\n",
      "          2.2056e-04,  1.0776e-03, -1.2253e-03,  4.8609e-03,  1.0454e-03,\n",
      "          7.9712e-04, -1.2920e-03, -1.2499e-03,  3.1645e-03, -1.8036e-04,\n",
      "         -3.8329e-03],\n",
      "        [ 5.3676e-05,  3.7820e-04, -2.2935e-05, -7.9033e-05, -6.1216e-04,\n",
      "          2.3458e-05,  1.8209e-04,  1.4229e-05, -5.2195e-04,  1.1240e-04,\n",
      "         -4.0091e-04,  1.6452e-04,  4.9451e-06, -1.6188e-04, -3.6373e-05,\n",
      "         -8.9316e-05],\n",
      "        [-9.2167e-05, -7.3761e-04,  3.3005e-04, -1.5208e-04,  2.1103e-04,\n",
      "          1.9902e-05, -1.9132e-04,  7.9810e-04,  1.4802e-03, -9.7015e-05,\n",
      "          1.0001e-03,  4.5169e-04,  2.5257e-04,  4.1142e-04,  5.7006e-05,\n",
      "          8.3090e-04],\n",
      "        [ 1.2027e-04, -1.2833e-04,  4.8095e-04, -2.4851e-03, -6.0928e-03,\n",
      "         -4.4782e-04,  1.5007e-04, -2.3595e-04, -2.4413e-03, -8.8943e-04,\n",
      "          4.4130e-03,  3.1045e-04, -1.2555e-04,  4.1353e-04,  4.0633e-04,\n",
      "          6.4662e-03],\n",
      "        [ 4.9785e-04,  1.3416e-04,  2.9660e-04, -1.7047e-03, -1.1924e-03,\n",
      "         -8.4755e-05,  9.0272e-04,  2.1643e-04,  6.0086e-03,  2.1978e-04,\n",
      "          5.8959e-03, -2.0415e-04, -7.6232e-04,  4.4050e-03,  2.4509e-04,\n",
      "          2.4350e-03],\n",
      "        [-1.4240e-04,  1.0595e-03, -2.8829e-04,  1.4650e-03,  6.1711e-04,\n",
      "          3.4286e-04, -2.0852e-05,  2.6905e-04, -4.0514e-03,  5.6546e-04,\n",
      "         -5.9816e-03,  7.6637e-04,  5.8743e-04, -3.2104e-03, -3.6913e-04,\n",
      "         -3.5647e-03],\n",
      "        [ 1.1401e-03,  6.9731e-03, -3.6738e-03,  5.1653e-03,  1.1212e-02,\n",
      "          8.8228e-04,  2.2738e-03, -6.3151e-03,  1.0247e-05,  3.2708e-03,\n",
      "         -1.2212e-02, -5.0717e-03, -2.8671e-03,  5.4938e-04, -1.2192e-03,\n",
      "         -1.9872e-02],\n",
      "        [ 1.0222e-03,  3.7590e-03, -1.7687e-03,  2.4909e-03,  6.9984e-03,\n",
      "          6.8934e-04,  1.7853e-03, -2.5022e-03,  5.0867e-03,  2.2455e-03,\n",
      "         -4.1841e-03, -2.4223e-03, -1.8175e-03,  2.9078e-03, -6.1941e-04,\n",
      "         -1.0808e-02],\n",
      "        [-8.9388e-06, -4.0631e-04,  2.9673e-04, -4.4277e-04, -6.3277e-04,\n",
      "         -2.5978e-05,  1.1317e-05,  5.7781e-04,  1.0941e-03, -8.9954e-05,\n",
      "          1.3294e-03,  4.0499e-04,  1.2709e-04,  5.7137e-04,  8.2983e-05,\n",
      "          1.2429e-03]], device='cuda:0'), 'exp_avg_sq': tensor([[4.6111e-05, 5.1890e-04, 1.4067e-04, 4.7374e-04, 4.3895e-03, 4.3803e-05,\n",
      "         1.4721e-04, 1.0533e-03, 1.4083e-02, 3.4359e-04, 2.9795e-03, 4.2951e-04,\n",
      "         1.6495e-04, 2.7806e-03, 3.6563e-05, 4.2474e-03],\n",
      "        [1.0686e-05, 1.0114e-04, 2.8370e-05, 1.1829e-04, 1.0693e-03, 6.5093e-06,\n",
      "         2.5695e-05, 1.7779e-04, 2.0118e-03, 5.9730e-05, 3.5453e-04, 8.4757e-05,\n",
      "         3.1187e-05, 3.5389e-04, 6.5416e-06, 9.4784e-04],\n",
      "        [1.1552e-05, 1.8874e-04, 3.7564e-05, 1.1569e-04, 5.2689e-04, 3.5772e-06,\n",
      "         3.3093e-05, 2.2123e-04, 4.4084e-04, 2.3965e-05, 6.8243e-04, 1.2796e-04,\n",
      "         4.6705e-05, 1.6878e-04, 4.1233e-06, 9.2297e-04],\n",
      "        [1.1117e-05, 1.7203e-04, 4.1354e-05, 1.5740e-04, 8.4004e-04, 4.6283e-06,\n",
      "         2.9352e-05, 2.3483e-04, 8.2702e-04, 3.8309e-05, 7.8449e-04, 1.3478e-04,\n",
      "         5.0633e-05, 2.3872e-04, 5.9465e-06, 1.2637e-03],\n",
      "        [3.9986e-05, 5.9044e-04, 2.2774e-04, 6.1866e-04, 3.1052e-03, 1.3538e-05,\n",
      "         1.3418e-04, 1.1519e-03, 1.5311e-03, 5.1280e-05, 2.3663e-03, 8.1193e-04,\n",
      "         1.9300e-04, 6.2665e-04, 1.3152e-05, 3.4592e-03],\n",
      "        [2.3736e-06, 2.6145e-05, 4.5371e-06, 2.1772e-05, 1.6463e-04, 1.1852e-06,\n",
      "         7.9456e-06, 3.0126e-05, 2.7918e-04, 9.0429e-06, 6.7592e-05, 1.3699e-05,\n",
      "         6.3880e-06, 5.8391e-05, 9.3196e-07, 1.4701e-04],\n",
      "        [2.7150e-06, 3.7157e-05, 7.7285e-06, 1.7910e-05, 9.5677e-05, 7.8176e-07,\n",
      "         7.2400e-06, 4.9906e-05, 1.1852e-04, 3.6390e-06, 1.0313e-04, 2.7931e-05,\n",
      "         1.0415e-05, 3.8890e-05, 7.0493e-07, 1.2020e-04],\n",
      "        [3.8298e-05, 4.8665e-04, 1.3617e-04, 4.2886e-04, 3.5626e-03, 2.2728e-05,\n",
      "         1.2910e-04, 6.5256e-04, 5.7603e-03, 2.1420e-04, 1.4546e-03, 4.5050e-04,\n",
      "         2.0358e-04, 1.5322e-03, 2.3885e-05, 3.8677e-03],\n",
      "        [9.7183e-06, 1.2848e-04, 2.9814e-05, 9.8112e-05, 4.0591e-04, 3.0232e-06,\n",
      "         2.4613e-05, 1.9232e-04, 2.9612e-04, 1.4068e-05, 5.3257e-04, 1.0943e-04,\n",
      "         4.2162e-05, 1.5757e-04, 2.4394e-06, 6.2699e-04],\n",
      "        [4.6313e-06, 6.1329e-05, 1.6196e-05, 4.3028e-05, 2.1535e-04, 1.2748e-06,\n",
      "         1.2586e-05, 1.0483e-04, 1.7829e-04, 8.0364e-06, 1.8989e-04, 6.1219e-05,\n",
      "         2.0750e-05, 6.0557e-05, 1.2178e-06, 2.7913e-04],\n",
      "        [1.2351e-05, 2.0859e-04, 7.7817e-05, 1.8824e-04, 7.7370e-04, 5.0601e-06,\n",
      "         3.5419e-05, 4.3222e-04, 1.0184e-03, 2.0464e-05, 9.5822e-04, 2.6070e-04,\n",
      "         6.3337e-05, 3.0024e-04, 4.1469e-06, 1.1676e-03],\n",
      "        [1.0449e-05, 1.1215e-04, 4.5888e-05, 1.1253e-04, 1.0076e-03, 1.0592e-05,\n",
      "         3.0019e-05, 3.0136e-04, 2.9884e-03, 7.7707e-05, 4.6807e-04, 1.5363e-04,\n",
      "         4.3549e-05, 5.4804e-04, 8.5960e-06, 9.6545e-04],\n",
      "        [2.4306e-06, 2.8407e-05, 1.1045e-05, 2.9272e-05, 2.8224e-04, 2.1558e-06,\n",
      "         8.4697e-06, 5.5423e-05, 5.8624e-04, 2.0567e-05, 9.2014e-05, 3.5667e-05,\n",
      "         1.1615e-05, 1.1990e-04, 2.6562e-06, 2.8997e-04],\n",
      "        [1.8741e-05, 2.5709e-04, 6.2861e-05, 1.8934e-04, 1.6211e-03, 9.1572e-06,\n",
      "         6.6888e-05, 3.4772e-04, 2.6270e-03, 9.1814e-05, 6.3016e-04, 1.9918e-04,\n",
      "         8.5455e-05, 5.9867e-04, 9.6449e-06, 1.6079e-03],\n",
      "        [3.6439e-05, 5.5201e-04, 1.9247e-04, 5.4398e-04, 2.3374e-03, 1.1898e-05,\n",
      "         1.0967e-04, 1.0388e-03, 1.9190e-03, 4.1286e-05, 2.4414e-03, 6.6218e-04,\n",
      "         1.6112e-04, 6.9208e-04, 9.6044e-06, 3.0236e-03],\n",
      "        [3.5262e-05, 4.0921e-04, 1.2569e-04, 5.2047e-04, 4.0171e-03, 1.8718e-05,\n",
      "         8.9802e-05, 6.6981e-04, 5.4382e-03, 1.7381e-04, 1.7048e-03, 3.9323e-04,\n",
      "         1.4811e-04, 1.1525e-03, 2.0066e-05, 4.0845e-03],\n",
      "        [3.5788e-06, 4.7922e-05, 9.1668e-06, 3.5094e-05, 2.1058e-04, 1.5940e-06,\n",
      "         1.4645e-05, 5.1433e-05, 2.7187e-04, 1.0150e-05, 1.4310e-04, 2.9357e-05,\n",
      "         1.1996e-05, 8.7334e-05, 1.3342e-06, 2.0714e-04],\n",
      "        [7.2424e-06, 1.2844e-04, 1.7366e-05, 5.7020e-05, 3.1689e-04, 2.5110e-06,\n",
      "         2.9687e-05, 1.0226e-04, 3.4094e-04, 2.8165e-05, 3.2535e-04, 5.5617e-05,\n",
      "         3.0628e-05, 1.2788e-04, 3.6988e-06, 5.6512e-04],\n",
      "        [4.3760e-06, 6.3174e-05, 1.4008e-05, 5.1543e-05, 2.1659e-04, 1.3932e-06,\n",
      "         1.3172e-05, 8.0670e-05, 1.4489e-04, 9.2025e-06, 2.7934e-04, 4.5406e-05,\n",
      "         1.7918e-05, 7.2655e-05, 1.3121e-06, 3.4630e-04],\n",
      "        [2.3261e-06, 3.7591e-05, 8.8516e-06, 2.2789e-05, 1.1009e-04, 7.5991e-07,\n",
      "         7.7172e-06, 4.9570e-05, 8.5608e-05, 4.1422e-06, 1.2687e-04, 2.8045e-05,\n",
      "         9.9795e-06, 3.6718e-05, 6.9857e-07, 1.6612e-04],\n",
      "        [2.3769e-05, 4.1389e-04, 9.0365e-05, 3.7115e-04, 1.3692e-03, 1.0433e-05,\n",
      "         6.2975e-05, 5.3397e-04, 7.8353e-04, 5.9828e-05, 2.2573e-03, 3.1881e-04,\n",
      "         1.2431e-04, 4.6933e-04, 9.1993e-06, 2.7730e-03],\n",
      "        [1.6619e-05, 3.2079e-04, 6.7583e-05, 2.4183e-04, 1.0663e-03, 5.8444e-06,\n",
      "         4.8863e-05, 3.6995e-04, 5.4043e-04, 5.1462e-05, 1.2933e-03, 2.1580e-04,\n",
      "         7.9655e-05, 2.2624e-04, 7.5938e-06, 2.1002e-03],\n",
      "        [2.6775e-05, 3.6862e-04, 1.3254e-04, 3.1856e-04, 1.9725e-03, 8.9641e-06,\n",
      "         8.1683e-05, 6.7029e-04, 9.8641e-04, 3.3337e-05, 1.1649e-03, 4.8646e-04,\n",
      "         1.2559e-04, 3.6162e-04, 8.6766e-06, 1.9294e-03],\n",
      "        [1.2699e-05, 1.4265e-04, 6.1282e-05, 1.3548e-04, 1.3957e-03, 9.3794e-06,\n",
      "         3.9542e-05, 3.3720e-04, 2.7461e-03, 8.9086e-05, 3.6345e-04, 2.1948e-04,\n",
      "         6.8625e-05, 5.5789e-04, 8.8795e-06, 1.3368e-03],\n",
      "        [2.1055e-06, 2.6271e-05, 7.1621e-06, 1.9230e-05, 1.2485e-04, 1.1435e-06,\n",
      "         5.4150e-06, 4.5093e-05, 2.1690e-04, 7.4345e-06, 8.6543e-05, 2.5629e-05,\n",
      "         8.8025e-06, 5.0745e-05, 1.0636e-06, 1.5077e-04],\n",
      "        [8.6882e-07, 1.2929e-05, 2.7892e-06, 6.6009e-06, 3.4516e-05, 3.0355e-07,\n",
      "         3.0670e-06, 1.5681e-05, 3.5540e-05, 2.0407e-06, 3.6686e-05, 8.9710e-06,\n",
      "         3.2588e-06, 1.2108e-05, 3.4580e-07, 4.8655e-05],\n",
      "        [1.9929e-05, 2.5690e-04, 1.2562e-04, 2.8417e-04, 1.5651e-03, 9.4401e-06,\n",
      "         4.7503e-05, 6.6465e-04, 1.4035e-03, 3.3803e-05, 1.1409e-03, 4.8267e-04,\n",
      "         1.1169e-04, 4.2100e-04, 8.2392e-06, 1.6352e-03],\n",
      "        [7.4891e-06, 7.6658e-05, 1.5765e-05, 5.9545e-05, 5.2450e-04, 5.9000e-06,\n",
      "         2.2183e-05, 1.1027e-04, 1.1724e-03, 3.6104e-05, 2.9042e-04, 5.9386e-05,\n",
      "         2.5119e-05, 2.5393e-04, 3.8516e-06, 5.0421e-04],\n",
      "        [3.9977e-06, 5.9409e-05, 1.4314e-05, 4.0418e-05, 1.9567e-04, 1.5724e-06,\n",
      "         1.2686e-05, 7.2953e-05, 1.4258e-04, 9.0932e-06, 2.4175e-04, 4.7144e-05,\n",
      "         1.6255e-05, 6.2096e-05, 1.5196e-06, 3.0638e-04],\n",
      "        [5.4110e-05, 9.3232e-04, 1.9813e-04, 8.0424e-04, 3.9845e-03, 2.1622e-05,\n",
      "         1.5341e-04, 1.1220e-03, 3.6286e-03, 2.0991e-04, 4.0947e-03, 6.1615e-04,\n",
      "         2.5021e-04, 1.0853e-03, 2.6620e-05, 6.8896e-03],\n",
      "        [1.0188e-05, 1.1883e-04, 4.2861e-05, 1.6435e-04, 1.2517e-03, 6.3631e-06,\n",
      "         2.8021e-05, 2.2340e-04, 1.8052e-03, 6.1549e-05, 5.2818e-04, 1.3032e-04,\n",
      "         3.9488e-05, 3.3661e-04, 6.8960e-06, 1.3127e-03],\n",
      "        [1.5045e-06, 2.2861e-05, 5.1590e-06, 1.1896e-05, 6.7703e-05, 4.8220e-07,\n",
      "         4.4385e-06, 2.8507e-05, 5.3104e-05, 3.0173e-06, 6.2646e-05, 1.7337e-05,\n",
      "         5.8549e-06, 1.7878e-05, 5.5611e-07, 9.7643e-05]], device='cuda:0')}, 18: {'step': tensor(19100.), 'exp_avg': tensor([[ 8.6773e-04,  2.4878e-04,  2.9484e-04,  ...,  1.0409e-03,\n",
      "          7.8878e-05,  6.5161e-05],\n",
      "        [ 1.3463e-02,  4.0527e-04,  2.6232e-03,  ...,  4.1274e-03,\n",
      "          1.8164e-03, -8.4601e-04],\n",
      "        [ 5.6052e-45,  5.6052e-45, -5.6052e-45,  ..., -5.6052e-45,\n",
      "          5.6052e-45, -5.6052e-45],\n",
      "        ...,\n",
      "        [ 4.5639e-04, -1.4928e-03,  1.9536e-04,  ..., -9.8863e-04,\n",
      "         -1.4419e-03, -3.4305e-05],\n",
      "        [ 3.3755e-04, -2.0854e-04,  3.5674e-04,  ...,  3.9735e-04,\n",
      "         -2.0184e-04,  4.2607e-05],\n",
      "        [ 5.6052e-45, -5.6052e-45,  5.6052e-45,  ...,  5.6052e-45,\n",
      "         -5.6052e-45, -5.6052e-45]], device='cuda:0'), 'exp_avg_sq': tensor([[3.7304e-05, 6.0406e-06, 3.0318e-06,  ..., 4.7535e-05, 3.5535e-06,\n",
      "         8.4367e-07],\n",
      "        [2.7417e-03, 1.1165e-04, 1.9643e-04,  ..., 1.9262e-03, 2.1842e-04,\n",
      "         1.9051e-05],\n",
      "        [3.5768e-09, 2.0786e-11, 5.2428e-10,  ..., 1.3850e-10, 2.7049e-10,\n",
      "         7.9000e-11],\n",
      "        ...,\n",
      "        [1.4879e-03, 7.7957e-05, 1.2962e-04,  ..., 1.2411e-03, 1.3003e-04,\n",
      "         1.4259e-05],\n",
      "        [4.7529e-05, 7.6118e-06, 1.1516e-05,  ..., 3.5975e-05, 1.8819e-05,\n",
      "         3.2241e-06],\n",
      "        [4.6791e-08, 5.1976e-09, 5.9069e-09,  ..., 3.4968e-08, 7.9669e-09,\n",
      "         4.9383e-10]], device='cuda:0')}, 19: {'step': tensor(19100.), 'exp_avg': tensor([-4.9466e-04,  1.6828e-02,  5.6052e-45, -9.2748e-05, -4.5007e-04,\n",
      "         1.9782e-03, -7.1124e-03, -1.0326e-03,  5.6052e-45,  5.6052e-45,\n",
      "         3.7482e-04, -2.3279e-04, -3.9230e-05, -1.2499e-02,  8.4016e-03,\n",
      "         1.8586e-25, -1.4003e-04,  1.1353e-02, -6.4108e-03,  5.6052e-45,\n",
      "         5.6052e-45,  1.8419e-03,  5.6052e-45,  5.6052e-45, -1.6894e-05,\n",
      "         2.0983e-25,  5.1634e-03,  5.6052e-45, -7.7691e-04,  4.9349e-04,\n",
      "         4.1684e-03,  5.6052e-45, -9.0339e-03,  2.3026e-02,  5.6052e-45,\n",
      "        -8.4480e-04, -1.1933e-03,  5.6052e-45, -5.0076e-04, -5.6052e-45,\n",
      "         4.7902e-04,  5.6052e-45, -7.1118e-03,  0.0000e+00, -7.8937e-03,\n",
      "        -3.3379e-03,  3.7603e-03, -1.0288e-02,  2.2469e-38,  5.6052e-45,\n",
      "         5.6052e-45, -3.8545e-05, -5.0534e-03,  5.6052e-45, -1.6205e-02,\n",
      "         5.6052e-45,  5.6052e-45,  5.6052e-45, -3.8005e-04,  5.6052e-45,\n",
      "        -6.1143e-04,  3.1955e-03,  9.6031e-04,  5.6052e-45], device='cuda:0'), 'exp_avg_sq': tensor([2.7726e-04, 4.8328e-03, 3.3845e-09, 2.4633e-04, 9.3718e-05, 2.5027e-04,\n",
      "        6.6507e-03, 8.8394e-04, 9.9893e-12, 8.7248e-09, 2.7721e-03, 9.6622e-05,\n",
      "        2.5998e-03, 3.6787e-03, 8.2281e-03, 5.5190e-06, 2.1955e-06, 4.6339e-03,\n",
      "        4.9034e-03, 1.0150e-10, 5.8605e-09, 2.3298e-04, 1.4800e-07, 1.3905e-08,\n",
      "        7.7264e-04, 2.6993e-05, 3.7117e-03, 9.6563e-11, 3.4240e-04, 1.5671e-03,\n",
      "        6.0243e-05, 2.7791e-07, 3.6282e-03, 4.2008e-03, 2.0697e-07, 1.3756e-04,\n",
      "        6.1847e-05, 8.2748e-14, 1.8436e-04, 8.8048e-07, 9.2125e-04, 1.2523e-08,\n",
      "        1.2261e-03, 0.0000e+00, 9.1040e-04, 1.5842e-03, 3.7885e-03, 4.8363e-03,\n",
      "        6.7922e-07, 1.9136e-06, 1.4492e-15, 2.8062e-09, 3.2019e-04, 1.8170e-08,\n",
      "        3.1854e-03, 2.7547e-11, 1.2574e-08, 1.5368e-11, 1.1168e-04, 1.7816e-09,\n",
      "        2.9716e-04, 3.4017e-03, 2.7248e-04, 1.6366e-07], device='cuda:0')}}\n",
      "param_groups \t [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]}]\n"
     ]
    }
   ],
   "source": [
    "# Print optimizer's state_dict\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'{model_name}_state_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, model_name+'.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1.014941930770874,\n",
       "  0.7574150562286377,\n",
       "  0.7180765271186829,\n",
       "  0.757636547088623,\n",
       "  0.3924352526664734,\n",
       "  0.6614164710044861,\n",
       "  0.5807397365570068,\n",
       "  0.5880841612815857,\n",
       "  0.7298559546470642,\n",
       "  0.5783298015594482,\n",
       "  0.7728349566459656,\n",
       "  0.31873348355293274,\n",
       "  0.6617896556854248,\n",
       "  0.6229442954063416,\n",
       "  0.935050368309021,\n",
       "  0.659864604473114,\n",
       "  0.660252034664154,\n",
       "  0.7406705617904663,\n",
       "  0.9769477248191833,\n",
       "  0.6398603916168213,\n",
       "  0.49660027027130127,\n",
       "  0.5416955351829529,\n",
       "  0.6801843643188477,\n",
       "  0.7598803043365479,\n",
       "  0.554585337638855,\n",
       "  0.3845713436603546,\n",
       "  0.7013891339302063,\n",
       "  0.5693373084068298,\n",
       "  0.6359130144119263,\n",
       "  0.9016086459159851,\n",
       "  0.5940358638763428,\n",
       "  0.36941611766815186,\n",
       "  0.7090832591056824,\n",
       "  1.1019935607910156,\n",
       "  0.4549536108970642,\n",
       "  0.9794498682022095,\n",
       "  0.5342934727668762,\n",
       "  1.2970455884933472,\n",
       "  0.3937184512615204,\n",
       "  0.7893402576446533,\n",
       "  0.6169090867042542,\n",
       "  1.0230070352554321,\n",
       "  1.0167659521102905,\n",
       "  0.6555982828140259,\n",
       "  1.0451072454452515,\n",
       "  1.240073323249817,\n",
       "  0.9302780628204346,\n",
       "  0.5579665899276733,\n",
       "  0.915648877620697,\n",
       "  0.8180795907974243,\n",
       "  0.792754054069519,\n",
       "  0.42690378427505493,\n",
       "  0.5574222207069397,\n",
       "  0.45804715156555176,\n",
       "  0.6705757975578308,\n",
       "  1.2129323482513428,\n",
       "  0.8355889320373535,\n",
       "  0.8932310938835144,\n",
       "  0.5331724286079407,\n",
       "  0.8864560723304749,\n",
       "  0.5990471839904785,\n",
       "  0.751425564289093,\n",
       "  0.5384098887443542,\n",
       "  1.1794627904891968,\n",
       "  0.6606758236885071,\n",
       "  0.7875714302062988,\n",
       "  0.6791741847991943,\n",
       "  1.0806463956832886,\n",
       "  1.3679566383361816,\n",
       "  0.4416879415512085,\n",
       "  0.5030132532119751,\n",
       "  0.8792884349822998,\n",
       "  0.5063016414642334,\n",
       "  0.7264665961265564,\n",
       "  0.9270033240318298,\n",
       "  1.0275413990020752,\n",
       "  0.6725084185600281,\n",
       "  0.558260977268219,\n",
       "  0.8073039054870605,\n",
       "  1.476299524307251,\n",
       "  0.9976356625556946,\n",
       "  0.8058220744132996,\n",
       "  0.7809773683547974,\n",
       "  0.9709742665290833,\n",
       "  0.628913402557373,\n",
       "  0.5202645063400269,\n",
       "  1.279232382774353,\n",
       "  1.0415822267532349,\n",
       "  0.5342992544174194,\n",
       "  0.7706437110900879,\n",
       "  0.6983070373535156,\n",
       "  0.5640421509742737,\n",
       "  0.43463629484176636,\n",
       "  1.2914572954177856,\n",
       "  0.4753369390964508,\n",
       "  1.08023202419281,\n",
       "  0.6476349830627441,\n",
       "  0.7228379249572754,\n",
       "  1.179300308227539,\n",
       "  1.3753069639205933,\n",
       "  0.3536570072174072,\n",
       "  0.6289171576499939,\n",
       "  0.5480701923370361,\n",
       "  0.5139838457107544,\n",
       "  0.7521183490753174,\n",
       "  0.7727834582328796,\n",
       "  0.7587529420852661,\n",
       "  0.7806451320648193,\n",
       "  0.6759033799171448,\n",
       "  0.6100418567657471,\n",
       "  0.9699078798294067,\n",
       "  1.100632905960083,\n",
       "  1.038875937461853,\n",
       "  0.8898695707321167,\n",
       "  0.8119125962257385,\n",
       "  0.7522072792053223,\n",
       "  0.6064667701721191,\n",
       "  0.4099832773208618,\n",
       "  0.7418199181556702,\n",
       "  0.9278929233551025,\n",
       "  1.0827794075012207,\n",
       "  0.5853077173233032,\n",
       "  1.0292390584945679,\n",
       "  1.0015509128570557,\n",
       "  0.6385120153427124,\n",
       "  0.9294956922531128,\n",
       "  0.9390320181846619,\n",
       "  0.6137315034866333,\n",
       "  0.4262791872024536,\n",
       "  0.7266227006912231,\n",
       "  0.9202712774276733,\n",
       "  0.6407022476196289,\n",
       "  0.9474468231201172,\n",
       "  0.4788527488708496,\n",
       "  0.6816034317016602,\n",
       "  1.0002542734146118,\n",
       "  0.7990312576293945,\n",
       "  0.7614317536354065,\n",
       "  0.8742969632148743,\n",
       "  1.1550852060317993,\n",
       "  0.2506886124610901,\n",
       "  0.4709185063838959,\n",
       "  0.6720805764198303,\n",
       "  0.40654972195625305,\n",
       "  0.4412708580493927,\n",
       "  0.40051525831222534,\n",
       "  1.0329254865646362,\n",
       "  0.7264116406440735,\n",
       "  0.8433329463005066,\n",
       "  0.673846960067749,\n",
       "  1.0849730968475342,\n",
       "  1.0736892223358154,\n",
       "  0.7318155169487,\n",
       "  0.6975562572479248,\n",
       "  0.5655454397201538,\n",
       "  0.87684565782547,\n",
       "  0.7437751889228821,\n",
       "  0.8654567003250122,\n",
       "  0.6166431307792664,\n",
       "  1.272114634513855,\n",
       "  0.8961385488510132,\n",
       "  1.2600657939910889,\n",
       "  0.6469525098800659,\n",
       "  0.7824855446815491,\n",
       "  0.7876284718513489,\n",
       "  0.5844032764434814,\n",
       "  0.5896341800689697,\n",
       "  0.8204859495162964,\n",
       "  0.6222617030143738,\n",
       "  0.43291065096855164,\n",
       "  0.554253876209259,\n",
       "  0.9182438850402832,\n",
       "  0.4847044050693512,\n",
       "  0.6780035495758057,\n",
       "  0.9499496221542358,\n",
       "  0.8290883302688599,\n",
       "  0.6031571626663208,\n",
       "  0.8995962738990784,\n",
       "  0.5082141757011414,\n",
       "  0.4994495213031769,\n",
       "  0.5858269333839417,\n",
       "  0.7316220998764038,\n",
       "  0.5349369049072266,\n",
       "  0.545044481754303,\n",
       "  0.3737219274044037,\n",
       "  1.0536940097808838,\n",
       "  0.7493485808372498,\n",
       "  0.8844040632247925,\n",
       "  0.8622207045555115,\n",
       "  0.8041265606880188,\n",
       "  0.7353596091270447,\n",
       "  0.9109393954277039,\n",
       "  1.0154273509979248,\n",
       "  0.838565468788147,\n",
       "  0.5710306763648987,\n",
       "  0.7929914593696594],\n",
       " [0.6603874564170837,\n",
       "  1.1371166706085205,\n",
       "  0.6899031400680542,\n",
       "  0.12931238114833832,\n",
       "  0.8330532908439636,\n",
       "  0.1621713638305664,\n",
       "  0.7064201831817627,\n",
       "  0.8741109371185303,\n",
       "  0.473849356174469,\n",
       "  0.34393155574798584,\n",
       "  0.12605123221874237,\n",
       "  0.2263631373643875,\n",
       "  0.5152066946029663,\n",
       "  0.33233094215393066,\n",
       "  0.7412782311439514,\n",
       "  0.5057899355888367,\n",
       "  0.4727415442466736,\n",
       "  0.5654289722442627,\n",
       "  0.3189638555049896,\n",
       "  0.6509915590286255,\n",
       "  0.7014128565788269,\n",
       "  0.6366497874259949,\n",
       "  1.2867019176483154,\n",
       "  0.36485397815704346,\n",
       "  0.6673744916915894,\n",
       "  0.5646206140518188,\n",
       "  0.19201800227165222,\n",
       "  0.5786137580871582,\n",
       "  0.4754282832145691,\n",
       "  1.0937069654464722,\n",
       "  0.38175567984580994,\n",
       "  1.2795790433883667,\n",
       "  0.4563993513584137,\n",
       "  0.23487554490566254,\n",
       "  0.6838099360466003,\n",
       "  0.5277822613716125,\n",
       "  0.6814427375793457,\n",
       "  0.38422149419784546,\n",
       "  0.9271778464317322,\n",
       "  0.3519934117794037,\n",
       "  0.26600298285484314,\n",
       "  0.42138001322746277,\n",
       "  0.8723504543304443,\n",
       "  0.6441199779510498,\n",
       "  0.49740612506866455,\n",
       "  0.2942356765270233,\n",
       "  0.5484529733657837,\n",
       "  0.32231953740119934,\n",
       "  0.6856119632720947,\n",
       "  0.7461366057395935,\n",
       "  0.7541106939315796,\n",
       "  0.46198633313179016,\n",
       "  0.7359771132469177,\n",
       "  0.2472163885831833,\n",
       "  0.058167170733213425,\n",
       "  0.5236740112304688,\n",
       "  0.3233295977115631,\n",
       "  1.1744725704193115,\n",
       "  0.9086403846740723,\n",
       "  0.9192383885383606,\n",
       "  0.47538113594055176,\n",
       "  0.5639566779136658,\n",
       "  0.26685190200805664,\n",
       "  0.6605009436607361,\n",
       "  0.6746780276298523,\n",
       "  0.8724463582038879,\n",
       "  0.40947532653808594,\n",
       "  0.9480316638946533,\n",
       "  0.4189087748527527,\n",
       "  0.4505126476287842,\n",
       "  0.5984556674957275,\n",
       "  0.5440911650657654,\n",
       "  0.7015009522438049,\n",
       "  0.19223102927207947,\n",
       "  0.23738311231136322,\n",
       "  0.5777757167816162,\n",
       "  0.16953380405902863,\n",
       "  0.6685962080955505,\n",
       "  0.6441002488136292,\n",
       "  0.4960939288139343,\n",
       "  0.8116124272346497,\n",
       "  0.7768387198448181,\n",
       "  0.2799331247806549,\n",
       "  0.5241444706916809,\n",
       "  0.6082070469856262,\n",
       "  0.39350467920303345,\n",
       "  0.4045749008655548,\n",
       "  0.4677174687385559,\n",
       "  0.5495527386665344,\n",
       "  0.6982718110084534,\n",
       "  0.36079537868499756,\n",
       "  0.5571304559707642,\n",
       "  0.46171045303344727,\n",
       "  0.1983746439218521,\n",
       "  0.44309791922569275,\n",
       "  0.19495804607868195,\n",
       "  0.8664215207099915,\n",
       "  0.2590094208717346,\n",
       "  0.5159276723861694,\n",
       "  0.4168613851070404,\n",
       "  0.48501086235046387,\n",
       "  0.7117851972579956,\n",
       "  0.4937204420566559,\n",
       "  0.44573235511779785,\n",
       "  0.35948681831359863,\n",
       "  0.5614266991615295,\n",
       "  0.4900650978088379,\n",
       "  0.5625344514846802,\n",
       "  0.5332090258598328,\n",
       "  0.8184988498687744,\n",
       "  0.35013723373413086,\n",
       "  0.9378862977027893,\n",
       "  0.5349239110946655,\n",
       "  0.22288864850997925,\n",
       "  0.14092354476451874,\n",
       "  0.6186886429786682,\n",
       "  0.14721259474754333,\n",
       "  0.3437367379665375,\n",
       "  0.9199495315551758,\n",
       "  0.5564164519309998,\n",
       "  0.23258355259895325,\n",
       "  0.28114378452301025,\n",
       "  1.066506028175354,\n",
       "  0.2823302149772644,\n",
       "  0.21647872030735016,\n",
       "  0.6603907942771912,\n",
       "  0.3903164267539978,\n",
       "  0.7102182507514954,\n",
       "  0.9921993017196655,\n",
       "  0.3799363076686859,\n",
       "  0.48977962136268616,\n",
       "  0.4986274838447571,\n",
       "  0.7819648385047913,\n",
       "  0.6168369054794312,\n",
       "  0.3725280463695526,\n",
       "  0.351203590631485,\n",
       "  0.08233513683080673,\n",
       "  0.3816647231578827,\n",
       "  0.5096848011016846,\n",
       "  0.5831705331802368,\n",
       "  0.589022159576416,\n",
       "  0.7439461946487427,\n",
       "  0.26420891284942627,\n",
       "  0.4478462338447571,\n",
       "  0.5244982242584229,\n",
       "  0.4235838055610657,\n",
       "  0.2938547730445862,\n",
       "  0.08972762525081635,\n",
       "  0.5428212285041809,\n",
       "  0.5108582377433777,\n",
       "  0.34832140803337097,\n",
       "  0.3586895763874054,\n",
       "  0.08034245669841766,\n",
       "  0.4223369359970093,\n",
       "  0.8678574562072754,\n",
       "  0.8193231821060181,\n",
       "  0.5514447689056396,\n",
       "  0.46419766545295715,\n",
       "  0.5077204704284668,\n",
       "  0.5696151852607727,\n",
       "  0.11811203509569168,\n",
       "  0.5872552394866943,\n",
       "  0.8363919258117676,\n",
       "  0.5233855247497559,\n",
       "  0.22194114327430725,\n",
       "  0.5213298201560974,\n",
       "  0.6863238215446472,\n",
       "  0.3999646306037903,\n",
       "  0.2882518768310547,\n",
       "  0.12240558862686157,\n",
       "  1.0711450576782227,\n",
       "  0.37488678097724915,\n",
       "  0.5361348986625671,\n",
       "  0.4260825514793396,\n",
       "  0.27924680709838867,\n",
       "  0.6003197431564331,\n",
       "  0.20992562174797058,\n",
       "  0.18597526848316193,\n",
       "  0.9364303350448608,\n",
       "  0.13293732702732086,\n",
       "  0.40107402205467224,\n",
       "  0.15588724613189697,\n",
       "  0.5202496647834778,\n",
       "  0.8436520099639893,\n",
       "  0.38440021872520447,\n",
       "  0.6551315188407898],\n",
       " [0.21805544197559357,\n",
       "  0.0,\n",
       "  0.029423538595438004,\n",
       "  0.039740439504384995,\n",
       "  0.6465135216712952,\n",
       "  0.23798222839832306,\n",
       "  0.02937023900449276,\n",
       "  0.18456676602363586,\n",
       "  0.057318899780511856,\n",
       "  0.008360925130546093,\n",
       "  0.08789023756980896,\n",
       "  0.34698647260665894,\n",
       "  0.08483583480119705,\n",
       "  0.01314947847276926,\n",
       "  0.03648911044001579,\n",
       "  0.08890335261821747,\n",
       "  0.02580191008746624,\n",
       "  0.24951405823230743,\n",
       "  0.2320629507303238,\n",
       "  0.05719275400042534,\n",
       "  0.07108588516712189,\n",
       "  0.38203194737434387,\n",
       "  0.11226735264062881,\n",
       "  0.05914529785513878,\n",
       "  0.007944629527628422,\n",
       "  0.025620097294449806,\n",
       "  0.0021093860268592834,\n",
       "  0.1327192783355713,\n",
       "  0.055221542716026306,\n",
       "  0.2747454345226288,\n",
       "  0.1279117316007614,\n",
       "  0.05784620717167854,\n",
       "  0.11174140870571136,\n",
       "  0.057715028524398804,\n",
       "  0.03362645208835602,\n",
       "  0.15985579788684845,\n",
       "  0.05086861923336983,\n",
       "  0.00026575452648103237,\n",
       "  0.0648503303527832,\n",
       "  0.12670588493347168,\n",
       "  0.10502209514379501,\n",
       "  0.2118961662054062,\n",
       "  0.05114090442657471,\n",
       "  0.02882879227399826,\n",
       "  0.2459910362958908,\n",
       "  0.09919726848602295,\n",
       "  0.18937630951404572,\n",
       "  0.04458365589380264,\n",
       "  0.09273576736450195,\n",
       "  0.20266063511371613,\n",
       "  0.06627990305423737,\n",
       "  0.004840136040002108,\n",
       "  0.082402303814888,\n",
       "  0.8277967572212219,\n",
       "  0.06655910611152649,\n",
       "  0.19881771504878998,\n",
       "  0.22269339859485626,\n",
       "  0.15939892828464508,\n",
       "  0.04231563210487366,\n",
       "  0.0,\n",
       "  0.01843520998954773,\n",
       "  0.16739745438098907,\n",
       "  0.14853830635547638,\n",
       "  0.00021111476235091686,\n",
       "  0.11301521956920624,\n",
       "  0.10844183713197708,\n",
       "  0.5980932712554932,\n",
       "  0.07286864519119263,\n",
       "  0.8186579346656799,\n",
       "  0.0,\n",
       "  0.10414975136518478,\n",
       "  0.02758335880935192,\n",
       "  0.18378885090351105,\n",
       "  0.022188549861311913,\n",
       "  0.23379749059677124,\n",
       "  0.13927696645259857,\n",
       "  0.07337921112775803,\n",
       "  0.0,\n",
       "  0.2321825921535492,\n",
       "  0.0,\n",
       "  0.07381285727024078,\n",
       "  0.05930699035525322,\n",
       "  0.42982617020606995,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.002430715598165989,\n",
       "  0.06195027753710747,\n",
       "  0.035378847271203995,\n",
       "  0.09769700467586517,\n",
       "  0.08878083527088165,\n",
       "  0.3804960548877716,\n",
       "  0.0035577318631112576,\n",
       "  0.016546737402677536,\n",
       "  0.20744635164737701,\n",
       "  0.021475860849022865,\n",
       "  0.16421809792518616,\n",
       "  0.09793698787689209,\n",
       "  0.1237059086561203,\n",
       "  0.14685635268688202,\n",
       "  0.043287765234708786,\n",
       "  0.15039856731891632,\n",
       "  0.05194548889994621,\n",
       "  0.054260171949863434,\n",
       "  0.23503130674362183,\n",
       "  0.27835941314697266,\n",
       "  0.2843395471572876,\n",
       "  0.10671643167734146,\n",
       "  0.0,\n",
       "  0.013515068218111992,\n",
       "  0.2708296775817871,\n",
       "  0.030557872727513313,\n",
       "  0.0016916095046326518,\n",
       "  0.1371162086725235,\n",
       "  0.052270811051130295,\n",
       "  0.0056999968364834785,\n",
       "  0.6896910667419434,\n",
       "  0.41281405091285706,\n",
       "  0.4224971532821655,\n",
       "  0.11299281567335129,\n",
       "  0.10896387696266174,\n",
       "  0.006446531508117914,\n",
       "  0.0803808867931366,\n",
       "  0.15902356803417206,\n",
       "  0.0356050468981266,\n",
       "  0.21813109517097473,\n",
       "  0.22759482264518738,\n",
       "  0.030894001945853233,\n",
       "  0.38057997822761536,\n",
       "  0.08383480459451675,\n",
       "  0.44938135147094727,\n",
       "  0.10653191804885864,\n",
       "  0.0,\n",
       "  0.08774275332689285,\n",
       "  0.057570427656173706,\n",
       "  0.022563455626368523,\n",
       "  0.0514649860560894,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.15585550665855408,\n",
       "  0.12349723279476166,\n",
       "  0.00728595769032836,\n",
       "  0.12186932563781738,\n",
       "  0.03741096332669258,\n",
       "  0.10148116201162338,\n",
       "  0.0026645446196198463,\n",
       "  0.1790749877691269,\n",
       "  0.0,\n",
       "  0.053624801337718964,\n",
       "  0.14801588654518127,\n",
       "  0.24605271220207214,\n",
       "  0.018476171419024467,\n",
       "  0.028175601735711098,\n",
       "  0.09756641834974289,\n",
       "  0.16691315174102783,\n",
       "  0.018565570935606956,\n",
       "  0.014371145516633987,\n",
       "  0.22350937128067017,\n",
       "  0.2074306458234787,\n",
       "  0.12305441498756409,\n",
       "  0.329355925321579,\n",
       "  0.3017390966415405,\n",
       "  0.0,\n",
       "  0.039181094616651535,\n",
       "  2.8018143893859815e-06,\n",
       "  0.1373627483844757,\n",
       "  0.01885334402322769,\n",
       "  0.023985978215932846,\n",
       "  0.0004213365609757602,\n",
       "  0.06885282695293427,\n",
       "  0.11507324129343033,\n",
       "  0.18495747447013855,\n",
       "  0.07742258906364441,\n",
       "  0.08183958381414413,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.10937939584255219,\n",
       "  0.15100495517253876,\n",
       "  0.10843856632709503,\n",
       "  0.026302030310034752,\n",
       "  0.04550831392407417,\n",
       "  0.2437903881072998,\n",
       "  0.09503040462732315,\n",
       "  0.06508750468492508,\n",
       "  0.15981759130954742,\n",
       "  0.1551973670721054,\n",
       "  0.10658722370862961,\n",
       "  0.0,\n",
       "  0.1376352459192276,\n",
       "  0.0,\n",
       "  0.01967635564506054,\n",
       "  0.06208118423819542,\n",
       "  0.03840889409184456,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.09816792607307434,\n",
       "  0.20888082683086395,\n",
       "  0.06885243207216263,\n",
       "  0.10212038457393646,\n",
       "  0.11810735613107681,\n",
       "  0.03072221390902996,\n",
       "  0.019004015251994133,\n",
       "  0.025813380256295204,\n",
       "  0.029100589454174042,\n",
       "  0.02405766397714615,\n",
       "  0.3753427267074585,\n",
       "  0.052519675344228745,\n",
       "  0.0760340467095375,\n",
       "  0.0004527535056695342,\n",
       "  0.0,\n",
       "  0.0335429310798645,\n",
       "  0.13309012353420258,\n",
       "  0.0,\n",
       "  0.08688607066869736,\n",
       "  0.006064598448574543,\n",
       "  0.017687909305095673,\n",
       "  0.11761866509914398,\n",
       "  0.2533181607723236,\n",
       "  0.1218804195523262,\n",
       "  0.03070063143968582,\n",
       "  0.07743408530950546,\n",
       "  0.09933802485466003,\n",
       "  0.1740591675043106,\n",
       "  0.03332787752151489,\n",
       "  0.002599956002086401,\n",
       "  0.0,\n",
       "  0.08598464727401733,\n",
       "  0.0,\n",
       "  0.06461486220359802,\n",
       "  0.157600536942482,\n",
       "  0.0,\n",
       "  0.12008129805326462,\n",
       "  0.15822143852710724,\n",
       "  0.1421557068824768,\n",
       "  0.06533671915531158,\n",
       "  0.0024855085648596287,\n",
       "  0.0018586637452244759,\n",
       "  0.33496853709220886,\n",
       "  0.07460204511880875,\n",
       "  0.16457724571228027,\n",
       "  0.06129768490791321,\n",
       "  0.037368159741163254,\n",
       "  0.43981167674064636,\n",
       "  0.0031785578466951847,\n",
       "  0.14307169616222382,\n",
       "  0.06454656273126602,\n",
       "  0.024839773774147034,\n",
       "  0.00992992240935564,\n",
       "  0.19138966500759125,\n",
       "  0.010835559107363224,\n",
       "  0.05907713621854782,\n",
       "  0.0013810310047119856,\n",
       "  0.4231500029563904,\n",
       "  0.13579697906970978,\n",
       "  0.15480045974254608,\n",
       "  0.027047256007790565,\n",
       "  0.0395209901034832,\n",
       "  0.05068906024098396,\n",
       "  0.5687137246131897,\n",
       "  0.03985459357500076,\n",
       "  0.0,\n",
       "  0.02343096397817135,\n",
       "  0.02019430138170719,\n",
       "  0.028456764295697212,\n",
       "  0.007900672033429146,\n",
       "  0.0,\n",
       "  0.2807329297065735,\n",
       "  0.2180599719285965,\n",
       "  0.0761733427643776,\n",
       "  0.25220417976379395,\n",
       "  0.4922288954257965,\n",
       "  0.0721755251288414,\n",
       "  0.11994149535894394,\n",
       "  0.13996289670467377,\n",
       "  0.053765106946229935,\n",
       "  0.12431500852108002,\n",
       "  0.3057336211204529,\n",
       "  0.19024252891540527,\n",
       "  0.17609141767024994,\n",
       "  0.15608903765678406,\n",
       "  0.06938820332288742,\n",
       "  0.17969168722629547,\n",
       "  0.06167209520936012,\n",
       "  0.0,\n",
       "  0.03742596507072449,\n",
       "  0.0033895166125148535,\n",
       "  0.01227249763906002,\n",
       "  0.05318831279873848,\n",
       "  0.0,\n",
       "  0.0,\n",
       "  0.035962145775556564,\n",
       "  0.04573580250144005,\n",
       "  0.09437481313943863,\n",
       "  0.07283393293619156,\n",
       "  0.12988853454589844,\n",
       "  0.007583642844110727,\n",
       "  0.0328262634575367,\n",
       "  0.17004379630088806,\n",
       "  0.009051302447915077,\n",
       "  0.1734747439622879,\n",
       "  0.27672645449638367,\n",
       "  0.03490366414189339,\n",
       "  0.07348037511110306,\n",
       "  0.0,\n",
       "  0.10028284788131714,\n",
       "  0.005393897648900747,\n",
       "  0.13754910230636597,\n",
       "  0.08971196413040161,\n",
       "  0.04317489266395569,\n",
       "  0.0,\n",
       "  0.06232214719057083,\n",
       "  0.004025506787002087,\n",
       "  0.14732760190963745,\n",
       "  0.023656126111745834,\n",
       "  0.02255023829638958,\n",
       "  0.13048841059207916,\n",
       "  0.08636043965816498,\n",
       "  0.08420159667730331,\n",
       "  0.01611313968896866,\n",
       "  0.060663871467113495,\n",
       "  0.06432990729808807,\n",
       "  0.003227498847991228,\n",
       "  0.07134231925010681,\n",
       "  0.08918441087007523,\n",
       "  0.37658756971359253,\n",
       "  0.3356456160545349,\n",
       "  0.15204645693302155,\n",
       "  0.10773935914039612,\n",
       "  0.16079454123973846,\n",
       "  0.09934438019990921,\n",
       "  0.1288895606994629,\n",
       "  0.162229984998703,\n",
       "  0.0069753248244524,\n",
       "  0.0033423283603042364,\n",
       "  0.17243348062038422,\n",
       "  0.13276374340057373,\n",
       "  0.349773645401001,\n",
       "  0.1369660496711731,\n",
       "  0.05183980241417885,\n",
       "  0.0012526663485914469,\n",
       "  0.014605378732085228,\n",
       "  0.07874178886413574,\n",
       "  0.005040621850639582,\n",
       "  0.12092605978250504,\n",
       "  0.12527421116828918,\n",
       "  0.02462857775390148,\n",
       "  0.13589192926883698,\n",
       "  0.2355181872844696,\n",
       "  0.07998567074537277,\n",
       "  0.041543979197740555,\n",
       "  0.0074915532022714615,\n",
       "  0.08576937019824982,\n",
       "  0.036013029515743256,\n",
       "  0.5736735463142395,\n",
       "  0.10814236849546432,\n",
       "  0.10349079966545105,\n",
       "  0.19611209630966187,\n",
       "  0.07027024775743484,\n",
       "  0.0,\n",
       "  0.03141321614384651,\n",
       "  0.14371944963932037,\n",
       "  0.006681437604129314,\n",
       "  0.009491616860032082,\n",
       "  0.0907733365893364,\n",
       "  0.019182898104190826,\n",
       "  0.03898921608924866,\n",
       "  0.18019190430641174,\n",
       "  0.02203417755663395,\n",
       "  0.03501766175031662,\n",
       "  0.01729327067732811,\n",
       "  0.003965720534324646,\n",
       "  0.4384474754333496,\n",
       "  0.0,\n",
       "  0.013030226342380047,\n",
       "  0.008836137130856514,\n",
       "  0.09200797975063324,\n",
       "  0.08043007552623749,\n",
       "  0.012150328606367111,\n",
       "  0.1353280246257782,\n",
       "  0.35587239265441895,\n",
       "  0.07388134300708771,\n",
       "  0.02142581157386303,\n",
       "  0.21459931135177612],\n",
       " [1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_206226/3520072505.py:2: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  tr_results = np.asarray(train_results)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "tr_results = np.asarray(train_results)\n",
    "tr_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tr_results[\u001b[39m0\u001b[39;49m]\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "tr_results[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(123)\n",
    "\n",
    "w = 0.8    # bar width\n",
    "x = [1, 2] # x-coordinates of your bars\n",
    "colors = [(0, 0, 1, 1), (1, 0, 0, 1)]    # corresponding colors\n",
    "y = [np.random.random(30) * 2 + 5,       # data series\n",
    "    np.random.random(10) * 3 + 8]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.bar(x,\n",
    "       height=[np.mean(yi) for yi in y],\n",
    "       yerr=[np.std(yi) for yi in y],    # error bars\n",
    "       capsize=12, # error bar cap width in points\n",
    "       width=w,    # bar width\n",
    "       tick_label=[\"control\", \"test\"],\n",
    "       color=(0,0,0,0),  # face color transparent\n",
    "       edgecolor=colors,\n",
    "       #ecolor=colors,    # error bar colors; setting this raises an error for whatever reason.\n",
    "       )\n",
    "\n",
    "for i in range(len(x)):\n",
    "    # distribute scatter randomly across whole width of bar\n",
    "    ax.scatter(x[i] + np.random.random(y[i].size) * w - w / 2, y[i], color=colors[i])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save(f'{model_name}_training_loss.npy', tr_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_training_accuracy.npy', tr_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_validation_loss.npy', v_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_validation_accuracy.npy', v_acc, allow_pickle=True)\n",
    "\n",
    "np.save(f'{model_name}_test_loss.npy', tst_loss, allow_pickle=True)\n",
    "np.save(f'{model_name}_test_accuracy.npy', tst_acc, allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "training_loss = np.load(f'{model_name}_training_loss.npy', allow_pickle=True)\n",
    "training_accuracy = np.load(f'{model_name}_training_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "validation_loss = np.load(f'{model_name}_validation_loss.npy', allow_pickle=True)\n",
    "validation_accuracy = np.load(f'{model_name}_validation_accuracy.npy', allow_pickle=True)\n",
    "\n",
    "test_loss = np.load(f'{model_name}_test_loss.npy', allow_pickle=True)\n",
    "test_accuracy = np.load(f'{model_name}_test_accuracy.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_loss, label='Training Loss')\n",
    "plt.plot(epochs, validation_loss, label='Validation Loss')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Training and Validation Loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a sequence of integers to represent the epoch numbers\n",
    "epochs = range(1, num_epochs+1)\n",
    " \n",
    "# Plot and label the training and validation loss values\n",
    "plt.plot(epochs, training_accuracy, label='Training Accuracy')\n",
    "plt.plot(epochs, validation_accuracy, label='Validation Accuracy')\n",
    "plt.plot(epochs, test_accuracy, label='Test Accuracy')\n",
    " \n",
    "# Add in a title and axes labels\n",
    "plt.title('Accuracy vs. Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    " \n",
    "# Set the tick locations\n",
    "plt.xticks(np.arange(0, num_epochs+1, num_epochs/10))\n",
    "plt.ylim(0,1)\n",
    " \n",
    "# Display the plot\n",
    "plt.legend(loc='best')\n",
    "plt.savefig(f'{model_name}_Accuracy vs. Epochs.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_fraction = [0,0]\n",
    "\n",
    "train_fraction = [0,0]\n",
    "val_fraction = [0,0]\n",
    "test_fraction = [0,0]\n",
    "\n",
    "for grph in train_dataset: \n",
    "    if grph.y == 1: \n",
    "        train_fraction[1] +=1\n",
    "        dataset_fraction[1] +=1 \n",
    "    else: \n",
    "        train_fraction[0] +=1\n",
    "        dataset_fraction[0] +=1 \n",
    "\n",
    "for grph in val_dataset: \n",
    "    if grph.y == 1:\n",
    "         val_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1  \n",
    "    else:\n",
    "         val_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "for grph in test_dataset: \n",
    "    if grph.y == 1:\n",
    "         test_fraction[1] +=1\n",
    "         dataset_fraction[1] +=1 \n",
    "    else:\n",
    "         test_fraction[0] +=1\n",
    "         dataset_fraction[0] +=1\n",
    "\n",
    "print(f'Overall dataset percentage of label 1 = {dataset_fraction[1]/len(dataset)})')\n",
    "print(f'Training dataset percentage of label 1 = {train_fraction} = {train_fraction[1]/len(train_dataset)}')\n",
    "print(f'Validation dataset percentage of label 1 = {val_fraction} = {val_fraction[1]/len(val_dataset)}')\n",
    "print(f'Test dataset percentage of label 1 = {test_fraction} = {test_fraction[1]/len(test_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Graph: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0, pos0, adj0 = torch.load(f'{model_name}_img0_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN\n",
    "print(x0[0].shape)\n",
    "x0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos0[0].shape)\n",
    "pos0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adj0[0].shape)\n",
    "adj0[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj0[0])\n",
    "visualize_points(pos0[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph After 1st Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_emb, x1_pool, pos1, adj1, s1= torch.load(f'{model_name}_img1_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj0 @ x_0 @ w_gnn_emb)\n",
    "print(x1_emb[0].shape)\n",
    "x1_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj_0 @ x_0 @ w_gnn_pool\n",
    "print(s1[0].shape)\n",
    "s1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s).t() @ pos_in)\n",
    "print(pos1[0].shape)\n",
    "pos1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s).t() @ x_in)\n",
    "print(x1_pool[0].shape)\n",
    "x1_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix = softmax(adj_out = softmax(s.t()) @ adj_in @ softmax(s))\n",
    "print(adj1[0].shape)\n",
    "adj1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj1[0])\n",
    "visualize_points(pos1[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 2nd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2_emb, x2_pool, pos2, adj2, s2 = torch.load(f'{model_name}_img2_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj1 @ x1_pool @ w_gnn_emb)\n",
    "print(x2_emb[0].shape)\n",
    "x2_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: adj1 @ x1_pool @ w_gnn_pool), dim=1\n",
    "print(s2[0].shape)\n",
    "s2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos2[0].shape)\n",
    "pos2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s2).t() @ x2_emb)\n",
    "print(x2_pool[0].shape)\n",
    "x2_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s).T @ adj @ softmax(s)\n",
    "print(adj2[0].shape)\n",
    "adj2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj2[0])\n",
    "visualize_points(pos2[0].cpu(), edge_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph after 3rd reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x3_emb, x3_pool, pos3, adj3, s3 = torch.load(f'{model_name}_img3_data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Embedding GNN (adj_0 @ x_0 @ w_gnn_emb)\n",
    "print(x3_emb[0].shape)\n",
    "x3_emb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output of Pooling GNN: torch.softmax(adj_0 @ x_0 @ w_gnn_pool), dim=1)\n",
    "print(s3[0].shape)\n",
    "s3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Coordinate Matrix (pos_out = softmax(s.t()) @ pos_in)\n",
    "print(pos3[0].shape)\n",
    "pos3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Feature Matrix (x_out = softmax(s.t()) @ x_0)\n",
    "print(x3_pool[0].shape)\n",
    "x3_pool[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output Adjacency Matrix (adj = softmax(s.t()) @ adj @ softmax(s)\n",
    "print(adj3[0].shape)\n",
    "adj3[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_index, _ = dense_to_sparse(adj3[0])\n",
    "visualize_points(pos3[0].cpu(), edge_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyg_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fec49b2b4a08384e01fd851531c21ff749e56c9a3b01e22f757b2c66b74daf11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
